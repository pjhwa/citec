---
title: "네트워크 데이터 유실률 분석 #2"
date: 2025-06-17
tags: [netstat, ethtool, drop, network]
categories: [Network, Analysis]
---

# 네트워크 데이터 패킷 유실률 분석 (2025/06/16 ~ 2025/06/17) - Grok

애플리케이션에서 패킷 유실률을 0.1% 이하로 유지하기를 원하며, 이를 위해 2025년 6월 16일과 17일의 데이터를 바탕으로 유실률을 분석하고 원인을 파악하고자 합니다. 유실률은 on-premise 환경의 baremetal 서버(nzeus10)를 기준으로 계산되며, 이 서버는 L2B를 통과하지 않는 전용선으로 연결되어 있습니다. 반면, 상암 VPC의 VM들은 L2B를 거쳐 데이터를 수신하므로 인프라 차이에 따른 영향을 고려해야 합니다. 아래에서는 제공된 데이터를 정밀 분석하여 유실률의 현황, 원인, 그리고 추가 조치 방안을 상세히 설명합니다.

---

## 1. 유실률 계산 및 현황

유실률은 nzeus10에서 수신한 패킷 수를 100%로 보고, 각 VM에서 처리된 패킷 수와의 차이를 비율로 계산합니다. 아래는 6월 16일과 17일의 07:55~09:55 구간에 대한 데이터입니다.

### 6월 16일 (07:55~09:55)
| 서버/VM   | 애플리케이션 패킷 수 | 유실 패킷 수         | 유실률 (%)       |
|-----------|----------------------|----------------------|------------------|
| nzeus10   | 115,989,899          | -                    | -                |
| mca5102   | 115,805,978          | 183,921              | 0.158%           |
| mca5204   | 115,838,115          | 151,784              | 0.131%           |
| mca5304   | 115,881,168          | 108,731              | 0.094%           |

- **계산 예시 (mca5102)**:  
  `(115,989,899 - 115,805,978) / 115,989,899 × 100 ≈ 0.158%`

### 6월 17일 (07:55~09:55)
| 서버/VM   | 애플리케이션 패킷 수 | 유실 패킷 수         | 유실률 (%)       |
|-----------|----------------------|----------------------|------------------|
| nzeus10   | 125,497,714          | -                    | -                |
| mca5102   | 125,202,425          | 295,289              | 0.235%           |
| mca5204   | 125,234,966          | 262,748              | 0.209%           |
| mca5304   | 125,216,928          | 280,786              | 0.224%           |

- **계산 예시 (mca5102)**:  
  `(125,497,714 - 125,202,425) / 125,497,714 × 100 ≈ 0.235%`

### 관찰 결과
- **6월 16일**: mca5102(0.158%), mca5204(0.131%), mca5304(0.094%)로, mca5304만 0.1% 이하를 달성.
- **6월 17일**: 모든 VM에서 유실률이 증가하여 mca5102(0.235%), mca5204(0.209%), mca5304(0.224%)로 0.1%를 초과.
- 고객 요구치(0.1% 이하)를 충족하려면 추가적인 원인 분석과 개선이 필요합니다.

---

## 2. 주요 요인 분석

유실률에 영향을 줄 수 있는 요인으로 **UDP 수신 버퍼 오류(UdpRcvbufErrors)**, **NUMA 마이그레이션**, **L2B 네트워크 오류**, **애플리케이션 처리 속도** 등을 검토합니다.

### 2.1 UDP 수신 버퍼 오류 (UdpRcvbufErrors)
UDP 수신 버퍼 오류는 시스템이 수신된 패킷을 처리할 버퍼를 할당하지 못해 발생하며, 이는 패킷 유실로 이어질 수 있습니다.

#### 데이터
| 날짜       | VM      | UdpRcvbufErrors | 유실 패킷 수 | 유실률 (%) |
|------------|---------|-----------------|--------------|------------|
| 6월 16일   | mca5102 | 9,880           | 183,921      | 0.158      |
|            | mca5204 | 2,195           | 151,784      | 0.131      |
|            | mca5304 | 8,548           | 108,731      | 0.094      |
| 6월 17일   | mca5102 | 38,306          | 295,289      | 0.235      |
|            | mca5204 | 19,655          | 262,748      | 0.209      |
|            | mca5304 | 22,851          | 280,786      | 0.224      |

#### 분석
- **상관관계**:  
  - 6월 16일 mca5102에서 UdpRcvbufErrors(9,880)는 유실 패킷(183,921)의 약 5.4%에 해당하며, 6월 17일에는 38,306으로 증가하여 유실 패킷(295,289)의 13%를 차지합니다.  
  - 오류 수가 증가할수록 유실률도 상승하는 경향이 관찰됩니다(예: mca5102의 0.158% → 0.235%).
- **불일치**:  
  - UdpRcvbufErrors는 유실 패킷 수보다 훨씬 적습니다(예: 6월 16일 mca5102에서 9,880 vs. 183,921). 이는 UDP 버퍼 오류 외에 다른 요인이 유실에 기여함을 시사합니다.
- **버퍼 설정**:  
  - 현재 `net.core.rmem_max`와 `rmem_default`가 32MB로 설정되어 있음에도 오류가 발생하므로, 패킷 처리 속도가 버퍼 크기를 초과할 가능성이 있습니다.

#### 결론
UDP 수신 버퍼 오류는 유실률에 영향을 미치지만, 전체 유실을 설명하지 못합니다. 추가적인 시스템 또는 애플리케이션 병목현상을 조사해야 합니다.

---

### 2.2 NUMA 마이그레이션
NUMA 마이그레이션은 VM이 NUMA 노드 간 이동하며 발생하는 성능 저하로, 패킷 처리 지연을 유발할 수 있습니다.

#### 데이터 (mca5102 기준)
| 날짜       | NUMA Migration Delta | 유실률 (%) |
|------------|----------------------|------------|
| 6월 16일   | 18 (3300 - 3282)     | 0.158      |
| 6월 17일   | 9 (3552 - 3543)      | 0.235      |

#### 분석
- **상관관계 부재**:  
  - 6월 16일에 NUMA 마이그레이션이 18회로 많았으나 유실률은 0.158%로 낮았고, 6월 17일에는 9회로 줄었지만 유실률이 0.235%로 증가했습니다.  
  - 이는 NUMA 마이그레이션과 유실률 간 직접적인 상관관계가 약함을 나타냅니다.
- **영향 가능성**:  
  - 마이그레이션 시 캐시 무효화 및 메모리 접근 지연이 발생할 수 있지만, CPU 경합 지표(CST < 0.54, READY < 0.35)는 낮아 큰 영향을 주지 않은 것으로 보입니다.

#### 결론
NUMA 마이그레이션은 유실률에 미미한 영향을 미칠 가능성이 있으며, 주요 원인으로 보기 어렵습니다.

---

### 2.3 L2B (L2 BMEdge) 네트워크 오류
L2B는 VM으로 패킷을 전달하는 경로에 위치하며, 여기서 발생하는 오류는 패킷 유실로 이어질 수 있습니다.

#### 데이터
| 날짜       | 시간 내 패킷 수 | 시간 내 에러 수 | 에러율 (%) | VM 유실률 (%) |
|------------|-----------------|-----------------|------------|---------------|
| 6월 16일   | 193,416,033     | 0               | 0.0000     | 0.094~0.158   |
| 6월 17일   | 204,817,490     | 180,900         | 0.0883     | 0.209~0.235   |

#### 분석
- **6월 16일**: L2B에서 에러가 없었고, VM 유실률은 0.094%~0.158%로 상대적으로 낮았습니다.
- **6월 17일**: L2B 에러율이 0.0883%(180,900개)로 증가하며, VM 유실률도 0.209%~0.235%로 상승했습니다.
- **영향**:  
  - L2B 에러 180,900개는 mca5102의 유실 패킷(295,289)의 약 61%를 차지하며, 유의미한 기여를 한 것으로 보입니다.  
  - 그러나 L2B는 여러 VM으로 패킷을 분배하므로, 개별 VM의 유실과 직접 매핑하기는 어렵습니다.

#### 결론
6월 17일 L2B 오류는 유실률 증가에 기여했으나, 6월 16일에는 영향이 없었으므로 일관된 원인은 아닙니다.

---

### 2.4 애플리케이션 처리 속도
애플리케이션이 시스템에서 수신한 패킷을 모두 처리하지 못하면 유실이 발생할 수 있습니다.

#### 데이터 비교 (mca5102)
| 날짜       | 시스템 수신 (Udp packets received) | 앱 처리 패킷 수 | 차이         |
|------------|------------------------------------|-----------------|--------------|
| 6월 16일   | 118,726,547                        | 115,805,978     | 2,920,569    |
| 6월 17일   | 128,513,393                        | 125,202,425     | 3,310,968    |

#### 분석
- **차이**: 시스템이 수신한 UDP 패킷 수와 애플리케이션이 처리한 패킷 수 간 큰 차이가 있으며(6월 16일: 2,920,569, 6월 17일: 3,310,968), 이는 애플리케이션이 모든 패킷을 처리하지 못함을 나타냅니다.
- **nzeus10과의 비교**:  
  - nzeus10은 115,989,899(6월 16일) 및 125,497,714(6월 17일)를 처리했으며, mca5102의 시스템 수신량(118,726,547 및 128,513,393)보다 적습니다.  
  - 이는 nzeus10과 VM 간 네트워크 경로 차이 또는 애플리케이션 필터링 차이가 있을 수 있음을 시사합니다.
- **병목 가능성**: CPU 경합(CST, READY)이 낮아 하드웨어 병목은 적지만, 애플리케이션 자체의 처리 속도가 제한 요인일 수 있습니다.

#### 결론
애플리케이션 처리 속도 부족이 유실률의 주요 원인 중 하나일 가능성이 높습니다.

---

## 3. 종합 원인 및 개선 방안

### 주요 원인
1. **UDP 수신 버퍼 오류**: 버퍼 크기(32MB)에도 불구하고 오류가 발생하며, 유실의 일부를 설명합니다.
2. **L2B 네트워크 오류**: 6월 17일에 유의미한 영향을 미쳤으나 일관된 원인은 아님.
3. **애플리케이션 처리 속도**: 시스템 수신량 대비 처리량이 적어 주요 병목으로 의심됨.

### 개선 방안
1. **애플리케이션 성능 분석**  
   - 프로파일링을 통해 CPU 사용량, 메모리 접근, I/O 병목을 확인하고, 멀티스레딩 또는 코드 최적화로 처리 속도를 개선합니다.
2. **UDP 버퍼 크기 추가 증가**  
   - `net.core.rmem_max`를 64MB로 늘려 테스트하고, UdpRcvbufErrors 감소 여부를 확인합니다.
3. **L2B 네트워크 점검**  
   - 6월 17일 오류 원인을 조사하고, 네트워크 안정성을 높입니다(스위치 설정, 대역폭 점검 등).
4. **부하 분산**  
   - VM 수를 늘리거나 패킷 처리를 분산하여 개별 VM의 부하를 줄입니다.
5. **NUMA 설정 최적화**  
   - 마이그레이션 영향이 적더라도, NUMA 고정 설정을 테스트하여 성능 변화를 확인합니다.

---

## 4. 추가 조사 필요 사항
정확한 원인을 파악하려면 다음 데이터를 추가로 확보해야 합니다:
- **nzeus10의 netstat 데이터**: UdpRcvbufErrors가 있는지 확인하여 baremetal 서버의 병목 여부를 파악.
- **애플리케이션 로그**: 처리 실패 패킷 수와 원인을 기록하여 시스템 유실과 구분.
- **실시간 모니터링**: 패킷 유실이 발생하는 정확한 시점과 상관관계 분석.
- **패킷 필터링 여부**: nzeus10과 VM 간 수신 패킷 차이의 원인(필터링, 네트워크 경로 등) 규명.

---

## 5. 결론
6월 16일과 17일 데이터 분석 결과, 유실률은 0.094%~0.235%로 고객 요구치(0.1%)를 초과하는 경우가 많았습니다. 주요 원인은 애플리케이션 처리 속도 부족, UDP 수신 버퍼 오류, 그리고 간헐적인 L2B 오류로 추정됩니다. 위 개선 방안을 적용하고 추가 데이터를 수집하여 유실률을 0.1% 이하로 줄이는 것이 필요합니다.

---

# Gemini 분석 

## 1\. 유실률 계산 및 분석 개요

유실률은 on-premise baremetal 서버(nzeus10)에서 수신한 패킷 수를 100% 기준으로, 각 VM(mca5102, mca5204, mca5304)에서 수신한 패킷 수와의 차이를 통해 계산합니다. 고객은 0.1% 이하의 유실률을 요구하고 있습니다.

유입 경로는 "증권시세스위치 - SCP Agg스위치 - L2B BMEdge(xstats) - ESXi(CPU Ready) - ESXi VM vNIC(rxSummary) - In-Guest vNIC(ethtool) - In-Guest(netstat) - Application"으로, 각 구간에서 유실이 발생할 가능성이 있습니다.

애플리케이션이 브로드캐스트 및 UDP 패킷을 많이 처리한다는 점을 고려하여 UDP 관련 지표를 중점적으로 분석하겠습니다. 또한, NUMA Migration, CPU Ready, UDP 수신 버퍼 오류, NIC ring buffer 등의 설정 및 통계가 유실률에 미치는 영향을 파악하겠습니다.

## 2\. 06/16 07:55 \~ 09:55 유실률 분석

### 2.1. 애플리케이션 단 유실률 계산

| App | nzeus10 (Base) | mca5102 | mca5204 | mca5304 |
|---|---|---|---|---|
| 07:55 \~ 09:55 | 115,989,899 | 115,805,978 | 115,838,115 | 115,881,168 |

**유실률 계산:**
$유실률 = ((기준\\ 패킷\\ 수 - VM\\ 패킷\\ 수) / 기준\\ 패킷\\ 수) \\times 100%$

  * **mca5102 유실률:** $((115,989,899 - 115,805,978) / 115,989,899) \\times 100% \\approx 0.1585%$
  * **mca5204 유실률:** $((115,989,899 - 115,838,115) / 115,989,899) \\times 100% \\approx 0.1309%$
  * **mca5304 유실률:** $((115,989,899 - 115,881,168) / 115,989,899) \\times 100% \\approx 0.0937%$

**결과:** mca5102와 mca5204는 고객의 요구사항(0.1% 이하)을 만족하지 못하며, mca5304는 0.1% 이하로 양호한 편입니다.

### 2.2. VM (In-Guest netstat/ethtool) 지표 분석 (06/16 07:55 \~ 09:55)

| VM | Udp packets received | Udp packet receive errors | IpExt InBcastPkts | UdpRcvbufErrors | Ip InDiscards | Udp InErrors | drv dropped rx total | rx buf alloc fail | pkts rx err | bcast pkts rx |
|---|---|---|---|---|---|---|---|---|---|---|
| mca5102 | 118,726,547 | 9,880 | 118,735,732 | 9,880 | 0 | 9,880 | 0 | 0 | 0 | 118,740,740 |
| mca5204 | 118,765,978 | 2,195 | 118,768,269 | 2,195 | 0 | 2,195 | 0 | 0 | 0 | 118,772,997 |
| mca5304 | 118,814,026 | 8,548 | 118,821,440 | 8,548 | 0 | 8,548 | 0 | 0 | 0 | 118,826,683 |

**유의미한 지표:**

  * **Udp packet receive errors / UdpRcvbufErrors / Udp InErrors:** 세 지표의 값이 모두 동일하게 나타나고 있으며, 이는 UDP 수신 버퍼 오류로 인한 패킷 유실이 발생하고 있음을 명확히 보여줍니다.
      * mca5102: 9,880
      * mca5204: 2,195
      * mca5304: 8,548
  * **Ip InDiscards, drv dropped rx total, rx buf alloc fail, pkts rx err:** 모든 VM에서 이 값들이 0으로 나타나는 것은 Guest OS 내부의 드라이버나 IP 스택 단에서의 직접적인 패킷 드롭은 발생하지 않음을 시사합니다. 즉, 패킷이 VM vNIC까지는 정상적으로 도달했음을 의미합니다.

**특이사항:** `Udp packets received` 값과 `App`에서 측정한 패킷 수에 차이가 있습니다. `Udp packets received`는 OS 커널 레벨에서 UDP 패킷을 수신한 총량을 의미하고, `App`에서 측정한 값은 애플리케이션이 직접 처리한 패킷 수를 의미합니다. 이 차이가 유실률의 핵심 원인일 가능성이 높습니다.

`UdpRcvbufErrors`가 발생하는 것은 `net.core.rmem_max`와 `net.core.rmem_default`를 32MB로 늘렸음에도 불구하고, 애플리케이션이 UDP 패킷을 처리하는 속도가 수신 속도를 따라가지 못하여 버퍼 오버플로우가 발생하고 있음을 의미합니다.

### 2.3. ESXi 서버 (rxSummary) 지표 분석 (06/16 07:55 \~ 09:55)

| ESXi 서버 | broadcast pkts rx ok | droppedRx | pktsRxBroadcast |
|---|---|---|---|
| mca5101 (mca5102 VM 호스팅) | 118,710,306 | 0 | 118,710,666 |
| mca5203 (mca5204 VM 호스팅) | 118,714,965 | 0 | 118,715,197 |
| mca5303 (mca5304 VM 호스팅) | 118,776,751 | 0 | 118,777,002 |

**유의미한 지표:**

  * **droppedRx:** 모든 ESXi 서버에서 `droppedRx`가 0으로 나타나는 것은 ESXi 하이퍼바이저 레벨에서 VM vNIC로의 패킷 전달에는 문제가 없음을 시사합니다.
  * `pktsRxBroadcast`와 `broadcast pkts rx ok`는 거의 동일한 값을 보이며, 브로드캐스트 패킷이 ESXi 호스트에 정상적으로 수신되고 있음을 보여줍니다.

### 2.4. 상암 L2B (L2 BMEdge) 지표 분석 (06/16 07:55 \~ 09:55)

| 시간 | 누적 패킷카운트 | 누적 에러카운트 | 시간내 패킷카운트 | 시간내 에러카운트 | 시간내 에러율 |
|---|---|---|---|---|---|
| 2025-06-16 0:00 | 16,810,323,011 | 116,247 | - | - | - |
| 2025-06-16 7:55 | 16,811,552,689 | 142,521 | 1,229,678 | 26,274 | 2.1367% |
| 2025-06-16 8:55 | 16,869,546,991 | 142,521 | 57,994,302 | - | 0.0000% |
| 2025-06-16 9:15 | 16,978,315,294 | 142,521 | 108,768,303 | - | 0.0000% |
| 2025-06-16 9:55 | 17,171,731,327 | 142,521 | 193,416,033 | - | 0.0000% |

**유의미한 지표:**

  * 7:55 시점에 L2B 단에서 `시간내 에러율`이 2.1367%로 매우 높게 나타납니다. 하지만 8:55, 9:15, 9:55에는 에러 카운트가 증가하지 않아 에러율이 0%로 기록되었습니다. 이는 7:55 이전 혹은 특정 시점에 에러가 집중적으로 발생했을 가능성을 시사합니다.
  * L2B 스위치 단에서 에러가 발생했다면 VM으로 전달되는 전체 패킷량에 영향을 미칠 수 있습니다. 하지만 7:55 이후 L2B에서의 에러는 발생하지 않았으므로, 이 시간대(07:55 \~ 09:55)의 VM 유실률에 직접적인 영향을 주었다고 보기는 어렵습니다. (단, 스위치 에러가 누적되어 스위치 내부 버퍼 등에 영향을 주어 간접적인 유실이 발생했을 가능성도 배제할 수는 없습니다.)

### 2.5. 06/16 NUMA Migration 및 CPU 경합

  * **NUMA Migration delta:** mca5102 VM에서 18 (3300 - 3282)건의 NUMA Migration이 발생했습니다. NUMA Migration은 CPU 캐시 적중률을 낮추고 메모리 접근 지연을 유발하여 애플리케이션 성능에 영향을 줄 수 있습니다.
  * **CPU 경합:** CST \< 0.54, READY \< 0.34로, CPU Ready 값이 0.34%로 낮은 편입니다. 이는 CPU 자원 부족으로 인한 성능 저하가 크지 않음을 의미합니다.

### 2.6. 06/16 분석 종합 및 원인 추정

  * **주요 유실 원인:** In-Guest VM의 UDP 수신 버퍼 오버플로우(`UdpRcvbufErrors`)가 가장 명확한 유실 원인으로 보입니다. ESXi 호스트나 L2B 스위치 단에서의 직접적인 패킷 드롭 징후는 현재 데이터에서 뚜렷하게 나타나지 않습니다 (L2B의 7:55 에러율은 주의 필요).
  * **NUMA Migration의 영향:** mca5102의 NUMA Migration 18건이 mca5102의 유실률(0.1585%)에 일부 영향을 미쳤을 가능성이 있습니다. NUMA Migration으로 인해 애플리케이션의 CPU/메모리 접근 지연이 발생하고, 이로 인해 UDP 패킷 처리 속도가 저하되어 버퍼 오버플로우가 심화되었을 수 있습니다. mca5204의 `UdpRcvbufErrors`가 가장 적고 유실률도 mca5304 다음으로 낮다는 점이 이를 뒷받침합니다.
  * **Buffer Size 및 NIC Ring Buffer:** `net.core.rmem_max`와 `net.core.rmem_default`를 32MB로, NIC rx/tx ring buffer를 4096으로 늘렸음에도 불구하고 `UdpRcvbufErrors`가 발생하는 것은 애플리케이션이 예상보다 훨씬 많은 UDP 패킷을 매우 짧은 시간 내에 수신하거나, 애플리케이션 자체의 처리 로직에 병목이 있어 수신 버퍼에서 패킷을 제때 가져가지 못하고 있다는 것을 의미합니다.

## 3\. 06/17 07:55 \~ 09:55 유실률 분석

### 3.1. 애플리케이션 단 유실률 계산

| App | nzeus10 (Base) | mca5102 | mca5204 | mca5304 |
|---|---|---|---|---|
| 07:55\~09:55 | 125,497,714 | 125,202,425 | 125,234,966 | 125,216,928 |

**유실률 계산:**

  * **mca5102 유실률:** $((125,497,714 - 125,202,425) / 125,497,714) \\times 100% \\approx 0.2359%$
  * **mca5204 유실률:** $((125,497,714 - 125,234,966) / 125,497,714) \\times 100% \\approx 0.2093%$
  * **mca5304 유실률:** $((125,497,714 - 125,216,928) / 125,497,714) \\times 100% \\approx 0.2230%$

**결과:** 06/17에는 모든 VM에서 유실률이 0.2%를 넘어 고객의 요구사항(0.1% 이하)을 만족하지 못하며, 06/16보다 유실률이 증가했습니다.

### 3.2. VM (In-Guest netstat/ethtool) 지표 분석 (06/17 07:55 \~ 09:55)

| VM | Udp packets received | Udp packet receive errors | IpExt InBcastPkts | UdpRcvbufErrors | Ip InDiscards | Udp InErrors | drv dropped rx total | rx buf alloc fail | pkts rx err | bcast pkts rx |
|---|---|---|---|---|---|---|---|---|---|---|
| mca5102 | 128,513,393 | 38,306 | 128,550,383 | 38,306 | 0 | 38,306 | 0 | 0 | 0 | 128,555,555 |
| mca5204 | 128,554,578 | 19,655 | 128,573,207 | 19,655 | 0 | 19,655 | 0 | 0 | 0 | 128,578,408 |
| mca5304 | 128,548,416 | 22,851 | 128,571,439 | 22,851 | 0 | 22,851 | 0 | 0 | 0 | 128,576,817 |

**유의미한 지표:**

  * **Udp packet receive errors / UdpRcvbufErrors / Udp InErrors:** 06/16에 비해 모든 VM에서 UDP 수신 버퍼 오류가 크게 증가했습니다.
      * mca5102: 9,880 -\> 38,306 (약 3.8배 증가)
      * mca5204: 2,195 -\> 19,655 (약 8.9배 증가)
      * mca5304: 8,548 -\> 22,851 (약 2.6배 증가)
  * **Ip InDiscards, drv dropped rx total, rx buf alloc fail, pkts rx err:** 여전히 모든 VM에서 이 값들이 0으로 나타나 Guest OS 내부의 드라이버나 IP 스택 단에서의 직접적인 패킷 드롭은 발생하지 않음을 확인했습니다.

### 3.3. ESXi 서버 (rxSummary) 지표 분석 (06/17 07:55 \~ 09:55)

| ESXi 서버 | broadcast pkts rx ok | droppedRx | pktsRxBroadcast |
|---|---|---|---|
| mca5101 | 128,527,382 | 0 | 128,527,732 |
| mca5203 | 128,545,925 | 0 | 128,546,197 |
| mca5303 | 128,525,741 | 0 | 128,526,034 |

**유의미한 지표:**

  * **droppedRx:** 모든 ESXi 서버에서 `droppedRx`가 0으로 나타나는 것은 06/16과 마찬가지로 ESXi 하이퍼바이저 레벨에서 VM vNIC로의 패킷 전달에는 문제가 없음을 시사합니다.

### 3.4. 상암 L2B (L2 BMEdge) 지표 분석 (06/17 07:55 \~ 09:55)

| 시간 | 누적패킷카운트 | 누적에러카운트 | 시간내 패킷카운트 | 시간내 에러카운트 | 시간내 에러율 |
|---|---|---|---|---|---|
| 2025-06-17 7:55 | 18,285,022,220 | 146,651 | 6,148,560 | 222 | 0.0036% |
| 2025-06-17 8:55 | 18,349,071,150 | 146,651 | 64,048,930 | 0 | 0.0000% |
| 2025-06-17 9:15 | 18,467,244,826 | 204,164 | 118,173,676 | 57,513 | 0.0487% |
| 2025-06-17 9:55 | 18,672,062,316 | 385,064 | 204,817,490 | 180,900 | 0.0883% |

**유의미한 지표:**

  * 06/17에는 9:15과 9:55 시점에 L2B 스위치 단에서 `시간내 에러율`이 0.0487%와 0.0883%로 발생했습니다. 이는 06/16에는 없었던 현상입니다. 이 에러는 L2B 스위치 단에서 패킷이 드롭되었음을 의미하며, VM으로 전달되는 전체 패킷량에 직접적인 영향을 미쳤을 것입니다. L2B 에러율은 고객 요구사항 0.1% 이하에 근접하거나 초과하는 수준입니다.

### 3.5. 06/17 NUMA Migration 및 CPU 경합

  * **NUMA Migration delta:** mca5102 VM에서 9 (3552 - 3543)건의 NUMA Migration이 발생했습니다. 06/16의 18건에 비해 감소했습니다.
  * **CPU 경합:** CST \< 0.13, READY \< 0.35로, CPU Ready 값이 0.35%로 여전히 낮은 편입니다.

### 3.6. 06/17 분석 종합 및 원인 추정

  * **주요 유실 원인:**

    1.  **UDP 수신 버퍼 오버플로우 심화:** 06/16에 비해 모든 VM에서 `UdpRcvbufErrors`가 크게 증가했으며, 이는 애플리케이션의 패킷 처리 속도가 수신 속도를 따라가지 못하는 문제가 더욱 심화되었음을 의미합니다.
    2.  **L2B 스위치 단에서의 패킷 유실:** 06/17에는 L2B 스위치 단에서 유의미한 수준의 에러율(0.0487%, 0.0883%)이 관찰되었습니다. 이는 L2B 스위치에서 패킷이 드롭되어 VM에 도달하지 못하고 유실률에 직접적으로 기여했음을 시사합니다.

  * **NUMA Migration의 영향 감소:** 06/16에 비해 NUMA Migration 건수가 감소했음에도 불구하고 유실률은 오히려 증가했습니다. 이는 NUMA Migration이 유실률에 미치는 영향보다는 UDP 수신 버퍼 오버플로우와 L2B 스위치 에러가 더 큰 영향을 미치고 있음을 보여줍니다.

## 4\. 유실률과 관련된 추가 고려사항 및 심층 분석

### 4.1. on-premise baremetal 서버(nzeus10)의 UDP 수신 버퍼 오류 확인 필요성

  * "baremetal 서버에서는 app 처리속도가 병목이어서 udprcvbuferrors가 있을까?(데이터 파악이 안되어있음)" 라고 언급하셨습니다. **nzeus10의 `netstat -su` 또는 `/proc/net/snmp` 값을 확인하여 `UdpRcvbufErrors`가 발생하는지 여부를 파악하는 것이 매우 중요합니다.** 만약 nzeus10에서도 `UdpRcvbufErrors`가 발생한다면, 이는 애플리케이션 자체의 처리 병목이 유실의 근본 원인일 수 있다는 가설을 강화합니다.
  * **데이터 파악이 안되어있을 경우:** nzeus10 서버에 접속하여 `netstat -su` 명령을 실행하여 `Udp: ... receive buffer errors` 값을 주기적으로 모니터링해야 합니다.

### 4.2. 애플리케이션 처리 방식 심층 분석

  * "App은 baremetal과 VM에 동일한 App이 구동되고 있으며, 처리방식 등이 동일함." 이라고 하셨지만, VM 환경에서는 가상화 오버헤드, 자원 공유 등으로 인해 baremetal과 동일한 성능을 내기 어려울 수 있습니다.
  * **애플리케이션이 UDP 패킷을 처리하는 방식에 대한 상세 분석이 필요합니다.**
      * 패킷 수신 후 버퍼에서 데이터를 가져가는 빈도 및 방식 (Polling vs Interrupt)
      * 패킷 처리 로직의 복잡성 및 CPU 사용량
      * 멀티스레딩/멀티프로세싱 활용 여부 및 효율성
      * 애플리케이션 내부 큐 또는 버퍼 사용 여부 및 크기

### 4.3. UDP 수신 버퍼(`rmem_max`, `rmem_default`) 추가 증설 고려

  * 현재 32MB로 증설된 버퍼에서도 `UdpRcvbufErrors`가 발생하고 있으므로, **추가적인 증설을 고려해볼 수 있습니다.** 다만, 무한정 늘리는 것은 메모리 낭비를 초래하고 근본적인 해결책이 아닐 수 있습니다.
  * 버퍼를 늘리기 전에 애플리케이션이 현재 버퍼 크기를 제대로 활용하고 있는지, 그리고 버퍼가 가득 찼을 때 애플리케이션이 어떻게 동작하는지 (`SO_RCVBUF` 옵션 등) 확인해야 합니다.

### 4.4. NIC Rx Ring Buffer(`ethtool -g`) 재확인

  * NIC rx/tx ring buffer 크기를 4096으로 늘렸다고 하셨습니다. `ethtool -g <interface>` 명령으로 현재 설정된 ring buffer 크기와 최대 가능한 크기를 다시 한번 확인하여 실제로 적용되었는지, 그리고 더 늘릴 여지가 있는지 확인해야 합니다. (일반적으로 4096은 상당히 큰 값입니다.)

### 4.5. CPU Ready Time과 NUMA Migration의 연관성 및 임계치 재검토

  * CPU Ready 값이 낮다고 하지만, 미세한 CPU 경합이라도 고빈도 패킷 처리에 영향을 미칠 수 있습니다.
  * **NUMA Migration이 발생했을 때 정확히 어떤 영향을 주는지 상세 로깅을 통해 파악할 필요가 있습니다.** 예를 들어, NUMA Migration 발생 시점에 `UdpRcvbufErrors`가 급증하는 패턴이 있는지 확인해야 합니다.
  * 고객의 요구사항(유실률 0.1% 이하)을 만족하기 위해 CPU Ready Time 및 NUMA Migration의 임계치를 더 엄격하게 관리할 필요가 있을 수 있습니다.

### 4.6. L2B 스위치 심층 분석 및 모니터링 강화

  * 06/17에 L2B 스위치에서 발생한 에러는 심각한 문제입니다.
      * **L2B 스위치 벤더 및 모델 확인 후 해당 에러 카운터의 의미를 정확히 파악해야 합니다.** 어떤 종류의 에러이며, 왜 발생하는지 알아야 합니다. (예: ASIC buffer full, CRC error, input queue drop 등)
      * **L2B 스위치에 대한 모니터링을 강화하여 에러 발생 시점을 정확히 파악하고, 해당 시점에 트래픽 패턴 변화 등을 분석해야 합니다.**
      * L2B 스위치와 SCP Agg 스위치 간의 링크 상태, 포트 사용량, 버퍼 상태 등도 점검해야 합니다.

### 4.7. 트래픽 패턴 분석 (브로드캐스트/UDP)

  * 애플리케이션이 브로드캐스트, UDP 패킷을 많이 처리한다고 하셨습니다.
  * **패킷 분석 도구(예: Wireshark, tcpdump)를 사용하여 실제 트래픽 패턴을 분석하는 것이 중요합니다.**
      * 평균 패킷 크기
      * 초당 패킷 수 (PPS) 및 초당 바이트 수 (BPS)
      * 피크 시점의 트래픽 볼륨
      * 브로드캐스트 패킷의 비율 및 특이사항

### 4.8. ESXi VM vNIC(rxSummary)와 In-Guest vNIC(ethtool) 간의 불일치 여부 확인

  * 제공된 데이터에는 rxSummary와 ethtool 데이터가 충분하지 않아 명확히 비교하기 어렵습니다. `rxSummary`의 `pkts rx ok` 또는 `bytes rx ok` 값과 `ethtool`의 `rx_packets` 또는 `rx_bytes` 값을 비교하여 ESXi vNIC에서 In-Guest vNIC로 패킷이 전달되는 과정에서 유실이 없는지 확인해야 합니다.
  * 파일에 있는 `pkts rx ok`, `bytes rx ok`는 VM 내부의 `rxSummary` 통계로 보입니다. ESXi 호스트 관점의 `rxSummary` 데이터가 추가로 필요합니다.

## 5\. 06/16 vs 06/17 비교 분석 및 종합 결론

| 항목 | 06/16 07:55 \~ 09:55 | 06/17 07:55 \~ 09:55 | 분석 및 유의미한 내용 |
|---|---|---|---|
| **VM 유실률 (최대)** | mca5102: 0.1585% | mca5102: 0.2359% | 06/17에 유실률이 전반적으로 증가하여 고객 요구사항(0.1% 이하)을 만족하지 못함. |
| **VM UDP RcvbufErrors** | mca5102: 9,880 \<br\> mca5204: 2,195 \<br\> mca5304: 8,548 | mca5102: 38,306 \<br\> mca5204: 19,655 \<br\> mca5304: 22,851 | 06/17에 모든 VM에서 UDP 수신 버퍼 오류가 크게 증가. 이는 애플리케이션이 버퍼에서 데이터를 제때 가져가지 못하는 문제가 심화되었음을 나타냄. 유실률 증가의 가장 큰 원인. |
| **ESXi droppedRx** | 0 | 0 | ESXi 호스트 레벨에서의 패킷 드롭은 발생하지 않음. ESXi에서 VM vNIC까지는 패킷이 정상 전달됨. |
| **L2B 시간내 에러율** | 07:55: 2.1367% (이후 0%) | 09:15: 0.0487% \<br\> 09:55: 0.0883% | 06/16 7:55 시점의 높은 에러율은 이전 누적분으로 추정되나, 06/17에는 9시 이후 L2B 단에서 실제 유실이 발생. 이는 VM의 유실률 증가에 직접적인 영향을 미쳤을 것. L2B 스위치의 버퍼 또는 처리량 문제 가능성. |
| **NUMA Migration (mca5102)** | 18 | 9 | 06/17에 NUMA Migration이 감소했음에도 유실률이 증가. NUMA Migration이 유실에 영향을 미치긴 하지만, UDP 버퍼 오류와 L2B 스위치 에러가 더 큰 영향을 미치고 있음을 시사. |
| **CPU Ready** | \< 0.34 | \< 0.35 | CPU 경합은 낮은 수준으로, 유실의 주요 원인은 아닐 것으로 보이나, 고빈도 패킷 처리에는 미미한 영향도 중요할 수 있음. |

**종합 결론:**

두 날짜의 데이터를 비교했을 때, 애플리케이션 단의 유실률이 증가한 06/17의 경우 **In-Guest VM의 UDP 수신 버퍼 오버플로우(`UdpRcvbufErrors`)가 크게 증가했고, 동시에 L2B 스위치 단에서 패킷 유실이 발생한 것**이 유실률 악화의 주요 원인으로 판단됩니다. ESXi 호스트 레벨에서의 드롭은 없었습니다.

**가장 유의미한 내용은 `UdpRcvbufErrors`의 지속적인 발생과 증가입니다.** 이는 증권 시세와 같은 고빈도 UDP 패킷 처리 환경에서, 현재의 시스템 자원 및 애플리케이션 처리 로직이 들어오는 데이터를 충분히 빠르게 처리하지 못하고 있음을 명확하게 보여줍니다. `rmem_max` 및 `rmem_default`를 늘렸음에도 발생하는 오류는 애플리케이션 자체의 병목일 가능성이 매우 높습니다.

또한, 06/17에 L2B 스위치에서 발생한 에러는 스위치 레벨에서의 새로운 유실 지점을 의미하며, 이 또한 유실률 증가에 기여했습니다. 이 스위치 에러의 근본적인 원인 파악이 시급합니다.

## 6\. 정확한 원인 파악을 위해 추가로 파악해야 할 내용

1.  **on-premise baremetal 서버(nzeus10)의 `netstat -su` 결과 확인:** `UdpRcvbufErrors` 및 `UdpInErrors` 값이 존재하는지, 존재한다면 어느 정도 수준인지 파악하여 애플리케이션 자체의 병목 여부 판단.
2.  **VM의 `ethtool -S <interface>` 결과 확인:** VM 내부 vNIC의 상세 통계(특히 `rx_dropped`, `rx_missed_errors`, `rx_queue_0_drops` 등)를 확인하여 Guest OS 드라이버 레벨에서 드롭되는 패킷이 있는지 재확인. (제공된 `mca5102-0616.txt`, `mca5102-0617 (1).txt` 파일에는 `ethtool` 결과가 충분히 상세하게 나와있지 않음.)
3.  **VM 애플리케이션 레벨의 로깅 및 메트릭스 확인:** 애플리케이션 자체에서 수신하는 패킷 수, 처리 시간, 내부 큐 상태 등을 로깅하여 애플리케이션 내부 병목 지점을 정확히 식별해야 합니다.
4.  **L2B 스위치 벤더 및 모델에 따른 에러 카운터 상세 분석:** `시간내 에러카운트`가 어떤 종류의 에러를 나타내는지 벤더 문서 등을 통해 정확히 파악해야 합니다. (예: Drop due to full buffer, CRC error, alignment error 등)
5.  **L2B 스위치 포트별 통계 및 버퍼 사용량 모니터링:** 특정 포트에서만 에러가 발생하는지, 스위치 내부 버퍼가 가득 차는 현상이 있는지 확인해야 합니다.
6.  **ESXi VM의 vNIC 설정 및 드라이버 버전 확인:** VMware Tools 설치 여부, vNIC 드라이버 버전(VMXNET3 권장), Rx/Tx Ring Buffer 설정이 ESXi와 VM 내부에서 일치하는지, 최적화되어 있는지 확인.
7.  **트래픽 피크 타임과 유실률의 상관관계 분석:** 07:55\~09:55 구간 내에서도 특정 시점에 트래픽이 급증하고 유실률이 높아지는지 확인하여, 해당 시간대의 시스템 부하를 면밀히 분석해야 합니다.
8.  **VPC 간 네트워크 경로 확인:** 6개의 VM이 3개의 VPC에 분산되어 있다고 하셨으므로, 각 VPC의 네트워크 구성 및 게이트웨이, 방화벽 등에서 잠재적인 병목이나 유실이 발생할 수 있는 요소를 검토해야 합니다.

## 7\. 권고사항

  * **가장 우선적으로는 nzeus10의 `UdpRcvbufErrors` 확인 및 L2B 스위치 에러 원인 파악이 필요합니다.** 이 두 가지가 현재 유실률의 가장 큰 기여 요인으로 보입니다.
  * **애플리케이션 개발팀과의 협의를 통해 UDP 수신 버퍼에서 데이터를 가져가는 로직을 최적화하거나, 애플리케이션 자체의 처리 성능을 개선하는 방안을 모색해야 합니다.** 버퍼 크기를 무작정 늘리는 것보다 애플리케이션의 처리 속도를 높이는 것이 근본적인 해결책입니다.
  * **L2B 스위치에 대한 심층 진단 및 벤더 지원을 요청하여 에러의 원인을 규명하고 해결해야 합니다.**
  * `net.core.rmem_max`를 32MB 이상으로 추가 증설하는 것을 시도해볼 수 있지만, 이는 임시 방편이 될 가능성이 높습니다. 증설 후에도 `UdpRcvbufErrors`가 지속되는지 모니터링해야 합니다.
  * CPU Ready가 낮더라도, 고빈도 트래픽 처리 환경에서는 vCPU 수를 늘리거나 ESXi 호스트의 부하를 분산하는 등의 추가적인 자원 확보 방안도 고려할 수 있습니다. 다만, 이는 현재로서는 최우선 과제가 아닙니다.
  * NUMA Migration 발생 시점에 유실률이 증가하는 패턴이 관찰된다면, VM의 NUMA 선호도 설정 (`numa.nodeAffinity` 등)을 통해 NUMA Migration을 최소화하는 방안을 고려해 볼 수 있습니다.

이 분석은 제공된 데이터를 기반으로 하며, 추가 데이터 및 상세 모니터링을 통해 더욱 정확한 원인 파악이 가능할 것입니다.
