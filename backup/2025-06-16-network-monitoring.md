---
title: "네트워크 패킷 유실률 분석"
date: 2025-06-16
tags: [network, netstat, ethtool, rxSummary, stats]
categories: [Network, Analysis]
---

# 네트워크 패킷 유실률 분석 접근법 개요
패킷 유실은 네트워크 스택의 여러 계층(하드웨어, 가상화 계층, OS 네트워크 스택, 애플리케이션)에서 발생할 수 있습니다. 따라서 주어진 통계값들을 **계층별로 묶어 분석**하고, **단계 간 비교**를 통해 유실이 발생하는 지점을 특정하며, **패턴과 상관관계**를 찾아내는 것이 핵심입니다. 아래에서는 VM(`netstat`, `ethtool`), ESXi(`rxSummary`, `stats`)에서 제공된 통계값들을 기반으로 분석 방법을 상세히 설명합니다.

---

## 1. VM netstat 통계값 분석
`netstat`은 주로 OS의 네트워크 스택에서 패킷 처리 상태를 보여줍니다. 여기서 제공된 통계값들을 하나씩 분석하고, 이를 통해 얻을 수 있는 인사이트를 살펴보겠습니다.

### **IpExt InBcastPkts (수신된 브로드캐스트 패킷 수)**
- **의미**: 네트워크 스택이 하위 계층(NIC)에서 성공적으로 수신한 브로드캐스트 패킷의 수.
- **분석 방법**: 
  - 이 값이 증가하면 NIC에서 OS로 브로드캐스트 패킷이 전달되고 있음을 의미합니다. 
  - 하지만 기대했던 브로드캐스트 트래픽 양(예: 네트워크 모니터링 툴로 확인한 값)보다 적다면, NIC나 그 하위 계층에서 유실이 발생했을 가능성이 있습니다.
- **인사이트**: 이 값과 후술할 `ethtool`의 `bcast pkts rx`를 비교해 NIC에서 OS로 넘어오는 과정에서 유실이 있는지 확인합니다.

### **UdpRcvbufErrors (UDP 수신 버퍼 에러)**
- **의미**: UDP 수신 버퍼가 가득 차서 패킷을 드롭한 횟수.
- **분석 방법**: 
  - 이 값이 증가하면 OS가 패킷을 수신했으나 애플리케이션이 이를 제때 처리하지 못해 버퍼가 넘친 상황입니다.
  - CPU 사용률(`top`)이나 애플리케이션 로그를 확인해 처리 지연의 원인을 조사합니다.
- **인사이트**: 브로드캐스트 패킷이 UDP로 전달되는 경우(예: DHCP, ARP), 이 값이 높으면 애플리케이션 병목현상이 유실의 원인일 수 있습니다.

### **Ip InDiscards (IP 패킷 폐기 수)**
- **의미**: IP 계층에서 처리되지 못하고 폐기된 패킷 수(버퍼 부족, 오류 등).
- **분석 방법**: 
  - 이 값이 증가하면 네트워크 스택 내부에서 패킷이 드롭되고 있음을 나타냅니다.
  - `IpExt InBcastPkts`와 비교해 브로드캐스트 패킷이 포함된 전체 트래픽 중どれほど가 폐기되었는지 비율을 계산합니다.
- **인사이트**: 브로드캐스트 트래픽이 많은 환경에서 이 값이 높다면, OS의 IP 스택이 과부하 상태일 가능성이 큽니다.

### **Udp InErrors (UDP 수신 에러)**
- **의미**: UDP 패킷 수신 중 발생한 에러 수(예: 체크섬 오류, 포트 오류).
- **분석 방법**: 
  - 이 값이 증가하면 UDP 패킷에 문제가 있거나 스택에서 처리 중 오류가 발생했음을 의미합니다.
  - `Udp packets received`와 비교해 에러 비율을 계산합니다.
- **인사이트**: 브로드캐스트 UDP 패킷(예: 네트워크 디스커버리 프로토콜)에 문제가 있다면, 네트워크 환경(노이즈, 충돌)이나 송신 측 문제를 의심할 수 있습니다.

### **Udp packets received (수신된 UDP 패킷 수)**
- **의미**: OS가 성공적으로 수신한 UDP 패킷의 총 수.
- **분석 방법**: 
  - 이 값은 브로드캐스트와 유니캐스트를 모두 포함하므로, `IpExt InBcastPkts`와 함께 비교해 브로드캐스트 비중을 파악합니다.
  - `Udp InErrors`와 대조해 정상 처리된 패킷과 손실된 패킷의 비율을 분석합니다.
- **인사이트**: 이 값이 낮고 에러가 높다면 UDP 계층에서 유실이 발생하고 있음을 시사합니다.

### **Udp packet receive errors (UDP 패킷 수신 에러)**
- **의미**: UDP 수신 중 발생한 모든 에러의 총합(포함 범위는 `Udp InErrors`와 유사할 수 있음).
- **분석 방법**: 
  - `Udp InErrors`와 중복 여부를 확인하고, 차이가 있다면 추가 에러 유형(버퍼 오버플로우 등)을 조사합니다.
- **인사이트**: 이 값이 높으면 UDP 기반 브로드캐스트 트래픽 처리에 문제가 있음을 나타냅니다.

---

## 2. VM ethtool 통계값 분석
`ethtool`은 NIC 하드웨어 레벨의 통계를 제공하며, 물리적 계층에서의 유실을 파악하는 데 유용합니다.

### **drv dropped rx total (드라이버에서 드롭된 수신 패킷 총합)**
- **의미**: NIC 드라이버가 처리하지 못해 드롭한 패킷 수.
- **분석 방법**: 
  - 이 값이 증가하면 NIC 하드웨어 또는 드라이버가 패킷을 처리하지 못한 상황입니다.
  - `rx buf alloc fail`과 연계해 원인을 특정합니다.
- **인사이트**: 브로드캐스트 패킷이 많은 경우 이 값이 높다면, NIC가 트래픽을 감당하지 못하고 있음을 의미합니다.

### **rx buf alloc fail (수신 버퍼 할당 실패)**
- **의미**: NIC가 수신 버퍼를 할당하지 못해 패킷을 드롭한 횟수.
- **분석 방법**: 
  - 이 값이 증가하면 NIC의 메모리 부족이나 과부하가 원인입니다.
  - 시스템 메모리 상태(`free`)를 점검해 간접적인 영향을 확인합니다.
- **인사이트**: 브로드캐스트 트래픽 급증 시 이 값이 높아진다면, NIC의 버퍼 크기를 늘리거나 트래픽을 분산해야 합니다.

### **pkts rx err (수신 패킷 에러)**
- **의미**: NIC에서 수신 중 에러가 발생한 패킷 수(예: CRC 오류).
- **분석 방법**: 
  - 이 값이 증가하면 물리적 네트워크 문제(케이블, 스위치)나 간섭을 의심해야 합니다.
- **인사이트**: 브로드캐스트 패킷에 특화된 문제라면 네트워크 환경을 점검해야 합니다.

### **bcast pkts rx (수신된 브로드캐스트 패킷 수)**
- **의미**: NIC가 수신한 브로드캐스트 패킷 수.
- **분석 방법**: 
  - `netstat`의 `IpExt InBcastPkts`와 비교해 NIC에서 OS로 전달 과정에서 유실이 있는지 확인합니다.
  - ESXi의 `broadcast pkts rx ok`와도 비교해 가상화 계층과의 일관성을 점검합니다.
- **인사이트**: 이 값이 기대보다 낮다면 NIC 레벨에서 유실이 발생하고 있음을 시사합니다.

---

## 3. ESXi rxSummary 통계값 분석
ESXi의 `rxSummary`는 가상 네트워크 계층에서의 수신 상태를 보여줍니다.

### **broadcast pkts rx ok (정상 수신된 브로드캐스트 패킷 수)**
- **의미**: VM으로 전달되기 전 가상 스위치가 정상적으로 수신한 브로드캐스트 패킷 수.
- **분석 방법**: 
  - `ethtool`의 `bcast pkts rx`와 비교해 가상 스위치에서 NIC로의 전달 과정에서 유실이 있는지 확인합니다.
- **인사이트**: 이 값이 높고 NIC에서 낮다면, VM 내부에서 유실이 발생한 것입니다.

### **droppedRx (드롭된 수신 패킷 수)**
- **의미**: 가상 스위치에서 VM으로 전달되지 못하고 드롭된 패킷 수.
- **분석 방법**: 
  - 이 값이 증가하면 가상 네트워크 계층에서 병목현상이 있음을 의미합니다.
  - VM의 CPU/메모리 사용량과 연계해 자원 부족 여부를 확인합니다.
- **인사이트**: 브로드캐스트 패킷이 포함된 경우, 이 값이 높으면 가상 스위치 설정(예: MTU, 버퍼)을 점검해야 합니다.

---

## 4. ESXi stats 통계값 분석
ESXi의 `stats`는 VM과 가상 스위치 간 상호작용을 보여줍니다.

### **pktsRxBroadcast (VM이 수신한 브로드캐스트 패킷 수)**
- **의미**: VM이 가상 스위치로부터 성공적으로 수신한 브로드캐스트 패킷 수.
- **분석 방법**: 
  - `rxSummary`의 `broadcast pkts rx ok`와 비교해 가상 스위치에서 VM으로의 전달 유실을 확인합니다.
  - `ethtool`의 `bcast pkts rx`와도 비교해 VM 내부 처리 상태를 점검합니다.
- **인사이트**: 이 값이 낮고 상위 계층에서 높다면, VM의 네트워크 스택에서 문제가 발생했을 가능성이 있습니다.

---

## 종합 분석 및 인사이트 도출
이제 각 계층의 통계값을 연결해 패킷 유실의 원인을 특정하는 방법을 정리하겠습니다.

### **단계별 비교 분석**
1. **ESXi → VM NIC**:
   - `broadcast pkts rx ok` (ESXi) → `bcast pkts rx` (ethtool): 가상 스위치에서 NIC로의 전달 유실 확인.
   - `droppedRx` (ESXi)가 높으면 가상 네트워크 병목을 의심.

2. **NIC → OS 네트워크 스택**:
   - `bcast pkts rx` (ethtool) → `IpExt InBcastPkts` (netstat): NIC에서 OS로의 전달 유실 확인.
   - `drv dropped rx total` 또는 `rx buf alloc fail`이 높으면 NIC 하드웨어 문제를 점검.

3. **OS → 애플리케이션**:
   - `IpExt InBcastPkts` (netstat) → 애플리케이션 로그: OS에서 애플리케이션으로의 전달 유실 확인.
   - `UdpRcvbufErrors`가 높으면 애플리케이션 처리 속도나 버퍼 크기를 조정.

### **패턴 분석**
- **브로드캐스트 특화 문제 여부**: `IpExt InBcastPkts`와 `Udp packets received`를 비교해 브로드캐스트 패킷만 유실되는지 확인.
- **자원 병목 여부**: `droppedRx`, `rx buf alloc fail`, `UdpRcvbufErrors`가 동시에 증가하면 CPU/메모리 부족을 의심.
- **하드웨어 문제 여부**: `pkts rx err`나 `Udp InErrors`가 높으면 물리적 네트워크 상태를 점검.

### **실제 시사점 예시**
- **시나리오 1**: `broadcast pkts rx ok`는 높으나 `bcast pkts rx`가 낮음 → VM NIC 설정(버퍼 크기, 드라이버) 문제.
- **시나리오 2**: `IpExt InBcastPkts`는 정상인데 `UdpRcvbufErrors`가 높음 → 애플리케이션 처리 지연.
- **시나리오 3**: `droppedRx`와 `drv dropped rx total`이 증가 → 가상 네트워크와 NIC 모두 과부하.

---

## **네트워크 패킷 유실률 분석을 위한 통계값 정리**

| **계층**             | **통계값**                   | **의미**                                                                 | **유실률 분석을 위한 비교**                                                                 | **패턴 분석 및 인사이트**                                                                 |
|----------------------|------------------------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|
| **ESXi (가상 네트워크)** | `broadcast pkts rx ok`       | 가상 스위치가 정상적으로 수신한 브로드캐스트 패킷 수                     | `broadcast pkts rx ok` → `bcast pkts rx` (ethtool): 가상 스위치 → VM NIC 전달 유실 확인     | `broadcast pkts rx ok` 높고 `bcast pkts rx` 낮으면 VM NIC 설정 문제 (버퍼 크기, 드라이버) |
|                      | `droppedRx`                  | 가상 스위치에서 VM으로 전달되지 못하고 드롭된 패킷 수                   | `droppedRx` 높으면 가상 네트워크 병목 확인 (VM 자원 부족 점검)                              | `droppedRx`와 `drv dropped rx total` 동시 증가 시 가상 네트워크와 NIC 과부하              |
|                      | `pktsRxBroadcast`            | VM이 가상 스위치로부터 성공적으로 수신한 브로드캐스트 패킷 수            | `pktsRxBroadcast` → `bcast pkts rx` (ethtool): VM 내부 처리 유실 확인                       | `pktsRxBroadcast` 낮고 상위 계층 높으면 VM 네트워크 스택 문제                              |
| **VM NIC (ethtool)** | `bcast pkts rx`              | NIC가 수신한 브로드캐스트 패킷 수                                        | `bcast pkts rx` → `IpExt InBcastPkts` (netstat): NIC → OS 전달 유실 확인                   | `bcast pkts rx` 낮으면 NIC 레벨 유실 발생                                                |
|                      | `drv dropped rx total`       | 드라이버가 처리하지 못해 드롭한 수신 패킷 총합                           | `drv dropped rx total` 높으면 NIC 하드웨어 또는 드라이버 문제 확인                           | `drv dropped rx total` 높으면 NIC 트래픽 감당 불가 (브로드캐스트 과부하)                   |
|                      | `rx buf alloc fail`          | 수신 버퍼 할당 실패로 드롭된 패킷 수                                    | `rx buf alloc fail` 높으면 NIC 메모리 부족 또는 과부하 확인                                  | `rx buf alloc fail` 높으면 NIC 버퍼 크기 조정 또는 트래픽 분산 필요                        |
|                      | `pkts rx err`                | 수신 중 에러 발생 패킷 수 (예: CRC 오류)                                | `pkts rx err` 높으면 물리적 네트워크 문제 (케이블, 스위치) 확인                              | `pkts rx err` 높으면 네트워크 환경 문제 (브로드캐스트 관련 문제 여부 확인)                 |
| **VM OS (netstat)**  | `IpExt InBcastPkts`          | 네트워크 스택이 수신한 브로드캐스트 패킷 수                              | `bcast pkts rx` (ethtool) → `IpExt InBcastPkts`: NIC → OS 전달 유실 확인                    | `IpExt InBcastPkts`가 `bcast pkts rx`보다 적으면 OS 내부 유실 발생                         |
|                      | `UdpRcvbufErrors`            | UDP 수신 버퍼 가득 차 드롭된 패킷 수                                    | `UdpRcvbufErrors` 높으면 애플리케이션 처리 지연 확인 (CPU, 메모리 점검)                     | `UdpRcvbufErrors` 높으면 애플리케이션 병목으로 유실 발생                                   |
|                      | `Ip InDiscards`              | IP 계층에서 처리되지 못하고 폐기된 패킷 수                              | `Ip InDiscards` 높으면 OS의 IP 스택 과부하 확인                                              | `Ip InDiscards` 높으면 브로드캐스트 트래픽 과다로 OS 과부하                                |
|                      | `Udp InErrors`               | UDP 패킷 수신 중 발생한 에러 수 (예: 체크섬 오류)                       | `Udp InErrors` 높으면 네트워크 환경 또는 송신 측 문제 확인                                   | `Udp InErrors` 높으면 브로드캐스트 UDP 패킷 문제 (네트워크 노이즈, 충돌 등)                |
|                      | `Udp packets received`       | OS가 성공적으로 수신한 UDP 패킷 수                                       | `Udp packets received`와 `Udp InErrors` 비교: 정상 처리 vs. 손실 비율 확인                   | `Udp packets received` 낮고 에러 높으면 UDP 계층 유실 발생                                |
|                      | `Udp packet receive errors`  | UDP 수신 중 발생한 모든 에러의 총합                                      | `Udp packet receive errors` 높으면 UDP 처리 문제 확인                                        | `Udp packet receive errors` 높으면 UDP 기반 브로드캐스트 트래픽 처리 문제                  |

---

### **표 활용 방법**
1. **계층별 통계값 확인**: 각 통계값이 어느 계층에서 어떤 의미를 가지는지 파악하세요.
2. **유실률 분석**: 표의 "유실률 분석을 위한 비교" 열을 참고하여 계층 간 통계값을 비교하며 유실 지점을 특정하세요.
   - 예: `broadcast pkts rx ok` (ESXi)와 `bcast pkts rx` (ethtool)을 비교해 가상 스위치에서 VM NIC으로의 유실을 확인.
3. **패턴 분석**: "패턴 분석 및 인사이트" 열을 통해 특정 패턴이 나타날 때의 원인과 해결 방안을 도출하세요.
   - 예: `UdpRcvbufErrors`가 높으면 애플리케이션 처리 지연이 원인일 가능성 있음.

---

## 결론
제공된 통계값들을 계층별로 분석하고, 단계 간 비교를 통해 유실 지점을 특정하며, 자원 상태와 연계해 패턴을 파악하면 다음과 같은 인사이트를 얻을 수 있습니다:
- **정확한 유실 위치**: 하드웨어, 가상화, OS, 애플리케이션 중 어디에서 문제가 발생했는지.
- **근본 원인**: 자원 부족, 설정 오류, 네트워크 환경 문제 등.
- **해결 방향**: NIC 버퍼 조정, 애플리케이션 최적화, 네트워크 재구성 등.

이 접근법을 통해 브로드캐스트 패킷 유실 문제를 체계적으로 진단하고, 네트워크 성능을 개선할 수 있는 실질적인 단서를 얻을 수 있습니다.
