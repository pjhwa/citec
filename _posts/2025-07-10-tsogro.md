---
title: "리눅스 네트워크 스택에서 TSO, RRO의 상호 작용 이해"
date: 2025-07-10
tags: [linux, network, tso, gro, ethtool, wireshark, tcpdump]
categories: [Howtos, Linux]
---


## 질문
환경: Ubuntu 22.04, PowerEdge XE9680, BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet [14e4:1751] (rev 12)

리눅스 네트워크 스택에서 TSO (TCP Segmentation Offload)가 활성화된 상태에서 GRO (Generic Receive Offload)를 활성화하면, 패킷 덤프 도구 (예: tcpdump 또는 Wireshark)에서 TSO가 적용되지 않은 것처럼 패킷이 작은 크기로 분할되어 보이는 현상이 발생하는데, 이는 왜 그런가? TSO와 GRO의 상호 작용, NIC (Network Interface Card)의 offload 처리 과정, 패킷 캡처 지점 (예: 네트워크 스택 내 위치)에 따른 차이, 그리고 관련 ethtool 설정이나 커널 동작을 고려하여 사실 기반으로 자세히 설명해 주세요. 비판적 검증을 통해 가능한 오해 (예: TSO가 실제로 비활성화되는지 여부)를 명확히 하고, 예시나 참조 자료를 포함하세요.

## 답변

리눅스 네트워크 스택에서 TSO (TCP Segmentation Offload)와 GRO (Generic Receive Offload)는 각각 송신과 수신 측의 성능 최적화를 위한 기능입니다. 사용자의 질문처럼 TSO가 활성화된 상태에서 GRO를 활성화하면 패킷 덤프 도구 (예: tcpdump 또는 Wireshark)에서 TSO가 비활성화된 것처럼 패킷이 작은 크기로 분할되어 보이는 현상이 발생할 수 있습니다. 이는 TSO가 실제로 비활성화되는 것이 아니라, GRO로 인한 TCP 스택의 오동작이 간접적으로 송신 패킷 크기를 제한하기 때문입니다. 아래에서 사실 기반으로 단계적으로 설명하겠습니다. 설명은 이해하기 쉽게 구조화하되, 비판적 검증을 통해 오해를 명확히 하고, 예시 및 참조 자료를 포함합니다.

### 1. TSO와 GRO의 기본 동작 및 상호 작용
- **TSO (TCP Segmentation Offload)**: 송신 측 offload 기능으로, TCP 스택이 큰 데이터 블록 (예: 64KB)을 NIC (Network Interface Card) 드라이버에 넘기면 NIC 하드웨어가 MTU (Maximum Transmission Unit, 보통 1500바이트) 크기로 자동 분할합니다. 이는 CPU 부하를 줄여 고속 네트워킹 (예: 10Gbps 이상)에서 유용합니다. 
  - 캡처 지점: tcpdump나 Wireshark는 PF_PACKET 소켓을 통해 NIC 드라이버 직전에서 패킷을 캡처하므로, TSO가 활성화되면 송신 패킷이 큰 크기 (MTU 초과)로 보입니다. 실제 와이어 상에서는 작은 패킷으로 분할됩니다.
  - 관련 설정: `ethtool -K <interface> tso on/off`로 제어. 예: `ethtool -K eth0 tso on`.

- **GRO (Generic Receive Offload)**: 수신 측 offload 기능으로, NIC 드라이버가 여러 작은 패킷을 하나의 큰 패킷으로 합쳐 (coalesce) TCP 스택에 넘깁니다. LRO (Large Receive Offload)의 일반화 버전으로, 유사한 성능 향상을 제공합니다.
  - 캡처 지점: GRO는 드라이버 수준에서 발생하므로, tcpdump는 합쳐진 큰 패킷을 캡처합니다. GRO가 비활성화되면 와이어 상의 작은 패킷 그대로 보입니다.
  - 관련 설정: `ethtool -K <interface> gro on/off`로 제어. 예: `ethtool -K eth0 gro on`.

- **상호 작용**: TSO와 GRO는 독립적이지만, TCP 연결에서 상호 영향을 줍니다. 송신 측 TSO가 큰 데이터를 보내도, 수신 측 GRO가 패킷을 합치면 TCP 스택이 수신 패킷 크기를 기반으로 MSS (Maximum Segment Size)나 TCP window 크기를 계산합니다. 여기서 문제가 발생: **GRO로 합쳐진 패킷 크기를 기반으로 MSS 추정이 잘못되면 TCP window가 비정상적으로 작아집니다 (window clamping). 결과적으로 송신 측이 작은 window를 광고받아 작은 패킷만 보내게 되고, TSO가 적용될 여지가 없어집니다.**

- **NIC offload 처리 과정**: 
  - 송신: TCP 스택 → 큰 세그먼트 → 드라이버 (GSO: Generic Segmentation Offload로 소프트웨어 지원) → NIC 하드웨어 분할 → 와이어.
  - 수신: 와이어 → NIC → 드라이버 (GRO 합침) → TCP 스택.
  - 커널 동작: 리눅스 커널 (예: 4.x 이상)에서 offload는 netdev_features로 관리되며, ethtool로 확인/설정 가능. 루프백 인터페이스 (lo)에서도 지원되지만, 가상 인터페이스라 실제 분할 없이 큰 MTU (보통 65536바이트)로 동작합니다.

### 2. 패킷 캡처 지점에 따른 차이와 현상 원인
패킷 덤프에서 TSO가 "비활성화된 것처럼" 보이는 이유는 GRO 활성화로 인한 TCP 스택의 MSS 추정 오류입니다. 이는 Red Hat와 같은 배포본에서 문서화된 알려진 문제로, 사실 기반으로 검증됩니다.

- **캡처 지점 영향**:
  - 송신 측 캡처: TSO on 시 큰 패킷 (MTU 초과)으로 보임. GRO는 수신 offload라 송신 캡처에 직접 영향 없음.
  - 수신 측 캡처: TSO on인 송신으로부터 작은 패킷 도착 → GRO on 시 합쳐져 큰 패킷으로 보임. 하지만 GRO가 MSS 계산을 왜곡하면 TCP ACK에서 작은 window를 송신 측에 알림.
  - 결과: 송신 측이 작은 window를 받으면 작은 데이터만 보내 (예: 720바이트), TSO가 적용되지 않음. tcpdump에서 작은 패킷으로 보임 (TSO off처럼).

- **왜 GRO 활성화 시 이런 현상?**
  - GRO on 시, 드라이버가 패킷을 합쳐 raw 패킷 크기를 기반으로 MSS를 추정하지만, 이는 실제 와이어 패킷 크기와 다름. TCP 스택이 혼란스러워 window가 작아짐 (clamping to small value).
  - 비판적 검증: 이는 TSO가 실제 비활성화되는 게 아닙니다. `ethtool -k <interface>`로 확인하면 TSO는 여전히 on 상태. 단지 작은 window로 인해 TSO가 무의미해짐. 오해: "TSO가 꺼진 것 같다"는 착시로, 실제 원인은 GRO-induced window issue. 이 문제는 고속 링크 (10Gbps+)나 NFS 같은 애플리케이션에서 자주 발생, 성능 저하 (e.g., 1Gbps로 제한)와 함께 관찰됨.
  - 루프백 테스트 시 더 두드러짐: lo 인터페이스에서 송/수신이 동일 스택이라 GRO 오류가 즉시 피드백되어 작은 패킷으로 보임.

- **ethtool 설정과 커널 동작 고려**:
  - 확인 명령: `ethtool -k eth0` (offload 상태 출력, e.g., "tcp-segmentation-offload: on", "generic-receive-offload: on").
  - 비활성화 예: `ethtool -K eth0 gro off`로 GRO 끄면 window 정상화, TSO가 큰 패킷으로 적용됨.
  - 커널: 리눅스 2.6.32+에서 GRO 도입, but 버그 픽스 (e.g., RHEL 7+ 패치) 필요. /etc/modprobe.d/에 모듈 옵션 (e.g., "options bnx2x disable_tpa=1")으로 영구 설정.

### 3. 가능한 오해 명확히 및 비판적 검증
- **오해 1: TSO가 실제 비활성화됨?** No. TSO는 여전히 on, but 작은 TCP window로 인해 큰 데이터 송신이 제한되어 offload가 적용되지 않음. 검증: tcpdump 중 `ethtool -k <interface>` 실행해 상태 확인. window clamping은 Wireshark의 "TCP Graphs" (Window Scaling Graph)에서 작은 window 값으로 증명됨.
- **오해 2: 캡처 도구 오류?** No. tcpdump/Wireshark는 정확히 캡처하지만, offload 지점 (드라이버 전/후)에 따라 다르게 보임. 외부 캡처 (SPAN port나 tap device)로 wire 패킷 확인하면 small packets 보임 (TSO 적용 증거).
- **오해 3: 모든 환경에서 발생?** No. 저속 링크나 GRO 호환 NIC (e.g., Intel ixgbe)에서는 덜 발생. VM (KVM/ESXi)에서 더 흔함, as offload가 hypervisor에 위임됨. 검증: Broadcom NIC (bnx2x)에서 자주 보고됨; disable GRO로 해결.
- 비판적 관점: 이 현상은 offload의 trade-off (성능 vs. 정확성). 문서 (e.g., kernel.org)에서 offload는 "편의"지만, 분석 시 disable 권장. 주관적 소스 (포럼)는 bias될 수 있으나, Red Hat 같은 공식 자료로 교차 검증.

### 4. 예시
- **테스트 시나리오**: 10Gbps NIC에서 NFS 서버-클라이언트 (같은 호스트 루프백 또는 로컬 네트워크).
  - TSO on, GRO off: tcpdump shows large packets on sender (e.g., 64KB TCP len), high throughput.
  - TSO on, GRO on: tcpdump shows small packets (e.g., 720바이트 window clamp), throughput drop to 100MB/s. Wireshark에서 TCP ACK의 window field가 작음 (e.g., 720).
  - 로그 예 (tcpdump -i eth0 -nnvv): GRO on 시 "tcp.len=720" 반복, retransmissions 증가.
- **해결 예**: `ethtool -K eth0 gro off` 후 재테스트. window 정상 (e.g., 64KB), large packets 복귀.

### 5. 참조 자료
- Red Hat 공식 솔루션: NICs with RX acceleration (GRO/LRO) may suffer from bad TCP performance. GRO로 인한 MSS miscalculation 설명. https://access.redhat.com/solutions/20278
- Kernel Documentation: segmentation-offloads.txt. TSO/GSO/GRO 동작 상세. (kernel.org에서 다운로드).
- PacketBomb 블로그: How Can the Packet Size Be Greater than the MTU? TSO 캡처 영향 설명. 
- Stack Overflow: Why Linux server is responding with TCP packets larger than MSS. TSO 캡처 예시. 
- NetworkDataPedia: Analyzing TCP Segmentation Offload (TSO) with Wireshark. 캡처 비교. 

이 설명은 검색된 사실 기반 자료를 종합한 것으로, 실제 환경에서 `ethtool`과 `tcpdump`로 재현 테스트 추천합니다. 추가 질문 있으시면 말씀해주세요.
