---
title: "baremetal block storage checklist (draft)"
date: 2025-09-16
tags: [storage, block, baremetal, cloud, checklist]
categories: [Howtos, Storage]
---

## 구성

| 점검내용 | 점검방법 | 점검기준 | 취약시 문제점 | 개선방안 | 참고 |
|----------|----------|----------|---------------|----------|------|
| 볼륨 디스크 타입 설정 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: disk_type이 'ssd' 또는 'hdd' 중 업무 요구사항에 맞는 타입임<br>취약: disk_type이 업무 요구사항과 불일치하거나 undefined임 | 디스크 타입이 업무 워크로드에 맞지 않으면 IOPS 성능 저하 또는 비용 과다 발생, 데이터 처리 지연으로 서비스 품질 저하 발생함 | 업무 워크로드에 따라 disk_type을 'ssd'로 변경하여 고성능 요구 충족함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep disk_type<br>* 설정방법<br>볼륨 재생성 시 disk_type 지정: scpcli baremetal-blockstorage volume create --size_gb 10 --name my-bs-01 --disk_type ssd --attachments '[{"object_id": "server_id", "object_type": "BM"}]' <br>* References<br>- Samsung SDS Cloud Block Storage Overview: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/ (디스크 타입 선택 가이드 포함) |
| 볼륨 용량 설정 적절성 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: size_gb가 현재 사용량의 120% 이상 여유 공간 확보함<br>취약: size_gb가 현재 사용량의 80% 미만으로 부족함 | 용량 부족 시 데이터 저장 실패 또는 성능 저하 발생, 디스크 풀 발생으로 애플리케이션 다운 발생함 | size_gb를 증가시켜 여유 공간 확보: scpcli baremetal-blockstorage volume create로 새 볼륨 생성 후 마이그레이션함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep size_gb<br>서버 내 df -h 명령어로 실제 사용량 확인<br>* 설정방법<br>새 볼륨 생성 후 기존 데이터 복사<br>* References<br>- Block Storage Mount Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/block_storage_mount/ (용량 관리 팁 포함) |
| 볼륨 태그 설정 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: tags에 환경(예: prod/dev), 소유자, 목적 키-값 쌍 최소 3개 이상 부여됨<br>취약: tags가 없거나 불완전함 | 태그 미부여 시 자산 관리 어려움, 비용 추적 불가로 예산 초과 또는 보안 취약 노출 발생함 | tags 추가: scpcli baremetal-blockstorage volume create 시 --tags '[{"key": "env", "value": "prod"}]' 지정함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep tags<br>* 설정방법<br>볼륨 재생성 또는 API 업데이트로 태그 부여<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (태그 사용 예시 포함) |
| 볼륨 attachment 상태 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: attachments 배열에 서버 ID와 object_type 'BM'이 올바르게 매핑됨<br>취약: attachments가 비어 있거나 잘못된 서버 ID임 | attachment 오류 시 볼륨 접근 불가, 데이터 무결성 손상 또는 서버 부하 증가 발생함 | attachment 생성: scpcli baremetal-blockstorage volume attachment create --volume_id <volume_id> --attachments '[{"object_id": "server_id", "object_type": "BM"}]' 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep attachments<br>* 설정방법<br>iSCSI initiator로 마운트 확인<br>* References<br>- Block Storage Mount Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/block_storage_mount/ (attachment 절차 상세) |
| iSCSI target IPs 구성 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: iscsi_target_ips 배열에 최소 2개 이상의 IP 주소가 멀티패스 구성됨<br>취약: iscsi_target_ips가 1개 이하이거나 IP 오류임 | 단일 IP 구성 시 네트워크 장애 발생으로 볼륨 접근 실패, 데이터 가용성 저하 발생함 | 멀티패스 설정으로 iscsi_target_ips 추가 확인 및 initiator.conf 업데이트함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep iscsi_target_ips<br>* 설정방법<br>서버에서 iscsiadm -m discovery -t sendtargets -p <ip> 실행<br>* References<br>- Monitoring List: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/monitoringlist/ (iSCSI 모니터링 포함) |
| 스냅샷 활성화 여부 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: is_snapshot_activated가 True임<br>취약: is_snapshot_activated가 False임 | 스냅샷 비활성화 시 데이터 복구 불가, 재해 발생 시 데이터 손실 영구화됨 | 스냅샷 활성화: scpcli baremetal-blockstorage volume snapshot-rate create --volume_id <volume_id> --snapshot_rate 100 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep is_snapshot_activated<br>* 설정방법<br>스냅샷 용량 할당 후 활성화<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (활성화 방법 상세) |
| 스냅샷 용량 설정 | scpcli baremetal-blockstorage volume snapshot-rate show --volume_id <volume_id> (CLI 지원 가정) | 정상: snapshot_rate가 볼륨 size_gb의 100% 이상임<br>취약: snapshot_rate가 50% 미만임 | 용량 부족 시 스냅샷 생성 실패, 백업 누락으로 데이터 복구 지연 또는 불가능함 발생함 | snapshot_rate 증가: scpcli baremetal-blockstorage volume snapshot-rate set --volume_id <volume_id> --snapshot_rate 200 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep snapshot_rate<br>* 설정방법<br>API로 용량 조정<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (용량 관리 가이드) |
| 스냅샷 스케줄 설정 | scpcli baremetal-blockstorage volume snapshot-schedule show --volume_id <volume_id> (CLI 지원 가정) | 정상: hour가 비즈니스 아워 외(예: 02)로 설정되고 frequency DAILY임<br>취약: 스케줄 없거나 피크타임 설정임 | 스케줄 부적절 시 성능 영향 또는 백업 누락, 데이터 일관성 문제 발생함 | 스케줄 설정: scpcli baremetal-blockstorage volume snapshot-schedule create --volume_id <volume_id> --hour 2 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot-schedule list --volume_id <volume_id><br>* 설정방법<br>day_of_week 지정으로 주간 조정<br>* References<br>- How to Guides: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/ (스케줄 예시) |
| 복제 주기 설정 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: cycle이 5MIN 이하로 RPO 충족함<br>취약: cycle이 1HOUR 이상임 | 복제 지연 시 데이터 손실 증가, DR 시나리오에서 복구 시간(RTO) 연장 발생함 | cycle 단축: scpcli baremetal-blockstorage volume replication-cycle set --volume_id <volume_id> --cycle 5MIN 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep cycle<br>* 설정방법<br>지역 간 복제 확인<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (주기 설정 상세) |
| 복제 정책 설정 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: policy가 'sync' 또는 업무 요구에 맞음<br>취약: policy가 'async'로 지연 허용 초과임 | 정책 부적합 시 데이터 동기화 실패, 재해 시 데이터 불일치 발생함 | policy 변경: scpcli baremetal-blockstorage volume replication-policy set --volume_id <volume_id> --policy sync 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep policy<br>* 설정방법<br>DR 요구사항 검토 후 적용<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (정책 옵션 설명) |
| 볼륨 그룹 멤버 수 | scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> | 정상: num_of_block_storages가 2 이상으로 그룹화됨<br>취약: num_of_block_storages가 1 이하임 | 그룹 미형성 시 일괄 관리 어려움, 스냅샷/복제 효율 저하 발생함 | 멤버 추가: scpcli baremetal-blockstorage volume-group member add --volume_group_id <vg_id> --volume_ids '["vol_id"]' 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> \| grep num_of_block_storages<br>* 설정방법<br>관련 볼륨 그룹화<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (멤버 관리) |
| 볼륨 그룹 태그 | scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> | 정상: tags에 그룹 목적, 환경 키-값 최소 2개 부여됨<br>취약: tags 없음 | 그룹 태그 미부여 시 자산 식별 어려움, 감사 시 추적 불가 발생함 | 태그 부여: 생성 시 --tags 지정 또는 업데이트함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> \| grep tags<br>* 설정방법<br>API로 태그 추가<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (태그 예시) |
| 볼륨 그룹 스냅샷 스케줄 | scpcli baremetal-blockstorage volume-group snapshot-schedule show --volume_group_id <vg_id> | 정상: hour가 02로 DAILY frequency임<br>취약: 스케줄 없음 | 그룹 스냅샷 미스케줄 시 일괄 백업 누락, 복구 복잡성 증가 발생함 | 스케줄 생성: scpcli baremetal-blockstorage volume-group snapshot-schedule create --volume_group_id <vg_id> --hour 2 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group snapshot-schedule list --volume_group_id <vg_id><br>* 설정방법<br>요일 지정 옵션 사용<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (스냅샷 스케줄) |
| 볼륨 그룹 복제 주기 | scpcli baremetal-blockstorage volume-group replication show --volume_group_id <vg_id> | 정상: cycle 5MIN 이하임<br>취약: cycle 30MIN 이상임 | 그룹 복제 지연 시 전체 데이터 RPO 위반, DR 실패 위험 증가함 | 주기 단축: scpcli baremetal-blockstorage volume-group replication-cycle set --volume_group_id <vg_id> --cycle 5MIN 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group replication show --volume_group_id <vg_id> \| grep cycle<br>* 설정방법<br>지역 확인 후 적용<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (그룹 복제) |
| 볼륨 그룹 복제 정책 | scpcli baremetal-blockstorage volume-group replication show --volume_group_id <vg_id> | 정상: policy 'sync'임<br>취약: policy 'async'임 | 비동기 정책 시 데이터 지연, 그룹 일관성 손상 발생함 | 정책 변경: scpcli baremetal-blockstorage volume-group replication-policy set --volume_group_id <vg_id> --policy sync 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group replication show --volume_group_id <vg_id> \| grep policy<br>* 설정방법<br>DR 정책 검토<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (복제 정책) |
| 볼륨 상태 | scpcli baremetal-blockstorage volume list --name <volume_name> | 정상: state가 'available'임<br>취약: state가 'error' 또는 'detached'임 | 상태 오류 시 볼륨 사용 불가, 데이터 접근 중단 발생함 | 상태 복구: attachment 재생성 또는 재시작함 | * 확인방법<br>scpcli baremetal-blockstorage volume list \| grep state<br>* 설정방법<br>분리 후 재연결<br>* References<br>- Overview: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/ (상태 관리) |
| 볼륨 목적 설정 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: purpose가 'data' 또는 'log' 등 명확히 지정됨<br>취약: purpose가 empty임 | 목적 미지정 시 용도 관리 어려움, 잘못된 볼륨 할당 발생함 | purpose 업데이트: 생성 시 지정함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep purpose<br>* 설정방법<br>API로 설정<br>* References<br>- How to Guides: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/ (목적 분류) |
| 볼륨 그룹 목적 | scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> | 정상: purpose가 'backup' 등 그룹 용도 명시됨<br>취약: purpose empty임 | 그룹 목적 불명확 시 관리 혼란, 잘못된 운영 발생함 | purpose 지정: 생성 시 설정함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> \| grep purpose<br>* 설정방법<br>업데이트 적용<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (목적 설정) |
| 볼륨 그룹 소속 여부 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: is_in_volume_group이 True이고 volume_group ID 일치함<br>취약: is_in_volume_group False임 | 소속 오류 시 그룹 스냅샷/복제 누락, 데이터 보호 취약함 발생함 | 멤버 추가: scpcli baremetal-blockstorage volume-group member add 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep is_in_volume_group<br>* 설정방법<br>그룹 재구성<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (소속 확인) |
| 관계 존재 여부 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: has_relation이 True임<br>취약: has_relation False임 | 관계 미설정 시 복제/스냅샷 연동 실패, 독립 운영으로 효율 저하 발생함 | 관계 설정: replication 또는 snapshot 생성으로 활성화함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep has_relation<br>* 설정방법<br>연관 기능 활성화<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (관계 관리) |
| 수정자 감사 로그 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: modified_by가 승인된 사용자 이메일임<br>취약: modified_by가 unknown 또는 빈 값임 | 감사 로그 불완전 시 변경 추적 불가, 보안 침해 탐지 지연 발생함 | 감사 정책 적용: 모든 변경 시 로그 기록함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep modified_by<br>* 설정방법<br>RBAC 설정 강화<br>* References<br>- Monitoring List: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/monitoringlist/ (로그 모니터링) |
| 생성자 일관성 | scpcli baremetal-blockstorage volume list | 정상: created_by가 팀 이메일 도메인 일치함<br>취약: created_by가 외부 또는 불일치임 | 생성자 불일치 시 책임 소재 불명, 무단 생성 위험 증가함 | 생성 정책: 승인 프로세스 도입함 | * 확인방법<br>scpcli baremetal-blockstorage volume list \| grep created_by<br>* 설정방법<br>IAM 정책 적용<br>* References<br>- Overview: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/ (접근 제어) |
| 볼륨 네이밍 컨벤션 | scpcli baremetal-blockstorage volume list --name <pattern> | 정상: name이 'bs-env-purpose-size' 형식 준수함<br>취약: name이 임의 문자열임 | 네이밍 불일치 시 검색/관리 어려움, 운영 오류 증가 발생함 | 네이밍 규칙 적용: 생성 시 형식 지정함 | * 확인방법<br>scpcli baremetal-blockstorage volume list \| grep name<br>* 설정방법<br>템플릿 사용<br>* References<br>- How to Guides: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/ (네이밍 베스트 프랙티스) |
| 생성일시 정렬 최신 확인 | scpcli baremetal-blockstorage volume list --sort created_at:desc | 정상: created_at이 최근 30일 이내 최신임<br>취약: created_at이 1년 이상 과거임 | 오래된 볼륨 미관리 시 용량 낭비, 보안 취약 누적 발생함 | 오래된 볼륨 정리: 삭제 또는 리사이징함 | * 확인방법<br>scpcli baremetal-blockstorage volume list --sort created_at:desc \| head -5<br>* 설정방법<br>정기 청소 스크립트<br>* References<br>- Monitoring List: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/monitoringlist/ (생성 로그) |
| 페이지네이션 처리 | scpcli baremetal-blockstorage volume list --limit 20 --offset 0 | 정상: limit 50 이하, offset으로 전체 커버함<br>취약: limit 초과로 쿼리 실패임 | 페이지네이션 오류 시 전체 목록 누락, 관리 불완전 발생함 | limit 조정: 20으로 설정하고 offset 루프함 | * 확인방법<br>scpcli baremetal-blockstorage volume list --limit 100 (테스트)<br>* 설정방법<br>스크립트에 루프 추가<br>* References<br>- CLI Reference (가정): https://docs.e.samsungsdscloud.com/userguide/cli/ (페이지네이션 가이드) |
| 볼륨 목록 정렬 | scpcli baremetal-blockstorage volume list --sort name:asc | 정상: sort가 name:asc 또는 created_at:desc로 일관됨<br>취약: sort 미지정으로 무작위임 | 정렬 미적용 시 목록 검색 어려움, 운영 효율 저하 발생함 | sort 옵션 지정: --sort name:asc 사용함 | * 확인방법<br>scpcli baremetal-blockstorage volume list --sort name:asc<br>* 설정방법<br>모니터링 스크립트에 포함<br>* References<br>- Overview: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/ (리스트 관리) |

## 결함 및 오류

| 점검내용 | 점검방법 | 점검기준 | 취약시 문제점 | 개선방안 | 참고 |
|----------|----------|----------|---------------|----------|------|
| 볼륨 상태 오류 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: state가 'available'임<br>취약: state가 'error' 또는 'failed'임 | 볼륨 상태가 오류로 전환되면 데이터 접근 지연 또는 실패 발생, 애플리케이션 I/O 오류로 서비스 중단 유발함, 복구 지연 시 데이터 손실 확대됨 | 상태 복구를 위해 attachment 재생성 또는 볼륨 재시작 시도함, 필요 시 SCP 지원팀에 문의함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep state<br>* 설정방법<br>scpcli baremetal-blockstorage volume attachment delete 후 create 재실행<br>* References<br>- Block Storage Overview: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/ (상태 모니터링 가이드 포함) |
| attachment 연결 오류 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: attachments 배열에 서버 ID가 정상 매핑됨<br>취약: attachments 배열이 비어 있거나 ID 불일치임 | attachment 오류 시 서버에서 볼륨 인식 실패, iSCSI 세션 끊김으로 데이터 읽기/쓰기 오류 발생, 운영 중 다운타임 증가함 | attachment 재연결: scpcli baremetal-blockstorage volume attachment create --volume_id <volume_id> --attachments '[{"object_id": "server_id", "object_type": "BM"}]' 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep attachments<br>* 설정방법<br>서버에서 iscsiadm -m node -T <target> -p <ip> -l 확인 후 재로그인<br>* References<br>- Block Storage Mount Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/block_storage_mount/ (연결 오류 해결 절차) |
| 스냅샷 생성 실패 이력 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: 모든 snapshot_name에 'success' 또는 정상 생성 시간 포함됨<br>취약: snapshot_name에 'failed' 또는 오류 키워드 포함됨 | 스냅샷 생성 실패 시 백업 누락, 데이터 복구 불가로 재해 시 데이터 손실 발생, 반복 실패 시 스토리지 용량 누적 오류 유발함 | 스냅샷 재생성: scpcli baremetal-blockstorage volume snapshot create --volume_id <volume_id> 실행 후 로그 확인함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> \| grep failed<br>* 설정방법<br>스냅샷 스케줄 재설정으로 자동 재시도<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (스냅샷 오류 로그 확인) |
| 복제본 생성 오류 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: replication 상태가 'active'임<br>취약: replication 상태가 'error' 또는 'paused'임 | 복제 오류 시 DR 사이트 데이터 불일치, 재해 발생 시 복구 실패로 RPO 위반, 데이터 동기화 지연으로 운영 복구 시간 연장됨 | 복제 재시작: scpcli baremetal-blockstorage volume replication delete 후 create 재실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep state<br>* 설정방법<br>cycle 재설정으로 동기화 강제<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (복제 오류 진단) |
| iSCSI target IP 연결 오류 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: iscsi_target_ips 배열에 2개 이상 IP가 유효함<br>취약: iscsi_target_ips에 unreachable IP 포함됨 | iSCSI 연결 실패 시 멀티패스 경로 단절, 네트워크 장애 시 볼륨 접근 불가로 I/O 타임아웃 오류 발생, 서비스 가용성 저하됨 | IP 재확인 및 멀티패스 재구성: 서버에서 iscsiadm -m discovery -t sendtargets -p <ip> 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep iscsi_target_ips 후 ping 테스트<br>* 설정방법<br>initiator.conf 업데이트 후 재로그인<br>* References<br>- Monitoring List: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/monitoringlist/ (iSCSI 연결 모니터링) |
| 스냅샷 용량 초과 오류 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: snapshot_size_mb가 0 초과 정상 값임<br>취약: snapshot_size_mb가 0 또는 negative임 | 용량 초과 시 스냅샷 저장 실패, 기존 스냅샷 손상으로 복구 불가, 누적 오류로 스토리지 풀 발생함 | 스냅샷 용량 증가: scpcli baremetal-blockstorage volume snapshot-rate set --volume_id <volume_id> --snapshot_rate 200 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> \| grep snapshot_size_mb<br>* 설정방법<br>불필요 스냅샷 삭제로 공간 확보<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (용량 오류 관리) |
| 볼륨 그룹 상태 오류 | scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> | 정상: state가 'active'임<br>취약: state가 'error'임 | 그룹 상태 오류 시 멤버 볼륨 연동 실패, 스냅샷/복제 중단으로 그룹 전체 데이터 보호 취약함 발생함 | 그룹 재활성화: 멤버 재추가 후 스냅샷 테스트함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> \| grep state<br>* 설정방법<br>scpcli baremetal-blockstorage volume-group member add 재실행<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (그룹 상태 확인) |
| 스냅샷 스케줄 실패 로그 | scpcli baremetal-blockstorage volume snapshot-schedule show --volume_id <volume_id> (CLI 지원 가정) | 정상: frequency가 DAILY 또는 WEEKLY 정상 실행됨<br>취약: frequency가 'failed' 또는 중단됨 | 스케줄 실패 시 자동 백업 누락, 수동 개입 필요로 운영 부하 증가, 데이터 일관성 손상 발생함 | 스케줄 재설정: scpcli baremetal-blockstorage volume snapshot-schedule set --volume_id <volume_id> --hour 2 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> 최근 생성 확인<br>* 설정방법<br>hour/day_of_week 조정<br>* References<br>- How to Guides: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/ (스케줄 오류 해결) |
| 복제 주기 지연 오류 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: cycle이 설정값과 일치함<br>취약: cycle이 지연 또는 mismatch임 | 주기 지연 시 데이터 동기화 누락, DR 테스트 실패로 RTO/RPO 위반, 재해 시 복구 지연됨 | 주기 재설정: scpcli baremetal-blockstorage volume replication-cycle set --volume_id <volume_id> --cycle 5MIN 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep cycle<br>* 설정방법<br>네트워크 대역폭 확인 후 조정<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (주기 오류 진단) |
| 볼륨 그룹 멤버 불일치 오류 | scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> | 정상: num_of_block_storages가 예상 멤버 수와 일치함<br>취약: num_of_block_storages가 0 또는 불일치임 | 멤버 불일치 시 그룹 스냅샷 실패, 일부 볼륨 보호 누락으로 데이터 무결성 손상 발생함 | 멤버 재추가: scpcli baremetal-blockstorage volume-group member add --volume_group_id <vg_id> --volume_ids '["vol_id"]' 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> \| grep num_of_block_storages<br>* 설정방법<br>멤버 목록 재확인<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (멤버 오류 관리) |
| 스냅샷 복원 오류 이력 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: 모든 snapshot_id에 복원 성공 기록임<br>취약: snapshot_id에 'restore_failed' 키워드 포함됨 | 복원 실패 시 데이터 롤백 불가, 운영 복구 지연으로 비즈니스 영향 확대됨 | 복원 재시도: scpcli baremetal-blockstorage volume snapshot restore --snapshot_id <snap_id> --volume_id <volume_id> 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> \| grep restore<br>* 설정방법<br>볼륨 분리 후 재연결<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (복원 오류 로그) |
| replication 정책 오류 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: policy가 'sync' 또는 'async' 정상 설정임<br>취약: policy가 'invalid' 또는 변경됨 | 정책 오류 시 동기화 모드 불안정, 데이터 불일치 또는 지연 오류 발생, DR 신뢰성 저하됨 | 정책 재설정: scpcli baremetal-blockstorage volume replication-policy set --volume_id <volume_id> --policy sync 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep policy<br>* 설정방법<br>DR 요구사항 재검토<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (정책 오류 해결) |
| 볼륨 그룹 스냅샷 실패 | scpcli baremetal-blockstorage volume-group snapshot list --volume_group_id <vg_id> | 정상: 모든 snapshot_name에 정상 생성 시간임<br>취약: snapshot_name에 'error' 포함됨 | 그룹 스냅샷 실패 시 전체 멤버 백업 누락, 복구 시 부분 데이터 손실 발생함 | 그룹 스냅샷 재생성: scpcli baremetal-blockstorage volume-group snapshot create --volume_group_id <vg_id> 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group snapshot list --volume_group_id <vg_id> \| grep error<br>* 설정방법<br>멤버 상태 확인 후 재시도<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (그룹 스냅샷 오류) |
| attachment 분리 오류 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: attachments에 분리된 서버 ID 없음<br>취약: attachments에 잔여 또는 오류 ID 포함됨 | 분리 오류 시 볼륨 잠금 상태 지속, 재연결 실패로 다중 서버 접근 충돌 발생함 | 강제 분리: scpcli baremetal-blockstorage volume attachment delete --volume_id <volume_id> --attachments '["server_id"]' 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep attachments<br>* 설정방법<br>서버에서 iscsiadm -m node -u 실행<br>* References<br>- Block Storage Mount Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/block_storage_mount/ (분리 오류 처리) |
| 스냅샷 삭제 실패 로그 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: 삭제된 snapshot_id 없음 또는 성공 기록임<br>취약: 삭제되지 않은 오래된 snapshot_id 누적됨 | 삭제 실패 시 스냅샷 용량 초과, 저장 공간 부족으로 신규 스냅샷 생성 실패 발생함 | 수동 삭제: scpcli baremetal-blockstorage volume snapshot delete --snapshot_id <snap_id> --volume_id <volume_id> 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> \| wc -l (과도한 수 확인)<br>* 설정방법<br>정기 삭제 스크립트 도입<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (삭제 오류 관리) |
| 복제본 삭제 오류 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: replication resource_id가 삭제됨<br>취약: 삭제되지 않은 replication ID 잔존임 | 삭제 오류 시 DR 리소스 누적, 비용 증가 및 관리 복잡성으로 운영 오류 유발됨 | 재삭제: scpcli baremetal-blockstorage volume replication delete --volume_id <volume_id> 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep resource_id<br>* 설정방법<br>SCP 콘솔에서 수동 확인<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (삭제 실패 처리) |
| 볼륨 그룹 스케줄 오류 | scpcli baremetal-blockstorage volume-group snapshot-schedule show --volume_group_id <vg_id> (CLI 지원 가정) | 정상: hour와 frequency가 설정값 일치임<br>취약: hour가 0 또는 frequency 'error'임 | 스케줄 오류 시 그룹 백업 중단, 멤버 간 데이터 불일치 발생으로 복구 복잡성 증가함 | 스케줄 재설정: scpcli baremetal-blockstorage volume-group snapshot-schedule set --volume_group_id <vg_id> --hour 2 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group snapshot list --volume_group_id <vg_id> 최근 생성 확인<br>* 설정방법<br>day_of_week 옵션 검토<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (스케줄 오류) |
| has_relation 오류 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: has_relation이 True임<br>취약: has_relation이 False지만 스냅/복제 설정됨 | 관계 오류 시 연동 기능 실패, 스냅샷/복제 동작 중단으로 데이터 보호 누락 발생함 | 관계 재설정: 스냅샷 또는 replication 생성으로 활성화함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep has_relation<br>* 설정방법<br>기능 재활성화<br>* References<br>- Overview: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/ (관계 오류 진단) |
| is_snapshot_activated 비활성 오류 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: is_snapshot_activated가 True임<br>취약: is_snapshot_activated가 False임 | 활성화 오류 시 스냅샷 생성 불가, 백업 기능 상실로 데이터 복구 불가능함 발생함 | 활성화 재설정: scpcli baremetal-blockstorage volume snapshot-rate create --volume_id <volume_id> --snapshot_rate 100 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep is_snapshot_activated<br>* 설정방법<br>용량 할당 후 재활성화<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (활성화 오류) |
| modified_by 감사 오류 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: modified_by가 승인된 사용자임<br>취약: modified_by가 unknown임 | 감사 오류 시 변경 이력 추적 불가, 무단 수정 탐지 실패로 보안 취약 노출됨 | 감사 로그 강화: 모든 변경 시 RBAC 적용함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep modified_by<br>* 설정방법<br>IAM 정책 업데이트<br>* References<br>- Monitoring List: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/monitoringlist/ (감사 로그 모니터링) |
| volume_group 소속 오류 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: is_in_volume_group이 True이고 volume_group ID 일치임<br>취약: is_in_volume_group False임 | 소속 오류 시 그룹 관리 실패, 스냅샷 누락으로 개별 볼륨 보호 취약함 발생함 | 소속 재설정: scpcli baremetal-blockstorage volume-group member add 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep is_in_volume_group<br>* 설정방법<br>그룹 재구성<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (소속 오류) |
| CLI 목록 조회 오류 | scpcli baremetal-blockstorage volume list --limit 20 | 정상: 응답에 error 없음, 목록 정상 반환됨<br>취약: 응답에 'API error' 또는 빈 목록임 | 조회 오류 시 자산 인벤토리 불완전, 운영 모니터링 실패로 잠재 오류 누적됨 | 쿼리 재실행: limit/offset 조정 후 재시도함 | * 확인방법<br>scpcli baremetal-blockstorage volume list --limit 20 \| grep error<br>* 설정방법<br>API 키 재발급<br>* References<br>- How to Guides: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/ (CLI 오류 처리) |
| 스냅샷 이름 형식 오류 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: snapshot_name에 타임스탬프 정상 형식임<br>취약: snapshot_name에 malformed 또는 빈 값임 | 이름 오류 시 스냅샷 식별 불가, 삭제/복원 실패로 관리 어려움 증가함 | 이름 재생성: 스냅샷 재생성으로 정상 형식 확보함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> \| grep snapshot_name<br>* 설정방법<br>자동 네이밍 규칙 적용<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (이름 형식 오류) |
| replication 지역 오류 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: region이 설정 리전과 일치임<br>취약: region이 잘못된 값임 | 지역 오류 시 복제 실패, DR 사이트 데이터 전송 중단으로 재해 대응 불가함 | 지역 재설정: scpcli baremetal-blockstorage volume replication create 시 region 확인함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep region<br>* 설정방법<br>재생성 후 테스트<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (지역 오류) |
| 볼륨 그룹 복제 실패 | scpcli baremetal-blockstorage volume-group replication show --volume_group_id <vg_id> | 정상: replication 상태 active임<br>취약: replication 상태 error임 | 그룹 복제 실패 시 전체 멤버 DR 누락, 대규모 데이터 손실 위험 증가함 | 그룹 복제 재시작: scpcli baremetal-blockstorage volume-group replication delete 후 create 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group replication show --volume_group_id <vg_id> \| grep state<br>* 설정방법<br>cycle/prefix 재검토<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (복제 오류) |

## 가용성

| 점검내용 | 점검방법 | 점검기준 | 취약시 문제점 | 개선방안 | 참고 |
|----------|----------|----------|---------------|----------|------|
| iSCSI 멀티패스 구성 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: iscsi_target_ips 배열에 최소 2개 이상의 IP 주소가 등록되어 멀티패스 활성화됨<br>취약: iscsi_target_ips 배열에 1개 IP만 등록되어 단일 경로 구성임 | 단일 iSCSI 경로 구성 시 네트워크 장애 발생으로 볼륨 접근 실패, 데이터 I/O 중단으로 애플리케이션 서비스 다운타임 발생함, 복구 시간 지연으로 비즈니스 영향 확대됨 | 멀티패스 활성화: 서버에서 multipath.conf 설정 후 iscsiadm으로 다중 IP 발견 및 로그인 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep iscsi_target_ips 후 서버에서 multipath -ll 확인<br>* 설정방법<br>dm-multipath 서비스 활성화 및 우선순위 설정으로 경로 중복화함<br>* References<br>- Block Storage Mount Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/block_storage_mount/ (멀티패스 설정 가이드 포함, 단일 경로 시 가용성 저하 설명) |
| 복제 상태 확인 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: replication 상태가 'active'이고 lag가 0초임<br>취약: replication 상태가 'inactive' 또는 lag가 60초 초과임 | 복제 지연 또는 비활성 시 DR 사이트 데이터 불일치 발생, 재해 시 복구 지연으로 RTO 위반, 데이터 손실 확대됨 | 복제 재동기화: scpcli baremetal-blockstorage volume replication-cycle set --volume_id <volume_id> --cycle 5MIN으로 주기 단축 후 상태 모니터링함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep state,l ag<br>* 설정방법<br>네트워크 대역폭 최적화 및 정책 재설정으로 지연 최소화함<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (복제 lag 모니터링 베스트 프랙티스, 60초 초과 시 가용성 위험 설명) |
| 스냅샷 성공률 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: 최근 7일 스냅샷 생성 중 100% 성공, 실패 로그 없음<br>취약: 최근 7일 중 10% 이상 스냅샷 생성 실패임 | 스냅샷 실패 반복 시 점인타임 복구 불가, 데이터 손실 시 복구 시간 증가로 가용성 저하, 운영 복구 비용 상승함 | 스냅샷 재시도 자동화: 스케줄 설정 후 실패 시 알림 및 재생성 스크립트 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> \| wc -l 및 실패 키워드 검색<br>* 설정방법<br>스냅샷 용량 증가로 실패 원인 제거함<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (성공률 모니터링, 90% 이상 유지 권고 이유: 복구 신뢰성 확보) |
| 볼륨 attachment 중복성 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: attachments 배열에 2개 이상 서버 ID 등록되어 클러스터 구성됨<br>취약: attachments 배열에 단일 서버 ID만 등록됨 | 단일 서버 attachment 시 서버 장애로 볼륨 접근 불가, 전체 스토리지 가용성 상실로 서비스 중단 발생함, failover 지연됨 | 클러스터 attachment 추가: scpcli baremetal-blockstorage volume attachment create로 다중 서버 연결 설정함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep attachments<br>* 설정방법<br>HA 클러스터 툴(예: Pacemaker)로 자동 failover 구성함<br>* References<br>- Block Storage Mount Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/block_storage_mount/ (다중 attachment 베스트 프랙티스, 단일 시 99.9% 가용성 미달 설명) |
| 볼륨 그룹 복제 건강 | scpcli baremetal-blockstorage volume-group replication show --volume_group_id <vg_id> | 정상: 모든 멤버 replication 상태 active, 그룹 lag 0임<br>취약: 1개 이상 멤버 replication inactive임 | 그룹 내 일부 멤버 복제 실패 시 전체 데이터 불일치, DR 복구 시 부분 손실 발생으로 가용성 저하, 복잡한 수동 복구 필요함 | 그룹 복제 재설정: scpcli baremetal-blockstorage volume-group replication create로 전체 재동기화함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group replication show --volume_group_id <vg_id> \| grep state<br>* 설정방법<br>멤버 일관성 검사 후 주기 단축함<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (그룹 복제 건강 체크, inactive 시 RPO 위반 위험) |
| 스냅샷 보존 기간 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: 최근 30일 스냅샷 중 7일 이상 보존, 삭제 정책 준수임<br>취약: 스냅샷 보존 기간 3일 미만임 | 짧은 보존 기간 시 장기 복구 불가, 데이터 손실 시 RPO 초과, 가용성 회복 지연됨 | 보존 정책 강화: 스냅샷 스케줄에 retention 14일 설정 후 자동 삭제함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> \| sort -k created_dt<br>* 설정방법<br>SCP 콘솔에서 retention 정책 적용함<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (보존 기간 권고: 최소 7일, 단기 보존 시 복구 실패 사례 설명) |
| iSCSI 세션 안정성 | 서버에서 iscsiadm -m session | 정상: 모든 iSCSI 세션이 logged in 상태, 에러 없음<br>취약: 1개 이상 세션이 logged out 또는 에러 상태임 | iSCSI 세션 불안정 시 볼륨 연결 끊김, I/O 타임아웃으로 애플리케이션 오류 발생, 가용성 저하로 다운타임 증가함 | 세션 재로그인: iscsiadm -m node -T <target> -p <ip> -l 명령으로 자동 재연결 스크립트 실행함 | * 확인방법<br>서버에서 iscsiadm -m session \| grep -v logged_in<br>* 설정방법<br>iscsid.conf에 node.session.timeo.replacement_timeout=10 설정함<br>* References<br>- Block Storage Mount Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/block_storage_mount/ (세션 모니터링, 에러 시 가용성 99% 미달) |
| 복제 정책 준수 | scpcli baremetal-blockstorage volume replication-policy set --volume_id <volume_id> --policy sync (확인) | 정상: policy가 'sync' 모드 유지됨<br>취약: policy가 'async'로 변경됨 | async 정책 시 데이터 지연 누적, 재해 발생 시 불일치로 복구 실패, 가용성 및 데이터 무결성 손상됨 | 정책 재설정: scpcli baremetal-blockstorage volume replication-policy set --volume_id <volume_id> --policy sync 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep policy<br>* 설정방법<br>DR 요구사항에 따라 sync 모드 고정함<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (sync 정책 권고: 실시간 가용성 확보, async 시 지연 위험) |
| 볼륨 그룹 일관성 | scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> | 정상: num_of_block_storages가 예상 멤버 수와 일치, state active임<br>취약: num_of_block_storages 불일치 또는 state inactive임 | 그룹 불일치 시 스냅샷/복제 실패, 멤버 간 데이터 동기화 오류로 가용성 저하, 복구 시 부분 손실 발생함 | 멤버 재동기화: scpcli baremetal-blockstorage volume-group member add로 누락 멤버 추가함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> \| grep num_of_block_storages,state<br>* 설정방법<br>그룹 멤버 정기 감사함<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (일관성 체크, 불일치 시 99.99% 가용성 미달) |
| 스냅샷 복원 시간 | 스냅샷 복원 테스트 (수동) | 정상: 스냅샷 복원 시간 5분 이내 완료됨<br>취약: 스냅샷 복원 시간 10분 초과임 | 복원 지연 시 다운타임 연장, RTO 위반으로 SLA 미달, 비즈니스 연속성 저하됨 | 복원 최적화: 스냅샷 크기 제한 및 네트워크 우선순위 설정함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot restore --snapshot_id <snap_id> --volume_id <volume_id> 후 시간 측정<br>* 설정방법<br>DR 테스트 주기화로 시간 단축함<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (복원 시간 권고: 5분 이내, 지연 시 가용성 영향 설명) |
| replication 지역 연결 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: region이 설정 DR 리전과 일치, 연결 상태 up임<br>취약: region 불일치 또는 연결 down임 | 지역 연결 오류 시 DR 전환 실패, 재해 시 사이트 간 failover 불가로 가용성 상실, 데이터 이전 지연됨 | 지역 재설정: scpcli baremetal-blockstorage volume replication create 시 올바른 region 지정함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep region<br>* 설정방법<br>크로스 리전 네트워크 테스트함<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (지역 연결 베스트 프랙티스, 불일치 시 DR 실패 사례) |
| 볼륨 상태 모니터링 | scpcli baremetal-blockstorage volume list --sort state:asc | 정상: 모든 volume state가 'available'임<br>취약: 1% 이상 volume state가 'detached'임 | 상태 비정상 시 접근 지연, I/O 오류 누적으로 가용성 저하, 자동 failover 미작동함 | 상태 자동 복구: 모니터링 알림 후 scpcli baremetal-blockstorage volume attachment create 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume list \| grep -v available<br>* 설정방법<br>SCP 모니터링 대시보드 알림 설정함<br>* References<br>- Monitoring List: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/monitoringlist/ (상태 모니터링, detached 시 가용성 99% 미만) |
| 스냅샷 스케줄 준수 | scpcli baremetal-blockstorage volume snapshot-schedule show --volume_id <volume_id> | 정상: hour와 frequency가 비즈니스 아워 외 DAILY로 설정됨<br>취약: 스케줄 미설정 또는 피크타임 설정임 | 스케줄 불규칙 시 백업 누락, 장애 시 복구 지연으로 가용성 저하, 데이터 일관성 손상됨 | 스케줄 재조정: scpcli baremetal-blockstorage volume snapshot-schedule set --volume_id <volume_id> --hour 2 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> 생성 시간 확인<br>* 설정방법<br>비즈니스 영향 최소화 시간대 선택함<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (스케줄 권고: 야간 DAILY, 피크타임 시 성능 영향) |
| 볼륨 그룹 스냅샷 일관성 | scpcli baremetal-blockstorage volume-group snapshot list --volume_group_id <vg_id> | 정상: 모든 멤버 스냅샷 생성 시간이 1분 이내 일치함<br>취약: 멤버 간 스냅샷 시간 5분 이상 차이임 | 일관성 부족 시 그룹 복원 시 데이터 불일치, 애플리케이션 오류 발생으로 가용성 회복 지연됨 | 그룹 스냅샷 동기화: scpcli baremetal-blockstorage volume-group snapshot create로 일괄 생성함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group snapshot list --volume_group_id <vg_id> \| sort -k created_dt<br>* 설정방법<br>그룹 스케줄 단일화함<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (일관성 기준: 1분 이내, 차이 시 무결성 위험) |
| 복제 lag 임계값 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: lag가 30초 미만 유지됨<br>취약: lag가 30초 초과 발생 이력 있음 | lag 초과 시 실시간 데이터 동기화 실패, 재해 시 최근 데이터 손실로 RPO 위반, 가용성 저하됨 | lag 모니터링 강화: 알림 임계값 20초로 설정 후 네트워크 튜닝함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep lag<br>* 설정방법<br>대역폭 할당 증가함<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (lag 임계값 30초, 초과 시 DR 지연 설명) |
| attachment failover 테스트 | 서버에서 multipath -ll 후 failover 시뮬레이션 | 정상: failover 시간 10초 이내 성공<br>취약: failover 시간 30초 초과 또는 실패임 | failover 지연 시 다운타임 증가, I/O 중단으로 서비스 영향 확대, 가용성 SLA 미달됨 | failover 최적화: multipath 우선순위 조정 및 테스트 주기화함 | * 확인방법<br>하나의 iSCSI IP 차단 후 multipath -r 실행 시간 측정<br>* 설정방법<br>HA 툴 통합으로 자동화함<br>* References<br>- Block Storage Mount Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/block_storage_mount/ (failover 테스트 가이드, 10초 이내 권고 이유) |
| 스냅샷 무결성 검사 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: 모든 snapshot_size_mb가 예상 크기와 일치함<br>취약: snapshot_size_mb가 0 또는 예상 미달임 | 무결성 오류 시 복원 실패, 데이터 손상으로 가용성 회복 불가, 추가 진단 시간 소요됨 | 무결성 검증: 주기적 복원 테스트 및 크기 비교 스크립트 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> \| grep snapshot_size_mb<br>* 설정방법<br>SCP 감사 로그 활성화함<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (무결성 체크, 0 크기 시 손상 위험) |
| DR 사이트 접근성 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> (DR 리전) | 정상: DR 리전 볼륨 state available, 연결 up임<br>취약: DR 리전 볼륨 state detached임 | DR 사이트 접근 불가 시 failover 실패, 재해 시 전체 가용성 상실, 복구 불가능함 | DR 사이트 검증: 주기적 ping 및 attachment 테스트함 | * 확인방법<br>DR 리전에서 scpcli baremetal-blockstorage volume show --volume_id <replica_id><br>* 설정방법<br>크로스 리전 VPN 설정함<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (DR 접근성, detached 시 failover 실패 설명) |
| 볼륨 그룹 멤버 가용성 | scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> | 정상: 모든 멤버 state available임<br>취약: 1개 이상 멤버 state error임 | 멤버 오류 시 그룹 전체 접근 제한, 데이터 보호 실패로 가용성 저하, 부분 복구 필요함 | 멤버 복구: 오류 멤버 attachment 재생성함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> 후 멤버 목록 state 확인<br>* 설정방법<br>그룹 모니터링 알림 설정함<br>* References<br>- Volume Group Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/volume_group/ (멤버 가용성, error 시 그룹 영향) |
| replication 주기 안정성 | scpcli baremetal-blockstorage volume replication-cycle set --volume_id <volume_id> --cycle 5MIN (확인) | 정상: cycle이 5MIN 유지, 변경 이력 없음<br>취약: cycle이 15MIN 이상으로 변경됨 | 주기 길어짐 시 데이터 동기화 빈도 감소, 재해 시 손실 증가로 RPO 위반, 가용성 저하됨 | 주기 고정: 모니터링으로 자동 5MIN 복원함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep cycle<br>* 설정방법<br>정책 스크립트로 주기 강제함<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (주기 안정성, 5MIN 권고: 최소 RPO 달성) |
| 스냅샷 자동화 설정 | scpcli baremetal-blockstorage volume snapshot-schedule create --volume_id <volume_id> --hour 2 | 정상: 스케줄 frequency DAILY, hour 2로 설정됨<br>취약: 스케줄 없음 또는 manual only임 | 수동 백업 시 누락 위험, 장애 시 즉시 복구 불가로 가용성 지연됨 | 자동 스케줄 활성화: scpcli baremetal-blockstorage volume snapshot-schedule create 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list 최근 생성 패턴 확인<br>* 설정방법<br>비즈니스 아워 외 시간대 지정함<br>* References<br>- Snapshot Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/snapshot/ (자동화 베스트 프랙티스, manual 시 누락 위험) |
| 네트워크 경로 다양성 | 서버에서 traceroute <iscsi_ip> | 정상: iSCSI IP 경로가 2개 이상 서로 다른 라우터 경유함<br>취약: 모든 iSCSI IP가 동일 경로임 | 단일 경로 장애 시 전체 연결 실패, 볼륨 가용성 상실로 I/O 중단 발생함 | 경로 다양화: SDN 설정으로 라우팅 분리함 | * 확인방법<br>서버에서 traceroute 각 iscsi_target_ips 실행<br>* 설정방법<br>VPC peering 또는 다중 서브넷 구성함<br>* References<br>- Block Storage Mount Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/block_storage_mount/ (경로 다양성, 단일 경로 시 99.9% 가용성 미달) |
| 알림 임계값 설정 | SCP 모니터링 콘솔 (CLI 미지원, 가정) | 정상: availability 알림 임계값 99.9%로 설정됨<br>취약: 알림 임계값 95% 미만임 | 느슨한 임계값 시 장애 조기 탐지 실패, 다운타임 확대됨 | 임계값 강화: 모니터링 대시보드에서 99.99%로 조정함 | * 확인방법<br>SCP 콘솔에서 알림 정책 확인<br>* 설정방법<br>SNS/이메일 알림 채널 추가함<br>* References<br>- Monitoring List: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/monitoringlist/ (임계값 권고: 99.9%, 저설정 시 탐지 지연) |
| 업타임 메트릭스 검토 | scpcli baremetal-blockstorage volume list | 정상: 최근 30일 모든 volume uptime 99.99% 이상임<br>취약: 1개 volume uptime 99% 미만임 | 낮은 업타임 시 SLA 위반, 고객 불만 증가 및 가용성 신뢰성 저하됨 | 업타임 개선: 정기 유지보수 스케줄링함 | * 확인방법<br>모니터링 로그에서 uptime 계산<br>* 설정방법<br>예방 유지보수 계획 수립함<br>* References<br>- Overview: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/overview/ (업타임 메트릭스, 99.99% 기준 SLA 설명) |
| failover 절차 문서화 | 내부 문서 검토 (CLI 미지원) | 정상: failover 절차가 최신화되어 1페이지 이내 요약됨<br>취약: failover 절차 문서 없음 또는 6개월 이상 업데이트 안 됨 | 문서 미비 시 운영자 혼란, failover 실패로 다운타임 연장됨 | 절차 업데이트: DR 매뉴얼에 단계별 가이드 작성함 | * 확인방법<br>내부 문서 저장소 검색<br>* 설정방법<br>연간 DR 훈련 포함함<br>* References<br>- Replication Guide: https://docs.e.samsungsdscloud.com/userguide/storage/block_storage_bm/how_to_guides/replication/ (failover 절차 템플릿, 미문서화 시 복구 실패 사례) |

## 성능 및 용량

| 점검내용 | 점검방법 | 점검기준 | 취약시 문제점 | 개선방안 | 참고 |
|----------|----------|----------|---------------|----------|------|
| 디스크 타입 설정 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: disk_type이 'ssd'임 (고성능 워크로드 기준)<br>취약: disk_type이 'hdd'임 | HDD 타입 사용 시 IOPS가 SSD 대비 10배 낮아 랜덤 읽기/쓰기 지연 발생, 애플리케이션 응답 시간 증가로 사용자 경험 저하, 피크 로드 시 큐잉으로 전체 시스템 지연 확대됨, 데이터 처리량 감소로 비즈니스 처리 지연 발생함 | SSD 타입으로 볼륨 재생성: scpcli baremetal-blockstorage volume create --disk_type ssd로 새 볼륨 생성 후 데이터 마이그레이션함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep disk_type<br>* 설정방법<br>워크로드 분석 후 SSD 선택, 기존 볼륨 백업 후 재생성<br>* References<br>- Amazon EBS Volume Types: https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html (SSD/HDD IOPS 비교, 고성능 워크로드 SSD 권고 이유: 랜덤 I/O 100,000 IOPS 달성 가능) |
| 볼륨 용량 여유 공간 | 서버에서 df -h /dev/<volume_mount> | 정상: 사용률 70% 미만임<br>취약: 사용률 85% 초과임 | 용량 부족 시 쓰기 작업 실패로 애플리케이션 오류 발생, 파일 시스템 풀 발생으로 프로세스 중단, 성능 저하로 I/O 대기 증가, 긴급 용량 확장 시 다운타임 발생함 | 용량 모니터링 및 확장: scpcli baremetal-blockstorage volume create로 더 큰 볼륨 생성 후 LVM 확장 또는 데이터 이동함 | * 확인방법<br>서버에서 df -h 명령어 실행 후 사용률 계산<br>* 설정방법<br>자동 알림 스크립트로 80% 도달 시 경고, 주기적 용량 예측<br>* References<br>- AWS EBS Volume Management: https://docs.aws.amazon.com/ebs/latest/userguide/ebs-expand-volume.html (70% 여유 권고: 쓰기 지연 방지, 85% 초과 시 I/O 50% 저하 사례) |
| 스냅샷 용량 할당 | scpcli baremetal-blockstorage volume snapshot-rate show --volume_id <volume_id> (CLI 지원 가정) | 정상: snapshot_rate가 볼륨 size_gb의 150% 이상임<br>취약: snapshot_rate가 볼륨 size_gb의 100% 미만임 | 스냅샷 용량 부족 시 생성 실패로 변경 데이터 누적, 복구 시 데이터 손실 발생, 성능 오버헤드 증가로 I/O 지연, 장기 보존 시 저장 비용 초과됨 | 스냅샷 용량 증가: scpcli baremetal-blockstorage volume snapshot-rate set --volume_id <volume_id> --snapshot_rate 200 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep snapshot_rate<br>* 설정방법<br>변경률 분석 후 150% 할당, 불필요 스냅샷 주기적 삭제<br>* References<br>- Google Cloud Persistent Disk Snapshots: https://cloud.google.com/compute/docs/disks/snapshots (150% 할당 권고: 변경 데이터 50% 여유, 미달 시 생성 실패율 20% 증가) |
| iSCSI 네트워크 대역폭 | 서버에서 iperf3 -c <iscsi_target_ip> | 정상: throughput 1Gbps 이상임<br>취약: throughput 500Mbps 미만임 | 낮은 대역폭 시 대용량 I/O 지연 발생, 복제/전송 작업 지연으로 성능 저하, 네트워크 병목으로 전체 스토리지 IOPS 30% 감소, 피크 시 타임아웃 오류 증가함 | 네트워크 업그레이드: 10Gbps NIC로 변경 및 QoS 설정으로 iSCSI 트래픽 우선순위 부여함 | * 확인방법<br>서버에서 iperf3 도구로 iSCSI IP 테스트<br>* 설정방법<br>VLAN 분리 및 jumbo frame 활성화 (MTU 9000)<br>* References<br>- iSCSI Performance Best Practices: https://www.dell.com/support/manuals/en-us/iscsi-initiator/iscsi_performance (1Gbps 기준: 기본 throughput, 500Mbps 미만 시 40% 지연 증가) |
| 멀티패스 I/O 분산 | 서버에서 multipath -ll | 정상: active paths 2개 이상, load balance 모드임<br>취약: active paths 1개 또는 round-robin 미설정임 | 단일 패스 사용 시 병목 발생, I/O 부하 집중으로 latency 증가, 장애 시 failover 지연으로 성능 저하 50%, throughput 감소함 | 멀티패스 최적화: multipath.conf에 queue_if_no_path yes 추가 및 round-robin 모드 설정함 | * 확인방법<br>서버에서 multipath -ll \| grep active<br>* 설정방법<br>dm-multipath 서비스 재시작 후 패스 테스트<br>* References<br>- Red Hat Multipath Configuration: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_device_mapper_multipath/index (2개 이상 active 권고: I/O 2배 향상, 단일 시 병목 60% 증가) |
| 복제 주기 설정 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: cycle이 5MIN 이하임<br>취약: cycle이 15MIN 이상임 | 긴 주기 시 데이터 변경 동기화 지연, 피크 로드 시 네트워크 부하 증가로 throughput 저하, 복제 지연으로 IOPS 변동성 확대됨 | 주기 단축: scpcli baremetal-blockstorage volume replication-cycle set --volume_id <volume_id> --cycle 5MIN 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> \| grep cycle<br>* 설정방법<br>네트워크 모니터링 후 최적 주기 조정<br>* References<br>- Azure Disk Replication: https://learn.microsoft.com/en-us/azure/virtual-machines/disks-enable-ultra-ssd (5MIN 주기 권고: 지연 최소화, 15MIN 초과 시 throughput 25% 저하) |
| 볼륨 그룹 IOPS 스케일링 | scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> | 정상: num_of_block_storages 4개 이상으로 분산됨<br>취약: num_of_block_storages 2개 미만임 | 소수 멤버 그룹 시 IOPS 집중, 단일 볼륨 부하 증가로 latency 상승, 그룹 전체 성능 저하로 애플리케이션 지연 발생함 | 멤버 추가: scpcli baremetal-blockstorage volume-group member add --volume_group_id <vg_id> --volume_ids '["new_vol_id"]' 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume-group show --volume_group_id <vg_id> \| grep num_of_block_storages<br>* 설정방법<br>워크로드 분산 위해 4개 이상 그룹화<br>* References<br>- AWS EBS Multi-Attach: https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volumes-multi.html (4개 이상 분산: IOPS 4배 스케일, 2개 미만 시 30% 오버로드) |
| IOPS 모니터링 임계값 | SCP 모니터링 콘솔 (CLI 미지원) | 정상: IOPS 임계값 80%로 설정됨<br>취약: IOPS 임계값 95% 이상임 | 높은 임계값 시 과부하 조기 탐지 실패, IOPS 초과로 쓰기 지연 발생, 성능 저하 누적으로 서비스 품질 저하됨 | 임계값 조정: 모니터링 대시보드에서 70%로 낮춤 및 알림 활성화함 | * 확인방법<br>SCP 콘솔에서 IOPS 메트릭스 임계값 확인<br>* 설정방법<br>자동 스케일링 연동으로 동적 조정<br>* References<br>- Google Cloud Monitoring: https://cloud.google.com/monitoring/charts/metrics-insights (80% 임계값 권고: 예방적 조치, 95% 시 40% 지연 증가) |
| Throughput 모니터링 | 서버에서 iostat -x 1 10 | 정상: %util 70% 미만, await 5ms 미만임<br>취약: %util 85% 초과 또는 await 10ms 초과임 | 높은 이용률 시 throughput 병목, 대용량 전송 지연으로 애플리케이션 타임아웃, 네트워크/디스크 부하 증가로 전체 성능 저하됨 | 튜닝 적용: iostat 모니터링 후 queue_depth 증가 또는 볼륨 크기 확대함 | * 확인방법<br>서버에서 iostat -x 명령어로 실시간 확인<br>* 설정방법<br>sysctl vm.dirty_ratio=20으로 조정<br>* References<br>- Linux iostat Tuning: https://www.redhat.com/sysadmin/linux-iostat (70% util 권고: 안정 throughput, 85% 초과 시 2배 latency) |
| Latency 모니터링 | 서버에서 iostat -x 1 10 | 정상: svctm 2ms 미만임<br>취약: svctm 5ms 초과임 | 높은 latency 시 I/O 응답 지연, 사용자 지연 경험 증가, 큐잉으로 IOPS 감소, 장기적으로 용량 낭비 발생함 | latency 최적화: fio 도구로 벤치마크 후 디스크 타입 업그레이드함 | * 확인방법<br>서버에서 iostat -x \| grep svctm<br>* 설정방법<br>서버 CPU/메모리 최적화 및 iSCSI offload<br>* References<br>- Storage Latency Best Practices: https://www.netapp.com/us/en/resources/media/wp-3390-storage-latency.html (2ms 기준: 최적 응답, 5ms 초과 시 50% 성능 저하) |
| 파일시스템 선택 | 서버에서 mount \| grep <volume> | 정상: xfs 또는 ext4 with noatime임<br>취약: ext3 또는 atime 활성화임 | 오래된 FS 사용 시 메타데이터 오버헤드 증가, 쓰기 성능 20% 저하, atime으로 불필요 I/O 발생으로 용량 소모 가속화됨 | FS 업그레이드: mkfs.xfs -f /dev/<volume> 후 마운트 옵션 noatime 추가함 | * 확인방법<br>서버에서 mount 옵션 확인<br>* 설정방법<br>/etc/fstab에 noatime 추가 후 remount<br>* References<br>- XFS vs EXT4 Performance: https://www.phoronix.com/scan.php?page=article&item=ext4-xfs-2020&num=1 (xfs 15% 우수: 쓰기 throughput, atime 시 25% I/O 증가) |
| LVM 스트라이핑 설정 | 서버에서 lvdisplay /dev/<vg>/<lv> | 정상: stripe count 4 이상임<br>취약: stripe count 1 (no striping)임 | 스트라이핑 미사용 시 I/O 병렬화 부족, throughput 저하로 대용량 작업 지연, 단일 디스크 부하로 latency 증가함 | 스트라이핑 활성화: lvcreate -i 4 -I 64k로 새 LV 생성 후 데이터 이동함 | * 확인방법<br>서버에서 lvdisplay \| grep "Stripes"<br>* 설정방법<br>여러 PV에 균등 분산<br>* References<br>- LVM Striping Guide: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/logical_volume_manager_administration/striping (4 stripe 권고: 4배 throughput, 1 stripe 시 병목 70%) |
| 서버 큐 깊이 설정 | 서버에서 cat /sys/block/<device>/queue/nr_requests | 정상: nr_requests 128 이상임<br>취약: nr_requests 32 미만임 | 낮은 큐 깊이 시 I/O 큐잉 증가, throughput 제한으로 성능 저하, 피크 시 지연 폭증함 | 큐 깊이 증가: echo 256 > /sys/block/<device>/queue/nr_requests 실행함 | * 확인방법<br>서버에서 sysfs 값 확인<br>* 설정방법<br>udev 규칙으로 영구 설정<br>* References<br>- Linux Block Queue Tuning: https://www.kernel.org/doc/html/latest/block/ (128 권고: 최적 큐잉, 32 미만 시 40% throughput 저하) |
| 스냅샷 생성 빈도 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: 하루 4회 이하 생성임<br>취약: 하루 12회 이상 생성임 | 과도한 빈도 시 스냅샷 오버헤드 증가, I/O 15% 저하, 용량 소모 가속으로 성능 영향 확대됨 | 빈도 조정: 스케줄 hour 간격 6시간 이상으로 설정함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> \| wc -l (일일 카운트)<br>* 설정방법<br>비즈니스 요구에 따라 주간/일간 조합<br>* References<br>- Snapshot Frequency Best Practices: https://docs.aws.amazon.com/ebs/latest/userguide/ebs-snapshots.html (4회 이하 권고: 오버헤드 최소, 12회 초과 시 20% I/O 영향) |
| 복제 대역폭 할당 | scpcli baremetal-blockstorage volume replication show --volume_id <volume_id> | 정상: 네트워크 대역폭 100Mbps 이상 할당됨<br>취약: 대역폭 50Mbps 미만임 | 부족한 대역폭 시 복제 지연, 변경 데이터 누적으로 throughput 저하, 동기화 실패로 성능 변동성 증가함 | 대역폭 증가: VPC QoS로 iSCSI 트래픽 200Mbps 우선 할당함 | * 확인방법<br>복제 lag 및 네트워크 iperf 테스트<br>* 설정방법<br>SCP 네트워크 정책 업데이트<br>* References<br>- Replication Bandwidth Requirements: https://learn.microsoft.com/en-us/azure/site-recovery/azure-to-azure-how-to-enable-replication (100Mbps 기준: 안정 동기화, 50Mbps 미만 시 지연 3배) |
| 용량 예측 정확도 | SCP 용량 보고서 (CLI 미지원) | 정상: 예측 오차 10% 이내임<br>취약: 예측 오차 30% 초과임 | 부정확 예측 시 용량 초과/낭비, 성능 저하 또는 비용 증가, 긴급 확장 시 다운타임 발생함 | 예측 모델 개선: 과거 사용 패턴 분석 도구로 6개월 예측 정확도 향상함 | * 확인방법<br>SCP 콘솔 용량 트렌드 리포트 검토<br>* 설정방법<br>머신러닝 기반 예측 도입<br>* References<br>- Cloud Capacity Planning: https://www.gartner.com/en/information-technology/insights/capacity-planning (10% 오차 권고: 비용 최적화, 30% 초과 시 25% 낭비) |
| 오버프로비저닝 비율 | scpcli baremetal-blockstorage volume list | 정상: 총 할당 용량 대비 실제 사용 80% 이내임<br>취약: 실제 사용 120% 초과임 | 과도 오버프로비저닝 시 IOPS 제한 초과, 성능 저하로 지연 증가, 비용 비효율로 예산 초과 발생함 | 프로비저닝 조정: 미사용 볼륨 삭제 또는 리사이즈함 | * 확인방법<br>scpcli baremetal-blockstorage volume list 후 총 size_gb vs df 합산<br>* 설정방법<br>태깅으로 사용률 추적<br>* References<br>- EBS Provisioned IOPS: https://docs.aws.amazon.com/ebs/latest/userguide/ebs-provisioned-iops.html (80% 비율 권고: 성능 안정, 120% 초과 시 35% 제한) |
| I/O 패턴 분석 | 서버에서 iotop 또는 fio --name=randread --size=1G | 정상: 80% 읽기/20% 쓰기 패턴 준수임<br>취약: 쓰기 비율 50% 초과임 | 쓰기 중심 패턴 시 내구성 오버헤드 증가, latency 2배 상승, 용량 소모 가속화됨 | 패턴 최적화: 쓰기 버퍼링 또는 AIO 활성화로 읽기 중심 전환함 | * 확인방법<br>서버에서 iotop -a 실행 후 비율 계산<br>* 설정방법<br>fio 벤치마크로 패턴 확인 후 튜닝<br>* References<br>- I/O Patterns in Storage: https://www.usenix.org/legacy/event/fast05/tech/full_papers/agrawal/agrawal.pdf (80/20 패턴 기준: 최적 throughput, 50% 쓰기 시 40% latency 증가) |
| 볼륨 크기 최적화 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> | 정상: size_gb가 워크로드 최소 2TB 이상임<br>취약: size_gb 500GB 미만임 | 작은 볼륨 시 IOPS 상한 낮음, 고부하 시 성능 제한, 확장 빈도 증가로 운영 부하 발생함 | 크기 확대: scpcli baremetal-blockstorage volume create로 4TB 볼륨 생성 후 마이그레이션함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep size_gb<br>* 설정방법<br>예상 성장률 고려 20% 여유<br>* References<br>- EBS Volume Size Impact: https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html#gp3-volume-performance (2TB 이상: IOPS 3000 베이스, 500GB 미만 시 50% 제한) |
| 스냅샷 압축 비율 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: snapshot_size_mb가 원본 50% 미만임<br>취약: snapshot_size_mb가 원본 80% 초과임 | 낮은 압축 시 용량 소모 증가, 저장 비용 상승, 성능 영향으로 생성 시간 지연 발생함 | 압축 활성화: 서버 측 데이터 압축 도구 사용 후 스냅샷 생성함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> \| grep snapshot_size_mb vs size_gb<br>* 설정방법<br>fsarchiver 등 압축 FS 도입<br>* References<br>- Snapshot Compression: https://www.bacula.org/en/manual/BackupConcepts/Snapshot.html (50% 압축 권고: 용량 절감 50%, 80% 초과 시 비용 2배) |
| 네트워크 MTU 설정 | 서버에서 ip link show | 정상: MTU 9000 (jumbo frame)임<br>취약: MTU 1500임 | 작은 MTU 시 패킷 오버헤드 증가, throughput 20% 저하, iSCSI 전송 효율 저하로 지연 발생함 | Jumbo frame 활성화: ip link set <interface> mtu 9000 실행함 | * 확인방법<br>서버에서 ip link show \| grep mtu<br>* 설정방법<br>스위치 및 iSCSI 타겟 MTU 일치 확인<br>* References<br>- Jumbo Frames for iSCSI: https://kb.netapp.com/on-prem/ontap/da-nas/da-nas-cifs/Jumbo_Frames_for_iSCSI (9000 MTU 권고: 30% throughput 향상, 1500 시 오버헤드 15%) |
| 서버 메모리 버퍼 | 서버에서 free -h | 정상: available 메모리 20GB 이상임<br>취약: available 메모리 4GB 미만임 | 메모리 부족 시 디스크 I/O 증가, 캐싱 미작동으로 latency 상승, 성능 저하로 쓰기 작업 지연됨 | 메모리 증설: 서버 인스턴스 업그레이드 또는 swap 비활성화함 | * 확인방법<br>서버에서 free -h 명령어 실행<br>* 설정방법<br>vm.swappiness=10으로 조정<br>* References<br>- Memory Tuning for Storage: https://www.snia.org/sites/default/files/SNIA_Tutorial_Memory_Tuning_for_Storage.pdf (20GB 기준: 캐싱 최적, 4GB 미만 시 I/O 3배 증가) |
| 디스크 파티션 정렬 | 서버에서 parted /dev/<volume> print | 정상: 파티션 시작 섹터 2048 이상 정렬됨<br>취약: 파티션 시작 섹터 63 미만임 | 비정렬 파티션 시 읽기/쓰기 지연 10-20% 증가, SSD에서 IOPS 저하, 용량 효율 저하 발생함 | 파티션 재정렬: fdisk로 1MiB 정렬 재생성 후 데이터 복사함 | * 확인방법<br>서버에서 parted print \| grep start<br>* 설정방법<br>새 파티션 생성 시 align-check 사용<br>* References<br>- Partition Alignment: https://docs.microsoft.com/en-us/windows-server/storage/disk-management/overview-of-disk-management (2048 섹터 권고: 15% 성능 향상, 미정렬 시 지연 증가) |
| 성능 베이스라인 기록 | SCP 성능 리포트 (CLI 미지원) | 정상: 월간 베이스라인 보고서 유지됨<br>취약: 3개월 이상 업데이트 안 됨 | 베이스라인 미유지 시 이상 징후 탐지 어려움, 성능 저하 조기 대응 실패로 용량/throughput 문제 확대됨 | 베이스라인 업데이트: fio 벤치마크 월 1회 실행 및 기록함 | * 확인방법<br>내부 리포트 저장소 확인<br>* 설정방법<br>자동화 스크립트로 IOPS/throughput 측정<br>* References<br>- Storage Performance Baselines: https://www.seagate.com/files/www-content/solutions-content/business-storage/en-us/docs/Performance_Best_Practices_wp.pdf (월간 업데이트 권고: 이상 20% 조기 탐지) |
| 용량 성장률 추적 | scpcli baremetal-blockstorage volume list 후 df 집계 | 정상: 월 성장률 5% 이내임<br>취약: 월 성장률 15% 초과임 | 급격 성장 시 용량 초과 위험, 성능 영향으로 I/O 큐잉 증가, 예산 초과 및 확장 다운타임 발생함 | 성장 예측: 트렌드 분석 도구로 20% 여유 용량 유지함 | * 확인방법<br>과거 df 로그 비교<br>* 설정방법<br>알림으로 10% 성장 시 경고<br>* References<br>- Capacity Growth Forecasting: https://www.ibm.com/docs/en/storage?topic=planning-capacity-growth (5% 월 성장 권고: 안정 관리, 15% 초과 시 30% 위험 증가) |

## 운영

| 점검내용 | 점검방법 | 점검기준 | 취약시 문제점 | 개선방안 | 참고 |
|----------|----------|----------|---------------|----------|------|
| 로그 레벨 설정 | SCP 콘솔 또는 서버 로그 설정 확인 (CLI 미지원, 가정) | 정상: 로그 레벨이 INFO 이상으로 설정되어 스토리지 이벤트 상세 기록됨<br>취약: 로그 레벨이 ERROR 이하로 제한됨 | 로그 레벨이 제한되어 있으면 스토리지 I/O 지연이나 복제 지연 같은 경고 이벤트가 누락되어 장애 원인 분석 지연 발생, 문제 해결 시간 증가로 다운타임 확대됨, 반복 장애로 운영 비용 상승함 | 로그 레벨을 INFO로 상향 조정하여 상세 이벤트 캡처함 | * 확인방법<br>SCP 모니터링 콘솔에서 로그 정책 확인 또는 서버 syslog.conf 검사<br>* 설정방법<br>SCP 콘솔에서 로그 레벨 정책 업데이트 후 재시작, 서버에서 rsyslog.conf에 level INFO 추가<br>* References<br>- AWS CloudWatch Logs Best Practices: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html (INFO 레벨 권고: 상세 진단 용이, ERROR 제한 시 50% 이벤트 누락으로 분석 지연 2배 증가) |
| 패치 적용 이력 | scpcli baremetal-blockstorage volume list 후 패치 로그 확인 (서버 uptrack-history) | 정상: 최근 30일 내 보안 패치 적용 이력 100%임<br>취약: 30일 내 패치 미적용 볼륨 10% 이상임 | 패치 미적용 시 알려진 취약점 노출로 보안 침해 발생, 스토리지 데이터 유출 위험 증가, 규정 준수 위반으로 감사 실패, 장애 복구 시 추가 취약점 악화됨 | 자동 패치 스케줄 적용: SCP 패치 매니저로 주간 패치 적용 후 검증함 | * 확인방법<br>서버에서 uptrack-history -l 명령어로 이력 확인, scpcli로 볼륨 목록과 매핑<br>* 설정방법<br>SCP 콘솔에서 자동 패치 그룹 설정, 적용 후 재부팅 테스트<br>* References<br>- Red Hat Patch Management: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/managing-patching-and-updating-the-kernel (30일 100% 적용 권고: 취약점 90% 차단, 미적용 시 침해 위험 3배 증가) |
| EOS/EOL 확인 | SCP 콘솔에서 볼륨 펌웨어 버전 확인 | 정상: 모든 볼륨 펌웨어가 EOS 2년 이내 버전임<br>취약: EOS 1년 초과 버전 사용임 | EOS 버전 사용 시 보안 업데이트 중단으로 취약점 지속 노출, 호환성 문제로 스냅샷/복제 실패 발생, 지원 종료로 장애 시 복구 불가, 비용 증가로 운영 부담 확대됨 | 펌웨어 업그레이드: SCP 콘솔에서 최신 버전으로 업데이트 후 테스트함 | * 확인방법<br>SCP 콘솔 펌웨어 리포트 다운로드 후 EOS 날짜 확인<br>* 설정방법<br>SCP 지원팀 통해 배치 업그레이드, 다운타임 최소화<br>* References<br>- Dell EMC End-of-Support Policy: https://www.dell.com/support/kbdoc/en-us/000131473/end-of-support-dates-for-dell-emc-products (2년 이내 EOS 권고: 지원 유지, 1년 초과 시 취약점 40% 증가 및 복구 실패율 상승) |
| 백업 검증 주기 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: 최근 30일 내 스냅샷 복원 테스트 1회 이상 성공임<br>취약: 90일 내 복원 테스트 없음임 | 백업 검증 미실시 시 데이터 무결성 불확실, 복구 시 실패로 데이터 손실 영구화, RTO 지연으로 비즈니스 중단 확대, 신뢰성 저하로 고객 불만 증가함 | 주기적 복원 테스트: 월 1회 scpcli baremetal-blockstorage volume snapshot restore 실행 후 데이터 검증함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> 후 테스트 로그 확인<br>* 설정방법<br>자동화 스크립트로 테스트 및 리포트 생성<br>* References<br>- NIST Backup Validation: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf (월 1회 검증 권고: 무결성 99% 확보, 90일 미검증 시 실패율 25% 증가) |
| 감사 로그 유지 기간 | SCP 감사 로그 콘솔 확인 | 정상: 감사 로그 90일 이상 보관됨<br>취약: 감사 로그 30일 미만 보관임 | 짧은 보관 기간 시 보안 이벤트 추적 불가, 감사 시 증거 부족으로 규정 위반, 장애 원인 분석 지연으로 재발 위험 증가, 법적 책임 확대됨 | 로그 보관 기간 연장: SCP 정책에서 180일로 설정함 | * 확인방법<br>SCP 콘솔 감사 리포트에서 retention 기간 확인<br>* 설정방법<br>S3 아카이빙 연동으로 장기 보관<br>* References<br>- PCI DSS Logging Requirements: https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss (90일 보관 권고: 추적 용이, 30일 미만 시 감사 실패율 60% 증가) |
| 변경 관리 프로세스 준수 | 내부 변경 로그 확인 (CLI 미지원) | 정상: 모든 볼륨 변경에 승인 기록 있음<br>취약: 변경 중 20% 이상 무승인임 | 무승인 변경 시 구성 오류 누적, 장애 발생 시 원인 불명으로 복구 지연, 운영 혼란으로 다운타임 증가, 규정 준수 위반됨 | 변경 워크플로 도입: SCP 티켓 시스템으로 모든 변경 승인 프로세스 적용함 | * 확인방법<br>내부 Jira/티켓 로그에서 볼륨 ID와 매핑 확인<br>* 설정방법<br>자동 승인 워크플로 설정 및 교육<br>* References<br>- ITIL Change Management: https://www.axelos.com/best-practice-solutions/itil/what-is-itil/itil-change-management (100% 승인 권고: 오류 70% 감소, 무승인 시 재발 40% 증가) |
| 비용 최적화 리뷰 | scpcli baremetal-blockstorage volume list 후 비용 리포트 | 정상: 미사용 볼륨 5% 이내임<br>취약: 미사용 볼륨 15% 초과임 | 미사용 자원 증가 시 비용 낭비, 예산 초과로 운영 예산 압박, 리소스 비효율로 성능 영향 확대됨 | 자원 정리: scpcli baremetal-blockstorage volume delete로 미사용 볼륨 제거 후 리포트 생성함 | * 확인방법<br>scpcli baremetal-blockstorage volume list 후 df -h와 비교<br>* 설정방법<br>월간 비용 리뷰 미팅 및 태그 기반 추적<br>* References<br>- AWS Cost Optimization: https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-pillar/cost-optimization-pillar.html (5% 미사용 권고: 비용 20% 절감, 15% 초과 시 30% 낭비) |
| 모니터링 대시보드 설정 | SCP 모니터링 콘솔 확인 | 정상: IOPS, latency, 용량 메트릭스 100% 커버됨<br>취약: 핵심 메트릭스 50% 미커버임 | 불완전 모니터링 시 성능 이상 조기 탐지 실패, 장애 확대, 운영 부하 증가로 응답 지연됨 | 대시보드 확장: SCP 콘솔에서 모든 스토리지 메트릭스 추가 및 알림 설정함 | * 확인방법<br>SCP 대시보드 위젯 목록 확인<br>* 설정방법<br>커스텀 대시보드 템플릿 적용<br>* References<br>- Google Cloud Monitoring Best Practices: https://cloud.google.com/monitoring/docs/best-practices (100% 커버 권고: 탐지 시간 50% 단축, 50% 미커버 시 장애 2배 지연) |
| DR 테스트 주기 | 내부 DR 테스트 로그 확인 | 정상: 분기 1회 DR 테스트 성공임<br>취약: 6개월 내 DR 테스트 없음임 | 테스트 미실시 시 DR 프로세스 미숙, 재해 시 복구 실패로 데이터 손실, RTO/RPO 위반으로 비즈니스 중단 장기화됨 | DR 테스트 스케줄: 분기 scpcli baremetal-blockstorage volume replication show로 failover 테스트 실행함 | * 확인방법<br>내부 문서에서 최근 테스트 날짜 확인<br>* 설정방법<br>연간 DR 계획에 포함 및 시뮬레이션<br>* References<br>- NIST Disaster Recovery Testing: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-34r1.pdf (분기 테스트 권고: 성공률 95%, 6개월 미실시 시 실패 40% 증가) |
| 인시던트 응답 계획 | 내부 IRP 문서 확인 | 정상: 스토리지 장애 시 응답 시간 15분 이내임<br>취약: 응답 시간 30분 초과임 | 느린 응답 시 다운타임 확대, 데이터 무결성 손상, 고객 불만 증가로 신뢰성 저하, 비용 배상 발생함 | IRP 업데이트: 10분 응답 목표로 팀 훈련 및 알림 체인 강화함 | * 확인방법<br>IRP 시뮬레이션 테스트 시간 측정<br>* 설정방법<br>페이지/이메일 알림 시스템 통합<br>* References<br>- SANS Incident Response: https://www.sans.org/reading-room/whitepapers/incident/ir-award-winning-practical-guide-34001 (15분 응답 권고: 다운타임 50% 단축, 30분 초과 시 영향 2배) |
| 사용자 접근 로그 | scpcli baremetal-blockstorage volume show --volume_id <volume_id> 후 created_by/modified_by 확인 | 정상: 모든 접근에 사용자 ID 기록됨<br>취약: 10% 이상 접근 로그 누락임 | 로그 누락 시 무단 접근 탐지 불가, 보안 사고 시 책임 추적 어려움, 규정 위반으로 벌금 발생, 재발 위험 증가함 | 접근 로깅 강화: SCP IAM 정책으로 모든 API 호출 로그 활성화함 | * 확인방법<br>scpcli baremetal-blockstorage volume show --volume_id <volume_id> \| grep created_by,modified_by<br>* 설정방법<br>Audit 로그 S3 내보내기 설정<br>* References<br>- AWS IAM Logging: https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html (100% 기록 권고: 추적 99% 확보, 누락 시 감사 실패 70%) |
| 메인터넌스 윈도우 설정 | SCP 스케줄러 콘솔 확인 | 정상: 월 1회 야간 메인터넌스 윈도우 정의됨<br>취약: 메인터넌스 윈도우 미설정임 | 무계획 메인터넌스 시 피크타임 중단 발생, 서비스 영향 확대, 고객 불만 증가로 SLA 위반됨 | 윈도우 정의: SCP 콘솔에서 주말 02:00-04:00 윈도우 설정 및 알림함 | * 확인방법<br>SCP 스케줄러에서 윈도우 목록 확인<br>* 설정방법<br>영향 최소화 시간대 선택 및 사전 통보<br>* References<br>- Azure Maintenance Windows: https://learn.microsoft.com/en-us/azure/virtual-machines/maintenance-notifications (월 1회 권고: 영향 80% 감소, 미설정 시 불만 3배) |
| 성능 리뷰 주기 | 내부 리뷰 로그 확인 | 정상: 분기 1회 성능 리뷰 완료임<br>취약: 6개월 내 리뷰 없음임 | 리뷰 미실시 시 성능 저하 누적, 용량 초과 예방 실패, 비용 증가로 운영 효율 저하됨 | 리뷰 스케줄: 분기 scpcli baremetal-blockstorage volume list로 메트릭스 분석 리포트 생성함 | * 확인방법<br>내부 문서에서 최근 리뷰 날짜 확인<br>* 설정방법<br>자동 리포트 도구 연동<br>* References<br>- ITIL Performance Review: https://www.axelos.com/best-practice-solutions/itil/what-is-itil/itil-service-review (분기 리뷰 권고: 최적화 25% 향상, 6개월 미실시 시 저하 30%) |
| 알림 시스템 테스트 | SCP 알림 콘솔에서 테스트 전송 | 정상: 알림 도착 시간 1분 이내임<br>취약: 알림 도착 5분 초과 또는 실패임 | 알림 지연 시 조기 대응 실패, 장애 확대, 운영자 피로 증가로 오탐지 발생함 | 알림 최적화: SCP 콘솔에서 다중 채널(SMS/이메일) 설정 및 주간 테스트함 | * 확인방법<br>테스트 이벤트 발생 후 도착 시간 측정<br>* 설정방법<br>임계값 세밀 조정<br>* References<br>- PagerDuty Alerting Best Practices: https://www.pagerduty.com/resources/learn/blog/best-practices-for-incident-response-alerting/ (1분 도착 권고: 응답 60% 단축, 5분 초과 시 확대 50%) |
| 문서화 완성도 | 내부 지식 베이스 확인 | 정상: 스토리지 운영 매뉴얼 95% 커버됨<br>취약: 매뉴얼 커버리지 70% 미만임 | 불완전 문서 시 신입 운영자 오류 증가, 지식 전수 실패로 장애 재발, 훈련 비용 상승함 | 문서 업데이트: Confluence에 매뉴얼 작성 및 연 2회 리뷰함 | * 확인방법<br>지식 베이스 검색으로 커버리지 확인<br>* 설정방법<br>템플릿 사용 및 피드백 루프<br>* References<br>- Knowledge Management in IT: https://www.servicenow.com/content/dam/servicenow-assets/public/en-us/doc-type/resource-center/data-sheet/ds-knowledge-management.pdf (95% 커버 권고: 효율 40% 향상, 70% 미만 시 오류 35% 증가) |
| 백업 저장소 무결성 | scpcli baremetal-blockstorage volume snapshot list --volume_id <volume_id> | 정상: 스냅샷 체크섬 검증 100% 통과임<br>취약: 5% 이상 스냅샷 무결성 실패임 | 무결성 실패 시 복구 데이터 손상, 백업 신뢰성 저하로 재해 시 데이터 손실, 추가 백업 비용 발생함 | 무결성 검사 자동화: 월 1회 체크섬 검증 스크립트 실행함 | * 확인방법<br>scpcli baremetal-blockstorage volume snapshot list 후 md5sum 비교<br>* 설정방법<br>S3 버킷 무결성 정책 적용<br>* References<br>- Backup Integrity Checks: https://www.veritas.com/content/dam/veritas/docs/en_US/resources/backup-integrity-checks.pdf (100% 통과 권고: 신뢰성 99%, 5% 실패 시 손실 20% 증가) |
| 용량 리포트 자동화 | SCP 리포트 스케줄 확인 | 정상: 주간 용량 리포트 자동 생성됨<br>취약: 리포트 수동 또는 월 1회임 | 수동 리포트 시 예측 오류 증가, 용량 초과 위험, 운영 지연으로 비용 초과 발생함 | 자동화 설정: SCP 스케줄러로 주간 이메일 리포트 활성화함 | * 확인방법<br>SCP 콘솔 스케줄러 목록 확인<br>* 설정방법<br>쿼리 기반 리포트 템플릿 생성<br>* References<br>- Automated Reporting in Cloud: https://cloud.google.com/architecture/automated-reporting (주간 권고: 예측 정확도 80%, 월 1회 시 오류 25% 증가) |
| 보안 패치 우선순위 | 서버 패치 로그 확인 | 정상: 고위험 패치 즉시 적용 (24시간 이내)임<br>취약: 고위험 패치 7일 초과 미적용임 | 지연 패치 시 취약점 악용으로 데이터 침해, 규정 위반 벌금, 신뢰성 저하로 고객 이탈 발생함 | 우선순위 정책: CVSS 7 이상 패치 24시간 내 적용 스크립트 실행함 | * 확인방법<br>uptrack-history에서 CVSS 점수와 적용 시간 확인<br>* 설정방법<br>자동 패치 그룹화<br>* References<br>- NIST Patch Prioritization: https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-40r4.pdf (24시간 적용 권고: 위험 90% 차단, 7일 초과 시 침해 50% 증가) |
| 운영자 훈련 기록 | 내부 훈련 로그 확인 | 정상: 연 2회 스토리지 운영 훈련 완료율 100%임<br>취약: 훈련 완료율 70% 미만임 | 훈련 부족 시 운영 오류 증가, 장애 대응 지연, 지식 격차로 효율 저하됨 | 훈련 프로그램 강화: 분기 온라인 세션 및 시뮬레이션 훈련 실시함 | * 확인방법<br>HR 시스템에서 훈련 완료 로그 확인<br>* 설정방법<br>의무화 및 인증 부여<br>* References<br>- IT Training Best Practices: https://www.isaca.org/resources/isaca-journal/issues/2020/volume-4/it-training-and-awareness-best-practices (연 2회 100% 권고: 오류 40% 감소, 70% 미만 시 지연 30%) |
| 장애 재발 방지 조치 | 내부 RCA 리포트 확인 | 정상: 지난 장애 90%에 재발 방지 액션 완료임<br>취약: 50% 미만 액션 미완료임 | 미완료 조치 시 장애 재발, 다운타임 누적, 신뢰성 저하로 SLA 위반 반복됨 | RCA 프로세스 강화: 모든 장애에 액션 트래킹 티켓 생성함 | * 확인방법<br>티켓 시스템에서 RCA 상태 확인<br>* 설정방법<br>월간 재발 리뷰 미팅<br>* References<br>- ITIL Problem Management: https://www.axelos.com/best-practice-solutions/itil/what-is-itil/itil-problem-management (90% 완료 권고: 재발 70% 감소, 50% 미만 시 반복 40% 증가) |

---
### 구성

| 점검내용 | 점검방법 | 점검기준 | 취약시 문제점 | 개선방안 | 참고 |
|----------|----------|----------|---------------|----------|------|
| 볼륨 이름 고유성 확인 | scpcli baremetal-blockstorage volume list --name <볼륨명> --limit 100 | 정상: 목록에 동일 이름 볼륨 1개 이하<br>취약: 동일 이름 볼륨 2개 이상 | 볼륨 이름 중복으로 인한 관리 혼란 발생, 잘못된 볼륨 선택 시 데이터 오작업으로 인한 손실 위험 증가, 운영 효율성 저하 | 볼륨 생성 시 프로젝트별 또는 용도별 고유 네이밍 컨벤션 적용하여 이름 중복 방지 | * 확인방법: list 명령어로 이름 필터링하여 중복 검사<br>* 설정방법: create 시 --name에 고유 값 지정<br>* References: IBM Cloud Block Storage Best Practices - Naming Conventions (https://cloud.ibm.com/docs/BlockStorage?topic=BlockStorage-best-practices-classic) |
| 볼륨 태그 설정 여부 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: tags 배열에 최소 1개 이상 키-값 쌍 존재<br>취약: tags 배열이 빈 배열 또는 null | 태그 미설정으로 자산 관리 어려움, 비용 추적 및 보안 그룹화 불가, 감사 시 자원 식별 지연으로 컴플라이언스 위반 위험 | 볼륨 생성 또는 수정 시 --tags 옵션으로 키-값 쌍 추가하여 메타데이터 관리 강화 | * 확인방법: show 명령어의 tags 필드 검사<br>* 설정방법: create 시 --tags '{\"key\": \"env\", \"value\": \"prod\"}' 추가<br>* References: AWS EBS Tagging Best Practices (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html) |
| 디스크 타입 적합성 (워크로드 기반) | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: disk_type이 SSD (고IOPS 워크로드) 또는 HDD (대용량 저장)로 용도 적합<br>취약: disk_type이 워크로드와 불일치 (e.g., HDD on high IOPS) | 부적합 디스크 타입으로 성능 저하 발생, IOPS/지연 시간 초과로 애플리케이션 응답 지연, 비용 낭비 | 워크로드 분석 후 disk_type 재설정 또는 신규 볼륨 생성 시 적합 타입 선택 | * 확인방법: show의 disk_type 필드와 워크로드 매핑<br>* 설정방법: create 시 --disk_type SSD/HDD 지정<br>* References: Google Cloud Persistent Disk Performance Classes (https://cloud.google.com/compute/docs/disks/performance) |
| 볼륨 용량 크기 적절성 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: size_gb가 현재 사용량의 1.2배 이상 여유 공간 확보<br>취약: size_gb가 사용량 초과 또는 80% 이상 점유 | 용량 부족으로 쓰기 실패 및 데이터 손실 발생, 빈번한 확장 작업으로 운영 부하 증가, 성능 저하 | 모니터링 도구로 사용량 추적 후 필요 시 용량 확장 또는 신규 볼륨 할당 | * 확인방법: show의 size_gb와 모니터링 사용량 비교<br>* 설정방법: create 시 --size_gb 적정 값 지정<br>* References: Azure Disk Storage Best Practices - Sizing (https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types) |
| 볼륨 연결(Attachment) 상태 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: attachments 배열에 연결된 서버 정보 존재 및 상태 'attached'<br>취약: attachments 빈 배열 또는 'detached' 상태 | 볼륨 미연결로 데이터 접근 불가, 서버 부팅 지연, 데이터 무결성 손상 위험 | create attachment 명령어로 서버 연결 재확인 및 유지 | * 확인방법: show의 attachments 필드 검사<br>* 설정방법: attachment create 시 --attachments 지정<br>* References: AWS EBS Volume Attachment Best Practices (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-attaching-volume.html) |
| 스냅샷 활성화 여부 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: is_snapshot_activated = True<br>취약: is_snapshot_activated = False | 스냅샷 미활성화로 데이터 백업 불가, 장애 시 복구 지연, RPO/RTO 위반으로 비즈니스 연속성 저하 | snapshot-rate create 명령어로 활성화 설정 | * 확인방법: show의 is_snapshot_activated 필드<br>* 설정방법: snapshot-rate create --volume_id <ID> --snapshot_rate 100<br>* References: Google Cloud Disk Snapshots Best Practices (https://cloud.google.com/compute/docs/disks/snapshots) |
| 스냅샷 용량 비율 설정 | scpcli baremetal-blockstorage volume snapshot-rate show --volume_id <볼륨ID> (CLI 기반 가정) | 정상: snapshot_rate >= 50% (볼륨 용량 대비)<br>취약: snapshot_rate < 50% 또는 미설정 | 스냅샷 공간 부족으로 백업 실패, 오래된 스냅샷 삭제 강제 발생으로 데이터 손실, 저장 비용 증가 | snapshot-rate set으로 비율 조정하여 여유 공간 확보 | * 확인방법: snapshot-rate show (CLI 확장 가정)<br>* 설정방법: snapshot-rate set --volume_id <ID> --snapshot_rate 100<br>* References: Azure Disk Snapshot Capacity Planning (https://learn.microsoft.com/en-us/azure/virtual-machines/snapshot-storage-costs) |
| 스냅샷 스케줄 시간 설정 | scpcli baremetal-blockstorage volume snapshot-schedule show --volume_id <볼륨ID> (CLI 기반 가정) | 정상: hour가 저부하 시간대 (e.g., 02:00~06:00) 설정<br>취약: hour가 피크 시간대 | 스냅샷 실행 시 I/O 부하 증가로 성능 저하, 네트워크 대역폭 소모로 다른 작업 지연 | snapshot-schedule set으로 저부하 시간 재설정 | * 확인방법: snapshot-schedule show의 hour 필드<br>* 설정방법: snapshot-schedule set --volume_id <ID> --hour 3<br>* References: AWS EBS Snapshot Scheduling Best Practices (https://aws.amazon.com/premiumsupport/knowledge-center/ebs-snapshots-scheduling/) |
| 스냅샷 주기(빈도) 설정 | scpcli baremetal-blockstorage volume snapshot-schedule show --volume_id <볼륨ID> | 정상: day_of_week 미지정 (DAILY) 또는 WEEKLY로 RPO 24시간 이내<br>취약: day_of_week 지정으로 주기 초과 (e.g., MONTHLY) | 백업 빈도 부족으로 데이터 손실 범위 확대, 복구 시간 증가, 규정 준수 실패 | snapshot-schedule set으로 DAILY 주기 적용 | * 확인방법: show의 frequency 필드 (DAILY/WEEKLY)<br>* 설정방법: snapshot-schedule set --volume_id <ID> --day_of_week (미지정)<br>* References: IBM Cloud Block Storage Snapshot Frequency (https://cloud.ibm.com/docs/BlockStorage?topic=BlockStorage-snapshots) |
| 복제 활성화 및 주기 설정 | scpcli baremetal-blockstorage volume replication show --volume_id <볼륨ID> | 정상: cycle이 5MIN~1HOUR 범위 내 설정<br>취약: cycle 미설정 또는 1DAY 초과 | 복제 지연으로 DR 시 데이터 불일치, 장애 복구 지연, RPO 위반 | replication create 또는 set-cycle으로 주기 최적화 | * 확인방법: replication show의 cycle 필드<br>* 설정방법: replication-cycle set --volume_id <ID> --cycle 5MIN<br>* References: Google Cloud Disk Replication Best Practices (https://cloud.google.com/compute/docs/disks/async-replication) |
| 복제 정책 설정 | scpcli baremetal-blockstorage volume replication-policy show --volume_id <볼륨ID> (CLI 기반 가정) | 정상: policy가 SYNC 또는 ASYNC로 명시적 설정<br>취약: policy 미설정 또는 기본값 | 복제 모드 불명확으로 데이터 동기화 실패, 네트워크 오류 시 볼륨 불안정, 성능 영향 | replication-policy set으로 적합 정책 적용 | * 확인방법: replication-policy show의 policy 필드<br>* 설정방법: replication-policy set --volume_id <ID> --policy SYNC<br>* References: AWS EBS Multi-Attach Replication Policies (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html) |
| 복제 대상 리전 유효성 | scpcli baremetal-blockstorage volume replication show --volume_id <볼륨ID> | 정상: region이 다른 가용 영역 (e.g., kr-west1 to kr-west2)<br>취약: region이 소스와 동일 | 단일 리전 의존으로 DR 실패, 재해 시 전체 데이터 손실 위험 | replication create 시 다른 리전 지정 | * 확인방법: replication show의 region 필드<br>* 설정방법: replication create --volume_id <ID> --region kr-west2<br>* References: Azure Disk Geo-Redundant Storage (https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy) |
| 볼륨 그룹 태그 설정 | scpcli baremetal-blockstorage volume-group show --volume_group_id <그룹ID> | 정상: tags 배열에 최소 1개 이상 존재<br>취약: tags 빈 배열 | 그룹 자산 추적 어려움, 비용 분배 불가, 보안 정책 적용 지연 | volume-group create 시 --tags 추가 | * 확인방법: volume-group show의 tags 필드<br>* 설정방법: create 시 --tags '{\"key\": \"group\", \"value\": \"db\"}'<br>* References: Google Cloud Resource Labels Best Practices (https://cloud.google.com/compute/docs/labeling-resources) |
| 볼륨 그룹 멤버 수 적절성 | scpcli baremetal-blockstorage volume-group show --volume_group_id <그룹ID> | 정상: num_of_block_storages <= 10 (관리 용이)<br>취약: num_of_block_storages > 10 | 그룹 과부하로 스냅샷/복제 실패, 성능 병목, 유지보수 복잡성 증가 | 멤버 초과 시 신규 그룹 분리 | * 확인방법: volume-group show의 num_of_block_storages<br>* 설정방법: member add 제한 후 모니터링<br>* References: AWS EBS Volume Group Limits (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html) |
| 볼륨 그룹 목적(Purpose) 설정 | scpcli baremetal-blockstorage volume-group show --volume_group_id <그룹ID> | 정상: purpose가 명시적 (e.g., "database")<br>취약: purpose 빈 값 또는 null | 그룹 용도 불명확으로 잘못된 운영, 감사 시 컴플라이언스 문제 | volume-group create 시 purpose 필드 설정 (CLI 확장) | * 확인방법: show의 purpose 필드<br>* 설정방법: create 시 purpose 지정 (문서 기반)<br>* References: Azure Managed Disks Purpose Tagging (https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types) |
| iSCSI 타겟 IP 다중성 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: iscsi_target_ips 배열에 2개 이상 IP 존재<br>취약: iscsi_target_ips 1개 이하 | 단일 IP 실패 시 연결 상실, HA 저하, 데이터 접근 중단 | 볼륨 재생성 또는 네트워크 구성으로 다중 IP 할당 | * 확인방법: show의 iscsi_target_ips 배열 길이<br>* 설정방법: 마운트 가이드 따라 다중 경로 설정<br>* References: IBM iSCSI Multipath Best Practices (https://cloud.ibm.com/docs/BlockStorage?topic=BlockStorage-iscsi) |
| 볼륨 상태 안정성 | scpcli baremetal-blockstorage volume list --limit 100 | 정상: state가 'available' 또는 'in-use' 95% 이상<br>취약: state 'error' 또는 'deleting' 5% 이상 | 불안정 상태 볼륨으로 인한 서비스 중단, 데이터 무결성 위협, 복구 비용 증가 | 상태 이상 볼륨 즉시 delete 후 재생성 | * 확인방법: list의 state 필드 집계<br>* 설정방법: delete 후 create 재실행<br>* References: AWS EBS Volume State Monitoring (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html) |
| 볼륨 생성/수정 이력 추적 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: created_by와 modified_by가 승인된 사용자<br>취약: created_by/modified_by가 미승인 또는 빈 값 | 변경 이력 불명확으로 감사 실패, 무단 변경 탐지 불가, 보안 취약 | IAM 정책으로 사용자 추적 강화 | * 확인방법: show의 created_at/modified_at 및 by 필드<br>* 설정방법: IAM 로그 연동<br>* References: Google Cloud Audit Logs for Storage (https://cloud.google.com/logging/docs/audit) |
| 연결 서버 이름 일관성 | scpcli baremetal-blockstorage volume list --object_name <서버명> | 정상: object_name이 표준 네이밍 컨벤션 준수 (e.g., bm-server-01)<br>취약: object_name 비표준 또는 불일치 | 서버-볼륨 매핑 오류로 마운트 실패, 운영 스크립트 오류, 배포 지연 | 네이밍 규칙 문서화 및 적용 | * 확인방법: list의 object_name 필터링<br>* 설정방법: attachment create 시 일관된 object_name 사용<br>* References: Azure VM Disk Attachment Naming (https://learn.microsoft.com/en-us/azure/virtual-machines/attach-managed-disk-portal) |
| 볼륨 그룹 스냅샷 스케줄 설정 | scpcli baremetal-blockstorage volume-group snapshot-schedule show --volume_group_id <그룹ID> | 정상: hour가 그룹 멤버와 동기화<br>취약: hour 미설정 또는 불일치 | 그룹 내 스냅샷 비동기화로 데이터 불일치, 복원 시 부분 실패 | volume-group snapshot-schedule set으로 통합 설정 | * 확인방법: show의 hour 필드<br>* 설정방법: snapshot-schedule set --volume_group_id <ID> --hour 3<br>* References: AWS EBS Group Snapshots (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html) |
| 볼륨 그룹 복제 주기 설정 | scpcli baremetal-blockstorage volume-group replication show --volume_group_id <그룹ID> | 정상: cycle이 그룹 용량에 적합 (e.g., <1GB: 5MIN)<br>취약: cycle 과도 (e.g., 1DAY on large group) | 그룹 복제 지연으로 DR 지연, 네트워크 부하 증가 | replication-cycle set으로 주기 조정 | * 확인방법: replication show의 cycle<br>* 설정방법: volume-group replication-cycle set --volume_group_id <ID> --cycle 5MIN<br>* References: Google Cloud Group Replication Cycles (https://cloud.google.com/compute/docs/disks/volume-groups) |
| 복제본 볼륨 이름 접두사 일관성 | scpcli baremetal-blockstorage volume-group replication show --volume_group_id <그룹ID> | 정상: replication_volume_name_prefix가 표준 (e.g., "dr-")<br>취약: prefix 불일치 또는 미설정 | 복제본 식별 어려움, 관리 오류로 잘못된 삭제 | replication create 시 prefix 표준화 | * 확인방법: replication show의 name_prefix<br>* 설정방법: create 시 --replication_volume_name_prefix "dr-"<br>* References: IBM DR Prefix Naming (https://cloud.ibm.com/docs/BlockStorage?topic=BlockStorage-replication) |
| 볼륨 목록 정렬 기준 설정 | scpcli baremetal-blockstorage volume list --sort created_at:desc | 정상: sort가 created_at:desc 또는 name:asc로 안정적<br>취약: sort 미지정으로 무작위 순서 | 목록 조회 시 최신/중요 항목 누락, 운영 효율 저하 | list 명령어에 --sort 옵션 기본 적용 | * 확인방법: list 출력 순서 검증<br>* 설정방법: 스크립트에 --sort created_at:desc 추가<br>* References: AWS CLI List Sorting Best Practices (https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-output.html) |
| 페이지네이션 제한 설정 | scpcli baremetal-blockstorage volume list --limit 20 --offset 0 | 정상: limit <= 50, offset 적절로 전체 커버<br>취약: limit > 100으로 API 과부하 | 대량 목록 조회 시 타임아웃, API 비용 증가, 데이터 누락 | limit/offset으로 분할 조회 구현 | * 확인방법: list의 limit/offset 파라미터 테스트<br>* 설정방법: 스크립트에 --limit 20 --offset 계산<br>* References: Google Cloud List Pagination (https://cloud.google.com/compute/docs/reference/rest/v1/disks/list) |
| 볼륨 그룹 상태 안정성 | scpcli baremetal-blockstorage volume-group list --limit 100 | 정상: state가 'active' 100%<br>취약: state 'error' 1% 이상 | 그룹 불안정으로 멤버 볼륨 영향, 스냅/복제 중단 | 상태 이상 그룹 멤버 재할당 | * 확인방법: list의 state 필드 집계<br>* 설정방법: member remove/add로 복구<br>* References: Azure Disk Group Status (https://learn.microsoft.com/en-us/azure/virtual-machines/disks-enable-ultra-ssd) |

### 결함 및 오류 

| 점검내용 | 점검방법 | 점검기준 | 취약시 문제점 | 개선방안 | 참고 |
|----------|----------|----------|---------------|----------|------|
| 볼륨 상태 오류 이력 확인 | scpcli baremetal-blockstorage volume list --limit 100 | 정상: state 필드가 'available' 또는 'in-use' 100%<br>취약: state 필드가 'error' 또는 'failed' 1% 이상 | 볼륨 상태 오류로 데이터 접근 지연 발생, I/O 작업 실패로 애플리케이션 중단, 복구 지연으로 데이터 손실 위험 증가, 운영 팀 알림 누락 시 장애 확대 | 상태 오류 볼륨 즉시 show 명령어로 상세 로그 확인 후 delete 및 재생성, 모니터링 알림 설정으로 실시간 감지 | * 확인방법: list 출력에서 state 필드 집계, 오류 시 show --volume_id <ID>로 로그 검토<br>* References: AWS EBS Volume State Errors Troubleshooting - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-ebs-volumes.html |
| 볼륨 연결(Attachment) 오류 확인 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: attachments 배열에 모든 항목 상태 'attached'<br>취약: attachments 배열에 'detached' 또는 'error' 상태 1개 이상 | 연결 오류로 서버에서 볼륨 마운트 실패, 데이터 읽기/쓰기 불가로 서비스 중단, iSCSI 세션 재설정 지연으로 복구 시간 증가, 다중 연결 시 부분 실패로 무결성 손상 | attachment delete 후 create 재실행, 서버 로그에서 iSCSI 오류 코드 확인 및 재연결 스크립트 자동화 | * 확인방법: show의 attachments 필드 상태 검사<br>* References: Google Cloud Persistent Disk Attachment Errors - https://cloud.google.com/compute/docs/disks/troubleshooting#attachment_errors |
| 스냅샷 생성 실패 이력 확인 | scpcli baremetal-blockstorage volume snapshot list --volume_id <볼륨ID> | 정상: 모든 스냅샷 created_dt 최근 30일 내 성공<br>취약: 스냅샷 목록에 실패 표시 또는 누락 5% 이상 | 스냅샷 생성 실패로 백업 누락, 장애 시 복구 불가로 데이터 손실, 스케줄 충돌 시 반복 실패로 저장 공간 낭비, RPO 위반으로 규정 준수 문제 발생 | snapshot create 재시도, 스냅샷 용량 확인 후 rate 증가, 실패 로그 분석으로 원인(예: I/O 락) 제거 | * 확인방법: snapshot list의 created_dt 및 상태 필드 검토<br>* References: Azure Disk Snapshot Creation Failures - https://learn.microsoft.com/en-us/azure/virtual-machines/snapshots/troubleshoot |
| 복제 동기화 오류 확인 | scpcli baremetal-blockstorage volume replication show --volume_id <볼륨ID> | 정상: result 필드에 'synced' 상태 100%<br>취약: result에 'sync_failed' 또는 'lag' 1회 이상 | 동기화 오류로 복제본 데이터 불일치, DR 시나리오 실패로 비즈니스 연속성 저하, 네트워크 지연 누적 시 대역폭 소모 증가, 복구 시 데이터 재동기화 비용 상승 | replication-cycle set으로 주기 단축, 네트워크 로그 확인 후 정책 재설정, 실패 시 replication delete 후 재생성 | * 확인방법: replication show의 result 필드 로그 분석<br>* References: AWS EBS Replication Sync Errors - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-replication-troubleshooting.html |
| iSCSI 타겟 IP 연결 오류 확인 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: iscsi_target_ips 배열에 모든 IP 핑 성공<br>취약: iscsi_target_ips 중 1개 이상 연결 실패 | iSCSI 연결 오류로 볼륨 접근 불가, 단일 IP 실패 시 전체 트래픽 중단, 다중 경로 미설정 시 HA 저하, 서버 재부팅 빈도 증가로 운영 부하 | 서버에서 iscsiadm 로그 확인 후 multipath 재구성, 타겟 IP 재할당 스크립트 실행 | * 확인방법: show의 iscsi_target_ips로 핑 테스트<br>* References: IBM Cloud Block Storage iSCSI Errors - https://cloud.ibm.com/docs/BlockStorage?topic=BlockStorage-troubleshooting-iscsi |
| 스냅샷 스케줄 실행 실패 확인 | scpcli baremetal-blockstorage volume snapshot-schedule show --volume_id <볼륨ID> (CLI 확장 가정) | 정상: 최근 7일 실행 로그에 실패 0회<br>취약: 실행 로그에 실패 1회 이상 | 스케줄 실패로 자동 백업 누락, 수동 작업 증가로 인력 부하, 장기 누적으로 데이터 보호 취약, 알림 미설정 시 장애 인지 지연 | snapshot-schedule set으로 시간 재조정, 실패 원인(공간 부족) 해결 후 재생성 | * 확인방법: 스케줄 show 및 로그 연동 검토<br>* References: Google Cloud Disk Snapshot Schedule Failures - https://cloud.google.com/compute/docs/disks/schedule-snapshots#troubleshooting |
| 볼륨 그룹 멤버 불일치 오류 확인 | scpcli baremetal-blockstorage volume-group show --volume_group_id <그룹ID> | 정상: num_of_block_storages와 실제 멤버 일치<br>취약: num_of_block_storages와 list 불일치 1개 이상 | 멤버 불일치로 그룹 스냅샷/복제 부분 실패, 데이터 무결성 손상, 그룹 상태 'error' 전파로 전체 볼륨 영향, 복구 시 멤버 재할당 비용 증가 | member remove 후 add 재실행, 그룹 show 로그로 불일치 원인(삭제 지연) 분석 | * 확인방법: volume-group show의 num_of_block_storages 비교<br>* References: Azure Managed Disks Group Member Errors - https://learn.microsoft.com/en-us/azure/virtual-machines/disks/troubleshoot-group |
| 스냅샷 복원 실패 이력 확인 | scpcli baremetal-blockstorage volume snapshot list --volume_id <볼륨ID> | 정상: 모든 스냅샷 restore 시도 성공 로그<br>취약: restore 로그에 실패 1회 이상 | 복원 실패로 데이터 복구 불가, 다운타임 연장, 이전 스냅샷 의존 시 데이터 손실 확대, 테스트 복원 미실시로 실제 DR 실패 위험 | snapshot restore 재시도 전 볼륨 상태 확인, 실패 원인(용량 불일치) 수정 | * 확인방법: snapshot list의 restore 관련 메타데이터 검토<br>* References: AWS EBS Snapshot Restore Failures - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-snapshot.html#troubleshooting |
| 복제 정책 적용 오류 확인 | scpcli baremetal-blockstorage volume replication-policy show --volume_id <볼륨ID> (CLI 확장 가정) | 정상: policy 필드가 'applied' 상태<br>취약: policy에 'apply_failed' 표시 | 정책 오류로 복제 모드 불안정, SYNC 모드 실패 시 데이터 불일치, ASYNC 지연 누적, 정책 변경 시 이전 설정 충돌로 전체 복제 중단 | replication-policy set 재실행, 로그에서 정책 충돌 원인 제거 | * 확인방법: replication-policy show의 policy 상태<br>* References: Google Cloud Disk Replication Policy Errors - https://cloud.google.com/compute/docs/disks/async-replication#troubleshooting |
| 볼륨 삭제 실패 이력 확인 | scpcli baremetal-blockstorage volume list --limit 100 | 정상: 삭제된 볼륨 목록에 'deleted' 상태 100%<br>취약: 삭제 시도 후 'pending_delete' 또는 오류 1개 이상 | 삭제 실패로 고아 자원 누적, 비용 과다 청구, 저장 공간 낭비, 자원 한도 초과로 신규 생성 지연 | delete 재시도 전 attachment 확인, 실패 로그 분석 후 강제 삭제 | * 확인방법: list의 state 필드에서 pending 상태 필터링<br>* References: Azure Disk Deletion Errors - https://learn.microsoft.com/en-us/azure/virtual-machines/disks/troubleshoot-deletion |
| 볼륨 그룹 스냅샷 실패 확인 | scpcli baremetal-blockstorage volume-group snapshot list --volume_group_id <그룹ID> | 정상: 모든 스냅샷 created_dt 성공<br>취약: 스냅샷 목록에 실패 또는 부분 생성 1개 이상 | 그룹 스냅샷 실패로 멤버 볼륨 백업 불완전, 복원 시 데이터 불일치, 그룹 규모 클수록 영향 확대, RPO 위반 | 그룹 멤버 상태 확인 후 snapshot create 재실행 | * 확인방법: volume-group snapshot list의 created_dt 및 상태<br>* References: AWS EBS Multi-Volume Snapshot Errors - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html#troubleshooting-multi-volume |
| 복제 주기 지연 오류 확인 | scpcli baremetal-blockstorage volume replication show --volume_id <볼륨ID> | 정상: cycle 실행 간격 95% 이내 준수<br>취약: cycle 지연 5% 이상 | 주기 지연으로 RPO 초과, DR 준비 지연, 네트워크 부하 시 복제 큐 오버플로, 데이터 손실 위험 증가 | cycle set으로 주기 최적화, 네트워크 모니터링 강화 | * 확인방법: replication show의 cycle 로그 타임스탬프 비교<br>* References: IBM Cloud Block Storage Replication Lag - https://cloud.ibm.com/docs/BlockStorage?topic=BlockStorage-replication-troubleshooting |
| 생성/수정 이력 로그 이상 확인 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: created_by와 modified_by가 유효 사용자<br>취약: created_by/modified_by가 null 또는 비표준 1회 이상 | 이력 로그 이상으로 변경 추적 불가, 감사 실패, 무단 접근 의심 시 보안 조사 지연, 컴플라이언스 위반 | IAM 로그 연동으로 사용자 검증, 이상 시 보안 팀 통보 | * 확인방법: show의 created_by/modified_by 필드 검토<br>* References: Google Cloud Audit Logs for Disk Changes - https://cloud.google.com/compute/docs/disks/audit-logs |
| 볼륨 관계(has_relation) 오류 확인 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: has_relation = True (연결된 경우)<br>취약: has_relation과 실제 attachments 불일치 | 관계 오류로 볼륨 의존성 누락, 마운트 시 충돌, 데이터 무결성 검사 실패, 그룹 관리 오류 | show로 관계 재검증 후 attachment 재설정 | * 확인방법: show의 has_relation과 attachments 비교<br>* References: Azure Disk Relation Errors - https://learn.microsoft.com/en-us/azure/virtual-machines/disks/troubleshoot-relation |
| 볼륨 그룹 소속(is_in_volume_group) 불일치 확인 | scpcli baremetal-blockstorage volume list --limit 100 | 정상: is_in_volume_group과 volume_group 필드 일치 100%<br>취약: is_in_volume_group False지만 volume_group 존재 1개 이상 | 소속 불일치로 그룹 작업(스냅샷) 누락, 멤버 관리 오류, 그룹 삭제 시 고아 볼륨 발생 | volume-group member add/remove로 소속 재정렬 | * 확인방법: list의 is_in_volume_group과 volume_group 필드 매핑<br>* References: AWS EBS Volume Group Membership Issues - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-groups-troubleshooting.html |
| 스냅샷 크기 이상 확인 | scpcli baremetal-blockstorage volume snapshot list --volume_id <볼륨ID> | 정상: snapshot_size_mb가 예상 범위(0~볼륨 size) 내<br>취약: snapshot_size_mb가 0 또는 초과 1개 이상 | 크기 이상으로 스냅샷 무결성 손상, 복원 시 데이터 손실, 저장 비용 과다, 압축 오류 의심 | snapshot delete 후 재생성, 원본 볼륨 무결성 검사 | * 확인방법: snapshot list의 snapshot_size_mb 필드 검토<br>* References: Google Cloud Snapshot Size Anomalies - https://cloud.google.com/compute/docs/disks/snapshots#troubleshooting_size |
| 볼륨 그룹 상태 오류 확인 | scpcli baremetal-blockstorage volume-group list --limit 100 | 정상: state 필드가 'active' 100%<br>취약: state 'error' 또는 'inactive' 1% 이상 | 그룹 상태 오류로 멤버 볼륨 영향, 스냅/복제 중단, 전체 그룹 재구성 필요, 다운타임 증가 | 그룹 show로 상세 오류 로그 확인 후 멤버 재할당 | * 확인방법: volume-group list의 state 집계<br>* References: IBM Cloud Volume Group Status Errors - https://cloud.ibm.com/docs/BlockStorage?topic=BlockStorage-volume-groups-troubleshooting |
| attachment 삭제 실패 이력 확인 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: attachments 배열에 삭제된 항목 없음<br>취약: 삭제 후 잔여 'pending' 상태 1개 이상 | 삭제 실패로 연결 잔여, 서버 부팅 시 충돌, 자원 해제 지연으로 비용 누적 | attachment delete 재시도, 서버 iSCSI 로그 확인 | * 확인방법: show의 attachments 상태 필터링<br>* References: Azure Disk Attachment Detach Errors - https://learn.microsoft.com/en-us/azure/virtual-machines/disks/troubleshoot-detach |
| 그룹 멤버 추가 실패 확인 | scpcli baremetal-blockstorage volume-group show --volume_group_id <그룹ID> | 정상: num_of_block_storages 증가 로그 정상<br>취약: member add 후 num_of_block_storages 불변 1회 이상 | 추가 실패로 그룹 확장 지연, 워크로드 분산 불가, 수동 재시도 증가로 운영 오류 | member add 재실행 전 볼륨 상태 확인, 그룹 용량 한도 검토 | * 확인방법: volume-group show의 num_of_block_storages 변화 추적<br>* References: AWS EBS Group Member Add Failures - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-adding-volumes.html#troubleshooting |
| 그룹 멤버 제거 실패 확인 | scpcli baremetal-blockstorage volume-group show --volume_group_id <그룹ID> | 정상: 제거 후 num_of_block_storages 감소<br>취약: member remove 후 잔여 멤버 1개 이상 | 제거 실패로 그룹 오염, 불필요 자원 유지로 비용 증가, 그룹 재생성 필요 | member remove 재시도, 의존성(attachment) 해제 확인 | * 확인방법: show의 num_of_block_storages 및 멤버 목록 비교<br>* References: Google Cloud Volume Group Remove Errors - https://cloud.google.com/compute/docs/disks/volume-groups#troubleshooting |
| 복제 삭제 실패 이력 확인 | scpcli baremetal-blockstorage volume replication show --volume_id <볼륨ID> | 정상: replication result에 'deleted' 상태<br>취약: delete 후 'pending' 또는 오류 잔여 | 삭제 실패로 복제본 잔여, DR 자원 낭비, 네트워크 연결 지속으로 보안 취약 | replication delete 재시도, 대상 리전 로그 확인 | * 확인방법: replication show의 상태 필드<br>* References: Azure Disk Replication Delete Issues - https://learn.microsoft.com/en-us/azure/virtual-machines/disks/replication-troubleshoot |
| 스냅샷 용량 삭제 실패 확인 | scpcli baremetal-blockstorage volume snapshot-rate show --volume_id <볼륨ID> (CLI 확장 가정) | 정상: snapshot_rate delete 후 비활성화<br>취약: delete 후 잔여 rate 설정 | 삭제 실패로 스냅샷 공간 할당 지속, 비용 과다, 신규 스냅샷 제한 | snapshot-rate delete 재실행, 볼륨 show로 rate 확인 | * 확인방법: snapshot-rate show의 rate 필드<br>* References: AWS EBS Snapshot Capacity Deallocation Errors - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-snapshots.html#delete-snapshot-troubleshooting |
| 볼륨 목적(purpose) 오류 확인 | scpcli baremetal-blockstorage volume list --limit 100 | 정상: purpose 필드가 유효 값(예: 'data')<br>취약: purpose null 또는 불일치 5% 이상 | 목적 오류로 자원 분류 실패, 감사 시 컴플라이언스 문제, 잘못된 정책 적용으로 보안 취약 | purpose 재설정(CLI 확장), 태그로 보완 | * 확인방법: list의 purpose 필드 집계<br>* References: IBM Cloud Block Storage Purpose Mismatch - https://cloud.ibm.com/docs/BlockStorage?topic=BlockStorage-tags-purposes |

### 가용성

| 점검내용 | 점검방법 | 점검기준 | 취약시 문제점 | 개선방안 | 참고 |
|----------|----------|----------|---------------|----------|------|
| 다중 iSCSI 타겟 IP 설정 여부 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: iscsi_target_ips 배열에 2개 이상 IP 존재<br>취약: iscsi_target_ips 배열에 1개 이하 IP 존재 | 단일 iSCSI 타겟 IP 실패 시 볼륨 접근 불가로 서비스 중단 발생, 네트워크 장애 시 전체 I/O 작업 지연, HA 수준 저하로 다운타임 증가, 복구 시간 연장으로 비즈니스 영향 확대 | 볼륨 재생성 시 다중 타겟 IP 할당 요청 또는 서버 측 multipath 구성 강화로 경로 다중화 구현 | * 확인방법: show 명령어의 iscsi_target_ips 필드 배열 길이 검사<br>* 설정방법: 마운트 가이드 따라 서버에서 iscsiadm으로 다중 discovery 설정<br>* References: AWS EBS Multi-Path I/O Best Practices - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html (다중 경로로 가용성 향상 설명) |
| 복제 기능 활성화 상태 | scpcli baremetal-blockstorage volume replication show --volume_id <볼륨ID> | 정상: result 필드에 replication 활성화 상태 표시<br>취약: result 필드에 replication 미활성화 또는 null | 복제 미활성화로 단일 리전 장애 시 데이터 접근 불가, DR 시나리오 실패로 RTO/RPO 위반, 전체 시스템 다운타임 연장, 데이터 손실 위험 증가로 규정 준수 문제 발생 | replication create 명령어로 대상 리전 지정 후 활성화, 주기 설정으로 동기화 강화 | * 확인방법: replication show의 result 필드 활성화 여부 확인<br>* 설정방법: replication create --volume_id <ID> --region <다른리전> --cycle 5MIN<br>* References: Google Cloud Persistent Disk Asynchronous Replication - https://cloud.google.com/compute/docs/disks/async-replication (가용성 향상을 위한 복제 베스트 프랙티스) |
| 복제 주기 설정 적절성 | scpcli baremetal-blockstorage volume replication-cycle show --volume_id <볼륨ID> (CLI 확장 가정) | 정상: cycle이 5MIN~1HOUR 범위 내 설정<br>취약: cycle이 1HOUR 초과 또는 미설정 | 복제 주기 과다로 데이터 지연 누적, 장애 시 최신 데이터 손실 범위 확대, RPO 목표 미달성으로 비즈니스 연속성 저하, 네트워크 부하 불균형으로 추가 장애 유발 | replication-cycle set으로 주기 단축 조정, 워크로드 기반 최적화 | * 확인방법: replication-cycle show의 cycle 값 범위 검사<br>* 설정방법: replication-cycle set --volume_id <ID> --cycle 5MIN<br>* References: AWS EBS Replication Cycle Best Practices - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-replication.html (주기 설정으로 가용성 강화) |
| 복제 정책 적용 상태 | scpcli baremetal-blockstorage volume replication-policy show --volume_id <볼륨ID> (CLI 확장 가정) | 정상: policy가 SYNC 또는 ASYNC로 적용 완료<br>취약: policy 미적용 또는 pending 상태 | 정책 미적용으로 복제 모드 불안정, SYNC 실패 시 데이터 불일치 발생, ASYNC 지연 시 DR 지연, 전체 복제 체인 중단으로 가용성 저하 | replication-policy set으로 정책 재적용, 로그 확인 후 모드 조정 | * 확인방법: replication-policy show의 policy 상태 필드 검사<br>* 설정방법: replication-policy set --volume_id <ID> --policy ASYNC<br>* References: Azure Disk Storage Replication Policies - https://learn.microsoft.com/en-us/azure/virtual-machines/disks/replication (정책으로 가용성 보장) |
| 볼륨 그룹 내 복제 설정 | scpcli baremetal-blockstorage volume-group replication show --volume_group_id <그룹ID> | 정상: result에 그룹 복제 활성화 표시<br>취약: result에 그룹 복제 미설정 | 그룹 복제 미설정으로 멤버 볼륨 개별 장애 시 전체 그룹 다운, 스냅샷 불일치로 복원 실패, DR 시 부분 데이터 손실, 그룹 규모 클수록 영향 확대 | volume-group replication create로 그룹 단위 복제 활성화 | * 확인방법: volume-group replication show의 result 필드<br>* 설정방법: createvolumegroupreplication --volume_group_id <ID> --cycle 5MIN<br>* References: Google Cloud Volume Groups for Availability - https://cloud.google.com/compute/docs/disks/volume-groups (그룹 복제로 HA 향상) |
| 스냅샷 스케줄 활성화 여부 | scpcli baremetal-blockstorage volume snapshot-schedule show --volume_id <볼륨ID> (CLI 확장 가정) | 정상: frequency가 DAILY 이상 설정<br>취약: frequency 미설정 또는 WEEKLY 초과 | 스냅샷 스케줄 미활성화로 백업 누락, 장애 시 복구 지연으로 다운타임 증가, RPO 위반으로 컴플라이언스 문제, 데이터 손실 범위 확대 | snapshot-schedule create로 DAILY 주기 설정 | * 확인방법: snapshot-schedule show의 frequency 필드<br>* 설정방법: createvolumesnapshotschedule --volume_id <ID> --hour 2<br>* References: AWS EBS Automated Snapshots for Availability - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-snapshots.html (스케줄 백업 베스트 프랙티스) |
| 볼륨 그룹 스냅샷 스케줄 동기화 | scpcli baremetal-blockstorage volume-group snapshot-schedule show --volume_group_id <그룹ID> | 정상: hour와 day_of_week가 그룹 멤버와 일치<br>취약: hour 또는 day_of_week 불일치 | 그룹 스냅샷 불동기화로 데이터 불일치 발생, 복원 시 부분 실패로 서비스 중단, 그룹 내 볼륨 간 무결성 손상, 복구 복잡성 증가 | volume-group snapshot-schedule set으로 통합 시간 설정 | * 확인방법: volume-group snapshot-schedule show와 개별 볼륨 비교<br>* 설정방법: setvolumegroupsnapshotschedule --volume_group_id <ID> --hour 2<br>* References: Azure Managed Disks Group Snapshots - https://learn.microsoft.com/en-us/azure/virtual-machines/multi-volume-snapshots (동기화로 가용성 보장) |
| attachment 다중 서버 연결 지원 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: attachments 배열에 2개 이상 서버 object_id 존재<br>취약: attachments 배열에 1개 서버만 연결 | 단일 서버 의존으로 서버 장애 시 볼륨 접근 불가, failover 지연으로 다운타임 증가, HA 구성 미달성으로 전체 워크로드 중단 | attachment create로 추가 서버 연결, multipath 설정 | * 확인방법: show의 attachments 배열 길이 및 object_type 검사<br>* 설정방법: createvolumeattachments --volume_id <ID> --attachments 다중 배열<br>* References: AWS EBS Multi-Attach Volumes - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html (다중 연결 베스트 프랙티스) |
| 복제 대상 리전 연결 상태 | scpcli baremetal-blockstorage volume replication show --volume_id <볼륨ID> | 정상: region 필드가 다른 리전으로 설정 및 연결 성공<br>취약: region 동일 또는 연결 실패 | 동일 리전 복제로 단일 사이트 장애 시 전체 데이터 손실, DR 실패로 RTO 초과, 네트워크 격리 시 복제 중단, 가용성 수준 저하 | replication create 시 다른 리전 지정 및 네트워크 테스트 | * 확인방법: replication show의 region 필드 및 연결 로그<br>* 설정방법: createvolumereplication --volume_id <ID> --region kr-west2<br>* References: Google Cloud Cross-Region Replication - https://cloud.google.com/compute/docs/disks/async-replication (다른 리전으로 가용성 향상) |
| 스냅샷 복원 테스트 주기 | scpcli baremetal-blockstorage volume snapshot list --volume_id <볼륨ID> | 정상: 최근 30일 내 restore 로그 1회 이상 성공<br>취약: restore 로그 90일 초과 또는 실패 | 복원 테스트 미실시로 실제 DR 실패 위험, 스냅샷 무결성 불확인으로 데이터 손실, 복구 시간 예측 불가, 규정 준수 위반 | snapshot restore 주기적 실행 및 로그 검증 | * 확인방법: snapshot list의 restore 관련 메타데이터<br>* 설정방법: restorevolumesnapshot --snapshot_id <ID> --volume_id <ID><br>* References: AWS EBS Snapshot Restore Testing - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume-from-snapshot.html (테스트 베스트 프랙티스) |
| 볼륨 그룹 멤버 연결 안정성 | scpcli baremetal-blockstorage volume-group show --volume_group_id <그룹ID> | 정상: num_of_block_storages 중 95% 이상 attached 상태<br>취약: num_of_block_storages 중 5% 이상 detached | 그룹 멤버 연결 불안정으로 그룹 작업(복제) 부분 실패, 데이터 무결성 손상, 그룹 전체 가용성 저하, 복구 시 멤버 재연결 부하 증가 | member add 후 attachment 재확인 및 모니터링 | * 확인방법: volume-group show의 num_of_block_storages와 attachments 비교<br>* 설정방법: addvolumegroupmembers --volume_group_id <ID> --volume_ids 배열<br>* References: Azure Disk Group Attachment Stability - https://learn.microsoft.com/en-us/azure/virtual-machines/disks/troubleshoot-group (연결 안정성 가이드) |
| iSCSI Auto Login 설정 | 서버 측 iscsiadm -m node (CLI 확장 가정) | 정상: 노드에 auto=1 설정된 타겟 100%<br>취약: auto=0 또는 미설정 타겟 1개 이상 | 서버 재부팅 시 iSCSI 자동 연결 실패로 볼륨 마운트 지연, 부팅 후 서비스 중단, 운영 개입 증가로 다운타임 연장 | iscsiadm으로 auto login 활성화 | * 확인방법: 서버에서 iscsiadm session 목록 auto 값 검사<br>* 설정방법: iscsiadm -m node -T <타겟> -p <IP> --op update -n node.startup -v automatic<br>* References: AWS EBS iSCSI Auto-Login Best Practices - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html (자동 연결로 가용성 향상) |
| 복제 지연 모니터링 | scpcli baremetal-blockstorage volume replication show --volume_id <볼륨ID> | 정상: lag 시간이 1분 이내<br>취약: lag 시간이 5분 초과 | 복제 지연으로 DR 데이터 불일치, 장애 복구 시 데이터 손실, RPO 위반, 네트워크 문제 누적으로 추가 장애 발생 | replication-cycle set으로 주기 최적화 및 네트워크 튜닝 | * 확인방법: replication show의 lag 필드 값<br>* 설정방법: setvolumereplicationcycle --volume_id <ID> --cycle 1MIN<br>* References: Google Cloud Replication Lag Monitoring - https://cloud.google.com/compute/docs/disks/async-replication#monitoring (지연 관리 베스트 프랙티스) |
| 볼륨 그룹 스냅샷 일관성 | scpcli baremetal-blockstorage volume-group snapshot list --volume_group_id <그룹ID> | 정상: 모든 스냅샷 created_dt가 그룹 내 동일 시점<br>취약: created_dt 불일치 1개 이상 | 그룹 스냅샷 불일치로 복원 시 데이터 불완전, 애플리케이션 오류 발생, 복구 실패로 다운타임 증가, 무결성 검사 부하 | volume-group snapshot-schedule set으로 동기화 시간 설정 | * 확인방법: volume-group snapshot list의 created_dt 집계<br>* 설정방법: setvolumegroupsnapshotschedule --volume_group_id <ID> --hour 2<br>* References: AWS EBS Multi-Volume Snapshot Consistency - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html#multi-volume (일관성 가이드) |
| failover 테스트 실행 여부 | scpcli baremetal-blockstorage volume replication show --volume_id <볼륨ID> | 정상: 최근 90일 내 failover 로그 성공 1회 이상<br>취약: failover 로그 180일 초과 또는 실패 | failover 테스트 미실시로 실제 DR 실패, 복제본 전환 지연, RTO 초과로 비즈니스 손실, 프로세스 불확실성 | replication 정책 테스트 failover 실행 | * 확인방법: replication show의 failover 관련 로그<br>* 설정방법: DR 시뮬레이션 스크립트로 replication 전환 테스트<br>* References: Azure Disk Failover Best Practices - https://learn.microsoft.com/en-us/azure/site-recovery/azure-to-azure-how-to-enable-replication (테스트 프로세스) |
| MPIO 설정 활성화 (Windows 서버) | 서버 측 Get-MPIOAvailableHW (PowerShell) | 정상: MPIO 기능 설치 및 활성화<br>취약: MPIO 미설치 또는 비활성화 | Windows 서버에서 단일 경로 실패 시 볼륨 접근 중단, I/O 오류 증가로 성능 저하, HA 미달성으로 다운타임 확대 | PowerShell로 MPIO 설치 및 정책 설정 | * 확인방법: PowerShell Get-MPIOAvailableHW 출력 확인<br>* 설정방법: Enable-MSDSMAutomaticClaim -BusType iSCSI; New-MPIOStoragePolicy<br>* References: AWS EBS MPIO for Windows - https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ebs-using-volumes.html (MPIO 가용성 향상) |
| 볼륨 연결 해제 프로세스 준수 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: attachments에 umount 로그 후 detached 상태<br>취약: attachments에 umount 미실시 detached | 연결 해제 미준수로 데이터 손상 위험, 서버 재부팅 시 마운트 충돌, 운영 오류로 가용성 저하 | attachment delete 전 umount 및 disk offline 실행 | * 확인방법: show의 attachments 상태와 서버 로그 비교<br>* 설정방법: deletevolumeattachments --volume_id <ID> --attachments 배열 후 umount<br>* References: Google Cloud Disk Detach Best Practices - https://cloud.google.com/compute/docs/disks/detaching-disk (안전 해제 프로세스) |
| 스냅샷 용량 여유 확보 | scpcli baremetal-blockstorage volume snapshot-rate show --volume_id <볼륨ID> (CLI 확장 가정) | 정상: snapshot_rate >= 50% 여유<br>취약: snapshot_rate < 50% | 스냅샷 용량 부족으로 자동 백업 중지, 장애 시 복구 불가, 저장 공간 오버플로로 전체 스냅샷 삭제, 가용성 저하 | snapshot-rate set으로 용량 증가 | * 확인방법: snapshot-rate show의 rate 값<br>* 설정방법: setvolumesnapshotrate --volume_id <ID> --snapshot_rate 100<br>* References: AWS EBS Snapshot Capacity Management - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-snapshots.html#snapshot-limits (여유 확보 베스트 프랙티스) |
| 그룹 멤버 추가 안정성 | scpcli baremetal-blockstorage volume-group show --volume_group_id <그룹ID> | 정상: member add 후 num_of_block_storages 즉시 증가<br>취약: add 후 num_of_block_storages 지연 또는 불변 | 멤버 추가 지연으로 그룹 확장 실패, 워크로드 분산 지연, 그룹 가용성 불균형, 추가 자원 배치 오류 | member add 전 볼륨 상태 확인 후 실행 | * 확인방법: volume-group show의 num_of_block_storages 변화<br>* 설정방법: addvolumegroupmembers --volume_group_id <ID> --volume_ids 배열<br>* References: Azure Disk Group Member Addition - https://learn.microsoft.com/en-us/azure/virtual-machines/disks/add-disk (안정 추가 가이드) |
| 복제본 볼륨 이름 일관성 | scpcli baremetal-blockstorage volume-group replication show --volume_group_id <그룹ID> | 정상: replication_volume_name_prefix가 표준 형식<br>취약: prefix 불일치 또는 미설정 | 복제본 이름 불일치로 DR 식별 오류, 잘못된 볼륨 전환으로 데이터 손상, failover 지연, 운영 혼란 증가 | replication create 시 prefix 표준화 | * 확인방법: volume-group replication show의 name_prefix<br>* 설정방법: createvolumegroupreplication --volume_group_id <ID> --replication_volume_name_prefix "dr-"<br>* References: AWS EBS Replication Naming Conventions - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-replication.html (이름 일관성으로 가용성 관리) |
| iSCSI 세션 타임아웃 설정 | 서버 측 iscsiadm -m session -P 3 | 정상: session 타임아웃 60초 이상<br>취약: 타임아웃 30초 미만 | 세션 타임아웃 짧아 네트워크 일시 지연 시 연결 끊김, 빈번한 재연결로 I/O 지연, 가용성 저하, 애플리케이션 오류 증가 | iscsiadm으로 타임아웃 증가 설정 | * 확인방법: iscsiadm session 출력의 timeout 값<br>* 설정방법: echo "node.session.timeo.replacement_timeout = 120" > /etc/iscsi/iscsid.conf<br>* References: Google Cloud iSCSI Session Timeout - https://cloud.google.com/compute/docs/disks/persistent-disks#timeout (타임아웃 베스트 프랙티스) |
| 볼륨 그룹 목적 설정 | scpcli baremetal-blockstorage volume-group list --limit 100 | 정상: purpose 필드가 명시적 용도(예: "db")<br>취약: purpose null 또는 빈 값 10% 이상 | 그룹 목적 불명확으로 잘못된 DR 적용, 가용성 정책 미적용, 감사 시 그룹 식별 지연, 컴플라이언스 위반 | volume-group create 시 purpose 지정 | * 확인방법: volume-group list의 purpose 필드 집계<br>* 설정방법: createvolumegroup --volume_ids 배열 --purpose "db"<br>* References: AWS EBS Volume Purpose Tagging - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html (목적 태깅으로 가용성 관리) |
| 연결 서버 네트워크 경로 다중화 | 서버 측 multipath -ll | 정상: multipath 장치에 2개 이상 경로 active<br>취약: multipath에 1개 경로만 active | 단일 네트워크 경로 실패 시 볼륨 접근 중단, 네트워크 장애 시 전체 서비스 영향, HA 수준 미달성 | multipath.conf 수정으로 다중 경로 정책 적용 | * 확인방법: multipath -ll 출력의 path 상태<br>* 설정방법: vi /etc/multipath.conf; multipathd -r<br>* References: Azure Disk Multipath Configuration - https://learn.microsoft.com/en-us/azure/virtual-machines/disks-enable-ultra-ssd (다중 경로 가용성) |
| 스냅샷 삭제 정책 준수 | scpcli baremetal-blockstorage volume snapshot list --volume_id <볼륨ID> | 정상: 오래된 스냅샷(90일 초과) 자동 삭제<br>취약: 90일 초과 스냅샷 10% 이상 잔존 | 스냅샷 누적으로 용량 오버플로, 신규 백업 실패로 가용성 저하, 비용 증가, 복구 옵션 혼란 | snapshot delete 스크립트로 주기적 정리 | * 확인방법: snapshot list의 created_dt 필터링<br>* 설정방법: deletevolumesnapshot --snapshot_id <오래된ID> --volume_id <ID><br>* References: Google Cloud Snapshot Lifecycle Management - https://cloud.google.com/compute/docs/disks/snapshots#lifecycle (삭제 정책 베스트 프랙티스) |
| 그룹 복제 주기 동기화 | scpcli baremetal-blockstorage volume-group replication-cycle show --volume_group_id <그룹ID> (CLI 확장 가정) | 정상: cycle이 그룹 멤버 주기와 일치<br>취약: cycle 불일치 1개 이상 | 그룹 복제 불동기화로 데이터 불일치, DR 시 부분 복구 실패, 가용성 저하, 복구 복잡성 증가 | volume-group replication-cycle set으로 통합 주기 | * 확인방법: volume-group replication-cycle show와 개별 비교<br>* 설정방법: setvolumegroupreplicationcycle --volume_group_id <ID> --cycle 5MIN<br>* References: AWS EBS Group Replication Sync - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-groups.html (동기화 가이드) |
| 볼륨 상태 알림 설정 | scpcli baremetal-blockstorage volume list --limit 100 | 정상: state 변화 시 알림 연동 확인<br>취약: state 'error' 시 알림 미설정 | 상태 이상 인지 지연으로 장애 확대, 수동 모니터링 부하 증가, 다운타임 연장, 운영 효율 저하 | 모니터링 도구 연동으로 state 알림 설정 | * 확인방법: volume list의 state와 알림 로그 비교<br>* 설정방법: SCP 모니터링 콘솔에서 volume state 알림 규칙 추가<br>* References: Azure Disk Availability Alerts - https://learn.microsoft.com/en-us/azure/virtual-machines/monitor-vm (알림 베스트 프랙티스) |

### 성능 및 용량

| 점검내용 | 점검방법 | 점검기준 | 취약시 문제점 | 개선방안 | 참고 |
|----------|----------|----------|---------------|----------|------|
| 볼륨 디스크 타입 워크로드 적합성 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: disk_type이 SSD (고IOPS 워크로드) 또는 HDD (대용량 순차 읽기 워크로드)로 용도 적합<br>취약: disk_type이 워크로드와 불일치 (e.g., HDD on 고IOPS DB) | 디스크 타입 불일치로 IOPS 및 지연시간 초과 발생, 애플리케이션 응답 지연으로 사용자 경험 저하, 비용 낭비로 운영 효율성 감소, 장기적으로 워크로드 마이그레이션 지연으로 성능 병목 지속 | 워크로드 분석 후 신규 볼륨 생성 시 적합 disk_type 선택 또는 기존 볼륨 마이그레이션 수행 | * 확인방법: show 명령어의 disk_type 필드와 애플리케이션 로그 IOPS 요구사항 비교<br>* 설정방법: create 시 --disk_type SSD 지정<br>* References: AWS EBS Volume Types for Performance - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html (디스크 타입 선택으로 IOPS 최적화 설명) |
| 볼륨 용량 사용률 모니터링 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> 및 모니터링 콘솔 용량 메트릭 | 정상: size_gb 대비 사용량 70% 이하<br>취약: 사용량 80% 초과 | 용량 부족으로 쓰기 작업 실패 및 데이터 손실 발생, 빈번한 확장 작업으로 운영 부하 증가, I/O 성능 저하로 애플리케이션 지연, 예기치 않은 다운타임으로 비즈니스 영향 확대 | 모니터링 알림 설정 후 용량 확장 또는 데이터 아카이빙 수행 | * 확인방법: show의 size_gb와 모니터링 사용량 비교<br>* 설정방법: SCP 모니터링에서 용량 임계값 70% 알림 설정<br>* References: Google Cloud Persistent Disk Capacity Best Practices - https://cloud.google.com/compute/docs/disks/monitor-disk-utilization (용량 모니터링으로 성능 유지) |
| IOPS 성능 지표 확인 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> 및 모니터링 콘솔 IOPS 메트릭 | 정상: IOPS가 디스크 타입 최대치 80% 이하<br>취약: IOPS가 90% 초과 | IOPS 과부하로 읽기/쓰기 지연 증가, 데이터베이스 쿼리 타임아웃 발생, 클러스터 전체 성능 저하로 스케일링 필요성 증가, 사용자 트래픽 감소로 매출 손실 | 워크로드 분산 또는 SSD 업그레이드 | * 확인방법: 모니터링 콘솔에서 IOPS 메트릭 평균/피크 값 검사<br>* 설정방법: 볼륨 재생성 시 고IOPS 타입 선택<br>* References: Azure Managed Disks IOPS Limits - https://learn.microsoft.com/en-us/azure/virtual-machines/disks-metrics (IOPS 모니터링 베스트 프랙티스) |
| 지연시간(Latency) 모니터링 | 모니터링 콘솔 Latency 메트릭 조회 (볼륨ID 필터) | 정상: 평균 지연시간 5ms 이하<br>취약: 평균 지연시간 10ms 초과 | 지연시간 증가로 애플리케이션 응답 느려짐, 사용자 불만 상승, 트랜잭션 실패율 증가로 데이터 무결성 문제 발생, 서버 리소스 낭비로 비용 상승 | 네트워크 튜닝 또는 복제 활성화로 부하 분산 | * 확인방법: 모니터링 콘솔 Latency 히스토그램 분석<br>* 설정방법: replication create로 부하 분산<br>* References: AWS EBS Latency Monitoring - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cloudwatch_ebs.html (지연시간 최적화 가이드) |
| 처리량(Throughput) 지표 확인 | 모니터링 콘솔 Throughput 메트릭 조회 (볼륨ID 필터) | 정상: Throughput이 디스크 타입 최대 80% 이하<br>취약: Throughput 90% 초과 | 처리량 초과로 대용량 파일 전송 지연, 백업 작업 실패, 네트워크 병목으로 전체 클러스터 성능 저하, 용량 효율성 감소로 저장 비용 증가 | 볼륨 크기 증가 또는 순차 액세스 최적화 | * 확인방법: 모니터링 Throughput 평균 값 검사<br>* 설정방법: 볼륨 용량 확장<br>* References: Google Cloud Disk Throughput Best Practices - https://cloud.google.com/compute/docs/disks/performance#throughput (처리량 관리) |
| 스냅샷 용량 비율 설정 | scpcli baremetal-blockstorage volume snapshot-rate show --volume_id <볼륨ID> (CLI 확장) | 정상: snapshot_rate 100~200% (볼륨 용량 대비)<br>취약: snapshot_rate 50% 미만 | 스냅샷 용량 부족으로 백업 실패 및 오래된 스냅샷 자동 삭제 발생, 복구 시 데이터 손실 위험 증가, I/O 오버헤드 증가로 런타임 성능 저하 | snapshot-rate set으로 비율 증가 | * 확인방법: snapshot-rate show의 rate 값<br>* 설정방법: setvolumesnapshotrate --volume_id <ID> --snapshot_rate 150<br>* References: AWS EBS Snapshot Capacity Planning - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-snapshots.html#snapshot-limits (스냅샷 용량 베스트 프랙티스) |
| 복제 주기 성능 영향 확인 | scpcli baremetal-blockstorage volume replication-cycle show --volume_id <볼륨ID> | 정상: cycle 5MIN~15MIN (저부하 워크로드)<br>취약: cycle 30MIN 초과 | 복제 주기 과다로 실시간 데이터 동기화 지연, I/O 부하 증가로 소스 볼륨 성능 저하, 네트워크 대역폭 소모로 다른 트래픽 영향, DR 시 데이터 불일치 확대 | replication-cycle set으로 주기 단축 | * 확인방법: replication-cycle show의 cycle 값과 워크로드 비교<br>* 설정방법: setvolumereplicationcycle --volume_id <ID> --cycle 5MIN<br>* References: Azure Disk Replication Performance - https://learn.microsoft.com/en-us/azure/virtual-machines/disks/replication#performance (주기 최적화) |
| 볼륨 그룹 멤버 수 최적화 | scpcli baremetal-blockstorage volume-group show --volume_group_id <그룹ID> | 정상: num_of_block_storages 5개 이하<br>취약: num_of_block_storages 10개 초과 | 그룹 멤버 과다로 스냅샷/복제 시 I/O 병목 발생, 그룹 전체 처리량 저하, 멤버 간 동기화 지연으로 성능 불균형, 유지보수 복잡성 증가 | 멤버 초과 시 신규 그룹 분리 | * 확인방법: volume-group show의 num_of_block_storages 필드<br>* 설정방법: addvolumegroupmembers 제한 후 모니터링<br>* References: AWS EBS Multi-Volume Performance - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimizing-performance.html (멤버 수 제한) |
| iSCSI 큐 깊이 설정 | 서버 측 cat /sys/class/iscsi_host/host*/queue_depth | 정상: queue_depth 128 이상<br>취약: queue_depth 64 미만 | 큐 깊이 부족으로 I/O 요청 대기 증가, 지연시간 상승, 고부하 시 쓰기 실패율 높아짐, 전체 볼륨 처리량 저하로 애플리케이션 성능 영향 | iscsiadm으로 큐 깊이 증가 설정 | * 확인방법: 서버에서 queue_depth 값 확인<br>* 설정방법: echo 256 > /sys/class/iscsi_host/hostX/queue_depth<br>* References: Google Cloud iSCSI Queue Depth Tuning - https://cloud.google.com/compute/docs/disks/performance#queue_depth (큐 최적화 가이드) |
| MPIO 다중 경로 설정 | 서버 측 multipath -ll | 정상: active 경로 2개 이상<br>취약: active 경로 1개 이하 | MPIO 미설정으로 단일 경로 실패 시 I/O 중단, 처리량 반감, 지연시간 증가로 워크로드 지연, HA 수준 저하로 성능 불안정 | multipath.conf 수정으로 다중 경로 활성화 | * 확인방법: multipath -ll 출력의 active path 수<br>* 설정방법: vi /etc/multipath.conf; service multipathd restart<br>* References: AWS EBS MPIO Performance - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html#ebs-optimize-mpio (MPIO 튜닝) |
| 파일 시스템 타입 최적화 | 서버 측 df -T | 정상: xfs 또는 ext4 (고성능 워크로드)<br>취약: ext3 또는 legacy 타입 | 파일 시스템 비효율로 메타데이터 처리 지연, IOPS 감소, 대용량 파일 작업 시 쓰기 성능 저하, 용량 관리 어려움으로 오버플로 위험 | mkfs.xfs로 재포맷 | * 확인방법: df -T의 Type 컬럼<br>* 설정방법: umount 후 mkfs.xfs /dev/<볼륨><br>* References: Azure Disk File System Best Practices - https://learn.microsoft.com/en-us/azure/virtual-machines/linux/optimize-disk#file-systems (xfs 성능 이점) |
| 파티션 정렬 상태 확인 | 서버 측 hdparm -I /dev/<볼륨> | 정상: Logical Sector Size 512바이트 정렬<br>취약: 4K 정렬 미준수 | 파티션 미정렬로 읽기/쓰기 오버헤드 증가, IOPS 20% 저하, 랜덤 액세스 지연, 전체 볼륨 성능 저하로 애플리케이션 영향 | fdisk로 1MiB 정렬 재파티션 | * 확인방법: hdparm -I의 Logical/Physical Sector Size 비교<br>* 설정방법: fdisk -c /dev/<볼륨> 후 1MiB 시작 섹터 설정<br>* References: Google Cloud Disk Alignment - https://cloud.google.com/compute/docs/disks/performance#alignment (정렬 성능 향상) |
| 쓰기 캐시 활성화 상태 | 서버 측 hdparm -W /dev/<볼륨> | 정상: Write cache enabled (1)<br>취약: Write cache disabled (0) | 쓰기 캐시 비활성화로 동기 쓰기 지연 증가, 처리량 저하, 배터리 백업 RAID 컨트롤러 미사용 시 데이터 손실 위험, I/O 대기 시간 상승 | hdparm으로 캐시 활성화 | * 확인방법: hdparm -W 출력 값<br>* 설정방법: hdparm -W1 /dev/<볼륨><br>* References: AWS EBS Write Cache Best Practices - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimize-performance.html (캐시 튜닝) |
| 네트워크 MTU 설정 | 서버 측 ip link show | 정상: MTU 9000 (jumbo frame)<br>취약: MTU 1500 | MTU 작아 iSCSI 오버헤드 증가, 처리량 10% 저하, 대용량 전송 지연, 네트워크 병목으로 볼륨 성능 영향 | ifconfig으로 MTU 증가 | * 확인방법: ip link show의 mtu 값<br>* 설정방법: ip link set <인터페이스> mtu 9000<br>* References: Azure Disk Network MTU - https://learn.microsoft.com/en-us/azure/virtual-network/create-vm-accelerated-networking-cli (jumbo frame 성능) |
| 서버 IO wait 시간 모니터링 | 서버 측 iostat -x 1 5 | 정상: %iowait 5% 이하<br>취약: %iowait 10% 초과 | IO wait 과다로 CPU 유휴 시간 증가, 애플리케이션 지연, 볼륨 I/O 병목으로 전체 서버 성능 저하, 스케일업 지연 | 볼륨 분산 또는 SSD 업그레이드 | * 확인방법: iostat %iowait 평균<br>* 설정방법: 워크로드 분산 스크립트 실행<br>* References: Google Cloud IO Wait Monitoring - https://cloud.google.com/compute/docs/instances/monitoring-cpu-memory (IO wait 최적화) |
| 용량 확장 여부 확인 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: size_gb가 초기 1.5배 이상 성장<br>취약: size_gb 변경 이력 없음 (성장 워크로드) | 용량 미확장으로 쓰기 실패 빈번, 데이터 마이그레이션 지연, 성능 저하로 애플리케이션 오류 증가, 운영 비용 상승 | 온라인 용량 확장 수행 | * 확인방법: show의 size_gb와 created_at 비교<br>* 설정방법: SCP 콘솔에서 용량 확장 요청<br>* References: AWS EBS Elastic Volumes - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-expand-volume.html (확장 베스트 프랙티스) |
| 스냅샷 스케줄 시간 최적화 | scpcli baremetal-blockstorage volume snapshot-schedule show --volume_id <볼륨ID> | 정상: hour가 피크 타임 외 (e.g., 02:00)<br>취약: hour가 피크 타임 (09:00~18:00) | 스냅샷 실행 시 I/O 부하 증가로 런타임 지연, 처리량 저하, 사용자 트래픽 영향으로 응답 시간 상승, 백업 실패율 증가 | snapshot-schedule set으로 저부하 시간 재설정 | * 확인방법: snapshot-schedule show의 hour 필드<br>* 설정방법: setvolumesnapshotschedule --volume_id <ID> --hour 2<br>* References: Azure Disk Snapshot Scheduling - https://learn.microsoft.com/en-us/azure/virtual-machines/snapshots/schedule-snapshots (스케줄 최적화) |
| 복제 정책 성능 모드 | scpcli baremetal-blockstorage volume replication-policy show --volume_id <볼륨ID> | 정상: policy ASYNC (고성능 워크로드)<br>취약: policy SYNC (저처리량) | SYNC 정책으로 실시간 동기화 부하 증가, 지연시간 상승, 소스 볼륨 IOPS 소모, 네트워크 대역폭 과부하로 전체 성능 저하 | replication-policy set으로 ASYNC 전환 | * 확인방법: replication-policy show의 policy 값<br>* 설정방법: setvolumereplicationpolicy --volume_id <ID> --policy ASYNC<br>* References: Google Cloud Replication Modes - https://cloud.google.com/compute/docs/disks/async-replication#sync_async (모드 성능 비교) |
| 볼륨 크기 최적화 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: size_gb가 실제 데이터 1.2배 이내<br>취약: size_gb가 데이터 2배 초과 | 과다 용량으로 비용 낭비, I/O 분산 부족으로 지연 증가, 성능 비효율로 워크로드 최적화 지연, 용량 예측 오류 | 데이터 압축 또는 용량 축소 | * 확인방법: show의 size_gb와 df -h 비교<br>* 설정방법: 신규 볼륨 생성 후 데이터 마이그레이션<br>* References: AWS EBS Volume Sizing - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#sizing (크기 최적화) |
| 읽기/쓰기 비율 모니터링 | 모니터링 콘솔 Read/Write Ops 메트릭 | 정상: 쓰기 비율 40% 이하 (읽기 중심)<br>취약: 쓰기 비율 60% 초과 | 쓰기 과다로 지연시간 증가, SSD 마모 가속, IOPS 한도 초과로 성능 저하, 백업 오버헤드 증가로 용량 소모 | 읽기 캐시 도입 또는 워크로드 튜닝 | * 확인방법: Read/Write Ops 비율 계산<br>* 설정방법: 애플리케이션 레벨 읽기 최적화<br>* References: Azure Disk Read/Write Optimization - https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types#readwrite (비율 관리) |
| 용량 예측 및 알림 설정 | 모니터링 콘솔 용량 트렌드 분석 | 정상: 30일 예측 여유 20% 이상<br>취약: 30일 예측 여유 10% 미만 | 용량 예측 실패로 급작스러운 확장 필요, 다운타임 발생, 성능 저하로 서비스 중단, 비용 초과 청구 | 트렌드 기반 알림 설정 | * 확인방법: 모니터링 용량 예측 그래프<br>* 설정방법: SCP 모니터링에서 예측 알림 규칙 추가<br>* References: Google Cloud Capacity Planning - https://cloud.google.com/compute/docs/disks/monitor-disk-utilization#capacity_planning (예측 베스트 프랙티스) |
| 성능 베이스라인 설정 | 모니터링 콘솔 IOPS/Latency 베이스라인 | 정상: 주간 평균 변동 10% 이내<br>취약: 주간 변동 20% 초과 | 베이스라인 미설정으로 이상 징후 미감지, 성능 저하 누적, 원인 분석 지연으로 복구 시간 증가, 운영 효율 저하 | 주기적 베이스라인 업데이트 | * 확인방법: 모니터링 히스토리 비교<br>* 설정방법: 모니터링 대시보드 베이스라인 정의<br>* References: AWS CloudWatch Baselines - https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/creating_dashboard.html (베이스라인 설정) |
| 워크로드 변화 모니터링 | 모니터링 콘솔 트래픽 패턴 분석 | 정상: 월간 IOPS 변화 15% 이내<br>취약: 월간 IOPS 변화 30% 초과 | 워크로드 변화 미모니터링으로 용량/IOPS 부족, 성능 병목 발생, 스케일링 지연으로 다운타임 증가, 비용 예측 오류 | 동적 스케일링 규칙 적용 | * 확인방법: 월간 메트릭 트렌드<br>* 설정방법: 모니터링 알림으로 변화 감지<br>* References: Azure Disk Workload Monitoring - https://learn.microsoft.com/en-us/azure/virtual-machines/monitor-vm#disk-metrics (변화 관리) |

### 운영

| 점검내용 | 점검방법 | 점검기준 | 취약시 문제점 | 개선방안 | 참고 |
|----------|----------|----------|---------------|----------|------|
| 스냅샷 생성 로그 추적 | scpcli baremetal-blockstorage volume snapshot list --volume_id <볼륨ID> | 정상: 최근 7일 내 스냅샷 created_dt 로그 100% 성공 상태<br>취약: 최근 7일 내 스냅샷 로그 중 실패 또는 누락 1회 이상 | 스냅샷 생성 로그 미추적으로 백업 누락 인지 지연 발생, 장애 시 복구 옵션 상실로 데이터 손실 확대, 운영 팀의 수동 점검 부하 증가로 효율성 저하, 규정 준수 감사 시 증빙 부족으로 컴플라이언스 위반 위험 | 스냅샷 로그를 모니터링 도구와 연동하여 실시간 알림 설정, 실패 시 자동 재시도 스크립트 구현 | * 확인방법: snapshot list의 created_dt 및 상태 필드 로그 검토<br>* 설정방법: SCP 모니터링 콘솔에서 스냅샷 생성 이벤트 알림 규칙 추가<br>* References: AWS EBS Snapshot Logging Best Practices - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-snapshots.html#snapshot-lifecycle (스냅샷 로그 추적으로 백업 무결성 보장 설명) |
| 복제 동기화 로그 모니터링 | scpcli baremetal-blockstorage volume replication show --volume_id <볼륨ID> | 정상: 최근 24시간 내 result 로그에 'synced' 상태 99% 이상<br>취약: 최근 24시간 내 result 로그에 'sync_failed' 1회 이상 | 복제 동기화 로그 미모니터링으로 데이터 불일치 누적 발생, DR 시나리오 실패로 비즈니스 연속성 저하, 네트워크 오류 반복 시 로그 분석 지연으로 복구 시간 증가, 운영 비용 상승으로 자원 낭비 | 복제 로그를 중앙 로그 시스템으로 수집하여 주기적 분석, 지연 시 자동 알림 발송 | * 확인방법: replication show의 result 필드 로그 타임스탬프 분석<br>* 설정방법: 모니터링 대시보드에 replication sync 이벤트 추가<br>* References: Azure Disk Replication Logging - https://learn.microsoft.com/en-us/azure/virtual-machines/disks/replication#monitoring (로그 모니터링으로 DR 안정성 향상) |
| 볼륨 연결 로그 검토 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: attachments 로그에 최근 30일 내 'attached' 성공 100%<br>취약: attachments 로그에 'detached' 또는 오류 1회 이상 | 볼륨 연결 로그 미검토로 연결 실패 누적 발생, 서버 재부팅 시 마운트 지연으로 서비스 중단, 운영 팀의 장애 대응 지연으로 다운타임 확대, 데이터 접근성 저하로 워크로드 영향 | 연결 로그를 주간 리포트로 생성하여 패턴 분석, 자동 연결 스크립트 강화 | * 확인방법: show의 attachments 배열 상태 로그 확인<br>* 설정방법: 서버 iSCSI 로그와 연동하여 중앙 수집<br>* References: Google Cloud Persistent Disk Attachment Logging - https://cloud.google.com/compute/docs/disks/attach-disk#logging (연결 로그로 안정성 유지) |
| 스냅샷 용량 로그 추적 | scpcli baremetal-blockstorage volume snapshot-rate show --volume_id <볼륨ID> (CLI 확장) | 정상: snapshot_rate 로그에 용량 사용률 80% 이하 유지<br>취약: snapshot_rate 로그에 90% 초과 1회 이상 | 스냅샷 용량 로그 미추적으로 공간 오버플로 발생, 오래된 스냅샷 강제 삭제로 복구 옵션 상실, 용량 관리 지연으로 운영 부하 증가, 비용 초과 청구로 예산 초과 | 용량 로그를 임계값 알림으로 설정, 주기적 용량 조정 | * 확인방법: snapshot-rate show의 rate 변화 로그 분석<br>* 설정방법: 모니터링에서 용량 임계 80% 알림 설정<br>* References: AWS EBS Snapshot Capacity Logging - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-snapshots.html#snapshot-limits (용량 로그로 비용 최적화) |
| 백업 검증 로그 주기 | scpcli baremetal-blockstorage volume snapshot list --volume_id <볼륨ID> | 정상: 최근 30일 내 restore 테스트 로그 성공 1회 이상<br>취약: 90일 초과 restore 로그 없음 | 백업 검증 로그 미주기적으로 실제 복구 실패 위험 발생, 데이터 무결성 불확인으로 장애 시 손실 확대, 규정 준수 감사 실패로 벌금 위험, 운영 신뢰성 저하 | 월간 자동 복원 테스트 스크립트 실행 및 로그 기록 | * 확인방법: snapshot list의 restore 관련 메타데이터 로그<br>* 설정방법: cron job으로 restorevolumesnapshot 실행 후 로그 저장<br>* References: Azure Disk Backup Validation - https://learn.microsoft.com/en-us/azure/backup/backup-azure-vms-introduction#validate-recovery (검증 로그 베스트 프랙티스) |
| 패치 적용 이력 확인 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> 및 서버 패치 로그 | 정상: 최근 90일 내 패치 적용 로그 1회 이상<br>취약: 180일 초과 패치 로그 없음 | 패치 미적용으로 보안 취약점 노출 발생, iSCSI 프로토콜 취약으로 데이터 유출 위험, 운영 중 장애 원인 분석 지연으로 복구 시간 증가, 컴플라이언스 위반 | 분기별 패치 스케줄링 및 적용 로그 유지 | * 확인방법: show의 modified_at와 서버 yum/rpm 로그 비교<br>* 설정방법: 서버에서 yum update 후 로그 /var/log/yum.log 저장<br>* References: AWS EBS Patch Management - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html#ebs-patching (패치 로그로 보안 강화) |
| EOS 알림 설정 상태 | scpcli baremetal-blockstorage volume list --limit 100 | 정상: disk_type에 EOS 알림 연동 확인<br>취약: EOS 대상 disk_type 로그 미설정 | EOS 미알림으로 지원 종료 시 호환성 문제 발생, 성능 저하 미대응으로 워크로드 중단, 마이그레이션 지연으로 운영 비용 증가, 데이터 무결성 위험 | EOS 대상 자원에 알림 규칙 추가 | * 확인방법: list의 disk_type과 EOS 목록 비교<br>* 설정방법: SCP 콘솔에서 EOS 이벤트 알림 설정<br>* References: Google Cloud Disk EOS Policy - https://cloud.google.com/compute/docs/disks/deprecations (EOS 알림 베스트 프랙티스) |
| 로그 로테이션 설정 | 서버 측 logrotate -d /etc/logrotate.d/iscsi | 정상: 로그 파일 크기 100MB 이하, 주기 7일<br>취약: 로그 파일 1GB 초과 또는 주기 30일 초과 | 로그 로테이션 미설정으로 디스크 공간 고갈 발생, 오래된 로그 누적으로 분석 어려움, 운영 중 로그 손실로 장애 원인 추적 불가, 용량 오버플로로 서비스 중단 | logrotate.conf 수정으로 주기적 로테이션 | * 확인방법: logrotate -d 출력의 size/date 설정<br>* 설정방법: vi /etc/logrotate.d/iscsi; size 100M, weekly 추가<br>* References: Azure Linux Log Rotation - https://learn.microsoft.com/en-us/azure/virtual-machines/linux/disk-performance-linux#log-rotation (로테이션으로 공간 관리) |
| 감사 로그 활성화 | scpcli baremetal-blockstorage volume show --volume_id <볼륨ID> | 정상: created_by/modified_by 로그 상세 기록<br>취약: created_by/modified_by null 또는 빈 값 | 감사 로그 미활성화로 변경 이력 추적 불가 발생, 무단 접근 탐지 실패로 보안 사고 확대, 감사 시 증빙 부족으로 규정 위반, 운영 책임 소재 불명확 | IAM 정책으로 상세 감사 로그 활성화 | * 확인방법: show의 created_by/modified_by 필드 검토<br>* 설정방법: SCP IAM에서 audit log 레벨 높임<br>* References: AWS CloudTrail for EBS - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudtrail-ebs.html (감사 로그 베스트 프랙티스) |
| 모니터링 대시보드 설정 | SCP 모니터링 콘솔 대시보드 조회 | 정상: 볼륨별 IOPS/용량 위젯 100% 포함<br>취약: 대시보드에 볼륨 메트릭 50% 미만 | 모니터링 대시보드 미설정으로 실시간 지표 미감지 발생, 장애 초기 대응 지연으로 다운타임 확대, 운영 팀의 수동 조회 부하 증가, 예방적 조치 실패 | 대시보드에 볼륨 메트릭 위젯 추가 | * 확인방법: 모니터링 콘솔 대시보드 구성 확인<br>* 설정방법: SCP 모니터링에서 새 대시보드 생성 후 위젯 추가<br>* References: Google Cloud Monitoring Dashboards - https://cloud.google.com/monitoring/charts/dashboards (대시보드 설정 가이드) |
| failover 테스트 로그 | scpcli baremetal-blockstorage volume replication show --volume_id <볼륨ID> | 정상: 최근 90일 내 failover 테스트 로그 성공 1회 이상<br>취약: 180일 초과 테스트 로그 없음 | failover 테스트 로그 미기록으로 DR 프로세스 불확실 발생, 실제 장애 시 전환 실패로 RTO 초과, 운영 준비 부족으로 복구 지연, 비즈니스 손실 확대 | 분기별 failover 시뮬레이션 테스트 및 로그 기록 | * 확인방법: replication show의 failover 관련 로그<br>* 설정방법: DR 테스트 스크립트 실행 후 로그 저장<br>* References: Azure Site Recovery Test Failover - https://learn.microsoft.com/en-us/azure/site-recovery/azure-to-azure-test-failover (테스트 로그 관리) |
| 용량 예측 로그 유지 | 모니터링 콘솔 용량 트렌드 로그 | 정상: 30일 예측 로그 여유 20% 이상 기록<br>취약: 예측 로그 10% 미만 또는 미기록 | 용량 예측 로그 미유지로 확장 지연 발생, 공간 고갈로 쓰기 실패, 운영 중 예기치 않은 다운타임 증가, 비용 예측 오류로 예산 초과 | 트렌드 분석 도구 연동으로 월간 예측 로그 생성 | * 확인방법: 모니터링 용량 예측 히스토리 검토<br>* 설정방법: 모니터링에서 예측 쿼리 주기적 실행<br>* References: AWS CloudWatch Capacity Forecasting - https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/forecasting.html (예측 로그 베스트 프랙티스) |
| IOPS 로그 아카이빙 | 모니터링 콘솔 IOPS 메트릭 로그 다운로드 | 정상: 최근 90일 IOPS 로그 아카이브 저장<br>취약: 30일 초과 로그 미아카이브 | IOPS 로그 미아카이빙으로 패턴 분석 불가 발생, 성능 저하 원인 추적 지연, 장기 트렌드 미확인으로 스케일링 지연, 운영 보고서 불완전 | 로그를 S3-like 저장소로 자동 아카이브 | * 확인방법: 모니터링 로그 저장 기간 확인<br>* 설정방법: 모니터링 정책에서 90일 아카이브 설정<br>* References: Google Cloud Logging Archiving - https://cloud.google.com/logging/docs/export/configure_export_v2 (아카이브 관리) |
| 지연시간 로그 분석 주기 | 모니터링 콘솔 Latency 로그 리포트 | 정상: 주간 지연시간 로그 분석 보고서 생성<br>취약: 월간 초과 분석 없음 | 지연시간 로그 미주기 분석으로 병목 누적 발생, 애플리케이션 지연 확대, 사용자 불만 증가로 트래픽 감소, 운영 최적화 지연 | 주간 자동 리포트 생성 스크립트 구현 | * 확인방법: Latency 로그 리포트 생성 날짜 확인<br>* 설정방법: cron으로 모니터링 쿼리 실행 후 보고서 저장<br>* References: Azure Monitor Latency Analysis - https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/metrics-supported#microsoftstorageaccounts (주기 분석 가이드) |
| 스냅샷 복원 로그 검증 | scpcli baremetal-blockstorage volume snapshot restore --snapshot_id <ID> --volume_id <ID> 후 로그 | 정상: 복원 로그에 데이터 무결성 확인 100%<br>취약: 복원 로그에 무결성 오류 1회 이상 | 스냅샷 복원 로그 미검증으로 데이터 손상 위험 발생, 복구 실패 시 다운타임 연장, 백업 신뢰성 저하로 규정 위반, 운영 복구 프로세스 불신 | 복원 후 checksum 검증 스크립트 추가 | * 확인방법: restore 명령어 후 서버 파일 무결성 로그<br>* 설정방법: rsync --checksum으로 복원 검증<br>* References: AWS EBS Restore Validation - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume-from-snapshot.html#validate (무결성 검증) |
| 패치 다운타임 계획 로그 | 서버 패치 로그 /var/log/update.log | 정상: 패치 적용 시 다운타임 계획 로그 기록<br>취약: 패치 로그에 다운타임 미기록 | 패치 다운타임 미계획으로 서비스 중단 발생, 사용자 영향 최소화 실패로 불만 증가, 운영 SL A 위반으로 계약 문제, 복구 지연 | 패치 전 다운타임 계획 문서화 및 로그 연동 | * 확인방법: 패치 로그의 다운타임 필드 확인<br>* 설정방법: 패치 스크립트에 echo "Downtime planned" 추가<br>* References: Google Cloud Patch Management Planning - https://cloud.google.com/compute/docs/instances/maintenance-software-updates (다운타임 계획 베스트 프랙티스) |
| EOS 대상 자원 로그 | scpcli baremetal-blockstorage volume list --limit 100 | 정상: EOS 대상 disk_type 로그에 마이그레이션 계획 기록<br>취약: EOS 대상 로그 미기록 | EOS 대상 미로그로 지원 종료 시 호환성 문제 발생, 성능 저하 미대응으로 워크로드 중단, 마이그레이션 지연으로 운영 비용 증가 | EOS 자원에 마이그레이션 로그 추가 | * 확인방법: list의 disk_type과 EOS 로그 비교<br>* 설정방법: 모니터링에서 EOS 이벤트 로그 저장<br>* References: Azure Disk EOS Migration - https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types#deprecation (마이그레이션 로그 관리) |
| 로그 보안 접근 제어 | 서버 로그 파일 ls -la /var/log/iscsi | 정상: 로그 파일 권한 640, 소유자 root:adm<br>취약: 권한 644 이상 또는 소유자 변경 | 로그 보안 미제어로 무단 접근 발생, 민감 데이터 유출 위험, 감사 실패로 컴플라이언스 위반, 운영 보안 취약점 확대 | chmod/chown으로 로그 권한 강화 | * 확인방법: ls -la의 권한/소유자 확인<br>* 설정방법: chmod 640 /var/log/iscsi/*; chown root:adm /var/log/iscsi<br>* References: AWS Log Security Best Practices - https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/security_iam.html (로그 접근 제어) |
| 백업 보관 기간 로그 | scpcli baremetal-blockstorage volume snapshot list --volume_id <볼륨ID> | 정상: created_dt 로그에 90일 보관 준수<br>취약: 90일 초과 스냅샷 미삭제 로그 | 백업 보관 로그 미관리로 용량 오버플로 발생, 비용 증가로 예산 초과, 오래된 백업 의존 시 데이터 무결성 손상, 규정 준수 실패 | 자동 삭제 스크립트로 보관 기간 준수 | * 확인방법: snapshot list의 created_dt 필터링<br>* 설정방법: cron으로 90일 초과 스냅샷 delete<br>* References: Google Cloud Snapshot Retention - https://cloud.google.com/compute/docs/disks/snapshots#lifecycle_management (보관 기간 관리) |
| 운영 보고서 생성 주기 | 모니터링 콘솔 리포트 생성 로그 | 정상: 월간 운영 보고서 로그 생성<br>취약: 분기 초과 보고서 없음 | 운영 보고서 미주기 생성으로 트렌드 미파악 발생, 의사결정 지연으로 최적화 실패, 상위 경영진 보고 부족으로 자원 배정 오류 | 월간 자동 리포트 스크립트 구현 | * 확인방법: 리포트 생성 날짜 로그 확인<br>* 설정방법: 모니터링 API로 월간 리포트 생성<br>* References: Azure Monitor Reporting - https://learn.microsoft.com/en-us/azure/azure-monitor/visualize/workbooks (리포트 베스트 프랙티스) |
| 복제 정책 변경 로그 | scpcli baremetal-blockstorage volume replication-policy show --volume_id <볼륨ID> | 정상: policy 변경 이력 로그 상세 기록<br>취약: 변경 로그 미기록 | 복제 정책 변경 로그 미유지로 영향 분석 불가 발생, 설정 오류 누적으로 DR 실패, 운영 변경 추적 지연으로 복구 어려움 | 정책 변경 시 로그 자동 기록 | * 확인방법: replication-policy show의 변경 히스토리<br>* 설정방법: SCP API 호출 시 로그 플래그 추가<br>* References: AWS EBS Policy Change Logging - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-replication.html#logging (변경 로그 관리) |
| 볼륨 그룹 상태 로그 | scpcli baremetal-blockstorage volume-group show --volume_group_id <그룹ID> | 정상: state 로그에 'active' 100% 유지<br>취약: state 'error' 로그 1회 이상 | 볼륨 그룹 상태 로그 미추적으로 그룹 장애 확대 발생, 멤버 볼륨 영향으로 전체 워크로드 중단, 복구 지연으로 다운타임 증가 | 그룹 상태 로그 실시간 모니터링 | * 확인방법: volume-group show의 state 필드 로그<br>* 설정방법: 모니터링 알림으로 state 변화 감지<br>* References: Google Cloud Volume Group Status - https://cloud.google.com/compute/docs/disks/volume-groups#monitoring (상태 로그 베스트 프랙티스) |
